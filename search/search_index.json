{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EPN CMS collaboration","text":""},{"location":"#advances-of-the-project","title":"Advances of the Project","text":""},{"location":"#reproducible-analysis","title":"Reproducible Analysis:","text":"<p>Reproducible Analysis of CMS Open Data: Search for Dark Matter in Association with Top Quarks (Based on the CMS publication: \u201cSearch for dark matter produced in association with a single top quark or a top quark pair in proton\u2013proton collisions at (\\(\\sqrt s = 13 \\TeV\\)\u201d).</p>"},{"location":"All_Hadronic/all%20hadronic%20si/","title":"All hadronic chanel","text":""},{"location":"All_Hadronic/all%20hadronic%20si/#physics-motivation-and-channel-strategy-all-hadronic-channel","title":"Physics Motivation and Channel Strategy: All-Hadronic Channel","text":"<p>The Large Hadron Collider (LHC) collides protons at center-of-mass energies high enough to probe physics beyond the Standard Model. Although the protons are composite objects, the relevant hard scatterings occur between their constituents \u2014 quarks and gluons. In the context of simplified dark matter models, these partonic interactions can produce top quarks together with a new mediator particle (commonly denoted \u03c6 for scalar or a for pseudoscalar). The mediator then decays invisibly into a pair of dark matter candidates (\u03c7\u03c7\u0304). At the detector level, this results in events with multiple top quarks plus significant missing transverse momentum (p_T^miss), the latter coming from the invisible \u03c7 particles.</p> <p>The production mechanisms of interest include:</p> <p>\u2022 Gluon fusion:   $$ gg \\to t \\bar{t}\\,\\phi \\to t \\bar{t} + \\chi \\bar{\\chi} $$</p> <p>\u2022 Single top associated production:   $$ gb \\to t \\phi \\to t + \\chi \\bar{\\chi} $$</p> <p>\u2022 t\u2013channel production:   $$ qq' \\to tb \\phi \\to tb + \\chi \\bar{\\chi} $$</p> <p>In all cases, the top quarks decay via \\(t \\to W b\\). Each W boson subsequently decays either leptonically (\\(W \\to \\ell \\nu\\)) or hadronically (\\(W \\to q \\bar{q}'\\)). Thus, the final states contain a mixture of b-tagged jets, light-flavor jets, charged leptons (electrons or muons), and genuine \\(p_T^{\\text{miss}}\\).</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#channel-strategy","title":"Channel Strategy","text":"<p>Because of the different W decay modes, analyses are divided into channels, each defined by the number of isolated charged leptons:</p> <p>\u2022 Single-lepton (SL): one isolated electron or muon, several jets (including \u22651 b-tag), and nonzero \\(p_T^{\\text{miss}}\\). This channel is statistically powerful and relatively clean, striking a balance between signal sensitivity and manageable backgrounds.</p> <p>\u2022 All-hadronic (AH): no isolated leptons, many jets including b-tagged jets, and \\(p_T^{\\text{miss}}\\). While it has the largest raw yield, it suffers from overwhelming QCD multijet background, which can fake \\(p_T^{\\text{miss}}\\).</p> <p>\u2022 Dilepton: two isolated leptons, large \\(p_T^{\\text{miss}}\\), and multiple jets. It provides a very clean signal region but is limited by low branching fraction, hence low statistics.</p> <p>In this notebook, we concentrate on the all-hadronic channel with no isolated leptons.</p> <p>There are both theoretical and practical reasons for this choice:</p> <ul> <li> <p>From the physics side: The AH channel has the highest branching ratio (~46% for \\(t\\bar{t}\\)) since both W bosons decay hadronically. This provides maximum statistical power despite the challenging QCD background.</p> </li> <li> <p>From the experimental side: Dedicated MET-based triggers and stringent angular cuts can effectively suppress QCD contamination. The presence of multiple b-jets further enhances signal-to-background discrimination.</p> </li> </ul> <p>This focus allows us to demonstrate the full workflow \u2014 from event selection to histograms \u2014 in a setting where the interplay between signal characteristics and background processes can be clearly explained. Splitting into channels is therefore not a stylistic decision but a physics necessity: each final state probes the same underlying processes under different background conditions and detector signatures.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#software-setup-and-package-imports","title":"Software Setup and Package Imports","text":"<p>Before we start analyzing data, we need to set up the software environment. This section imports the Python packages that allow us to read CMS NanoAOD files, manipulate event data, and produce plots in a reproducible and physics-oriented way.</p> <p>\u2022 Core utilities:   - <code>os</code>, <code>time</code>, <code>json</code>, <code>logging</code>, <code>asyncio</code> \u2192 for file handling, timing, and bookkeeping.   - These are standard Python libraries that help organize the workflow and log progress.</p> <p>\u2022 Numerical analysis:   - <code>numpy</code> (<code>np</code>) \u2192 fundamental for vectorized calculations on arrays.   - <code>pandas</code> (<code>pd</code>) \u2192 convenient for storing metadata (cross sections, cutflows, etc.) in table form.</p> <p>\u2022 Visualization:   - <code>matplotlib</code> (<code>mpl</code>, <code>plt</code>) \u2192 general-purpose plotting.   - <code>hist</code> \u2192 modern histogramming library designed for HEP, integrates smoothly with <code>mplhep</code>.</p> <p>\u2022 HEP-specific data access:   - <code>uproot</code> \u2192 reads CMS <code>.root</code> files into Python without needing C++/ROOT. Essential for open data workflows.   - <code>awkward</code> (<code>ak</code>) \u2192 handles \"jagged arrays\" (variable-length collections per event), e.g. different numbers of jets per event.   - <code>vector</code> \u2192 enables 4-vector operations (pT, eta, phi, invariant masses) in a NumPy/Awkward-friendly way. We register it with Awkward to use directly on event data.</p> <p>\u2022 Coffea ecosystem:   - <code>coffea.processor</code> \u2192 framework to run HEP analyses at scale.   - <code>NanoAODSchema</code> and <code>NanoEventsFactory</code> \u2192 provide a high-level interface to CMS NanoAOD, automatically building event objects like muons, jets, MET, etc.   - <code>transforms</code> and <code>methods.base/vector</code> \u2192 ensure physics-style behavior (Lorentz vectors, masks, object methods) are available on Awkward arrays.</p> <p>Finally, we configure Coffea to ignore unusual cross-references in CMS open data (they don't affect our analysis) and print the versions of the key packages. This ensures reproducibility: anyone re-running the notebook can confirm they are using the same software environment.</p> <pre><code>import asyncio, logging, os, time, json\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport hist\nimport uproot\nimport awkward as ak\nimport vector as v\nv.register_awkward()\n\n# Coffea\nfrom coffea import processor\nfrom coffea.nanoevents import NanoAODSchema, NanoEventsFactory\nfrom coffea.nanoevents import transforms\nfrom coffea.nanoevents.methods import base, vector as cvector\n\n# Ensure Coffea behaviors are available on Awkward objects\nak.behavior.update(base.behavior)\nak.behavior.update(cvector.behavior)\n\n# Be gentle with open-data odd cross-refs\nNanoAODSchema.warn_missing_crossrefs = False\n\nimport coffea\nprint(\"versions:\",\n      \"coffea\", coffea.__version__,\n      \"| awkward\", ak.__version__,\n      \"| uproot\", uproot.__version__)\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#dataset-definitions-and-luminosity-masks","title":"Dataset Definitions and Luminosity Masks","text":"<p>The CMS Open Data portal provides access to collision and simulation datasets in the NanoAOD format. Each dataset (e.g. MET, TTToHadronic, W+jets) is split across many <code>.root</code> files, sometimes hundreds per process. To manage this efficiently, we build a Python dictionary <code>nanoaod_filenames</code> that maps process names to their corresponding file index lists. Each entry points to a <code>.txt</code> file containing the remote paths (via XRootD) of the NanoAOD files.</p> <p>Examples:</p> <p>\u2022 Data (collision events): <code>MET</code>. These correspond to recorded events with MET-based trigger paths.</p> <p>\u2022 Signal MC: e.g. hypothetical \\(t\\bar{t} + DM\\) samples.</p> <p>\u2022 Background MC: top pair production (hadronic, semileptonic, dileptonic), single top (t-channel, tW), and electroweak processes like W+jets, Z\u2192\u03bd\u03bd, or dibosons (WW, ZZ).</p> <p>This separation is not just organizational:</p> <ul> <li> <p>From a physics perspective, each dataset represents a different process contributing to the observed events. Signal vs. background categories are crucial for defining the search strategy.</p> </li> <li> <p>From a programming perspective, keeping datasets in a dictionary allows us to iterate over them in loops, automate file loading, and apply the same selections consistently.</p> </li> </ul>"},{"location":"All_Hadronic/all%20hadronic%20si/#luminosity-masks","title":"Luminosity Masks","text":"<p>Real CMS data are recorded in luminosity sections (blocks of events). Not all sections are usable: some are flagged as problematic by the detector monitoring. To ensure reproducibility, CMS provides certified luminosity JSON files, which specify the \"good\" sections.</p> <p>The function <code>build_lumi_mask()</code> implements this filter:</p> <p>\u2022 Reads the certified JSON file. \u2022 Compares the <code>run</code> and <code>luminosityBlock</code> of each event to the approved ranges. \u2022 Returns a boolean mask selecting only events in certified sections.</p> <p>This step is critical in real data analysis:</p> <ul> <li>Physics motivation: prevents contamination from detector malfunctions.</li> </ul>"},{"location":"All_Hadronic/all%20hadronic%20si/#helper-functions","title":"Helper Functions","text":"<p>\u2022 <code>get_files_for_dataset(dataset, random=False, n=0)</code> \u2192 loads a subset of filenames for a given dataset. Useful when testing code with fewer files to save time.</p> <p>\u2022 <code>pretty_print(fields, ...)</code> \u2192 formats lists of branches or variables, making the NanoAOD structure easier to inspect.</p> <p>Together, these utilities allow us to handle dozens of datasets and millions of events in a manageable, modular way.</p> <pre><code>import dpoa_workshop\nfrom dpoa_workshop import nanoaod_filenames\nfrom dpoa_workshop import get_files_for_dataset\nfrom dpoa_workshop import pretty_print\nfrom dpoa_workshop import build_lumi_mask\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#building-the-ntuple-file-index","title":"Building the Ntuple File Index","text":"<p>CMS Open Data provides file index text files (<code>file_index.txt</code>) for each dataset. These contain the actual XRootD paths to the NanoAOD <code>.root</code> files, along with metadata such as the number of events per file.</p> <p>To streamline the workflow:</p> <p>\u2022 We define a function <code>download_and_parse_fileindex(url)</code> that fetches each <code>file_index.txt</code> via HTTP and extracts only the ROOT file paths.</p> <p>\u2022 We loop over all entries in <code>nanoaod_filenames</code> (the dictionary we built earlier) and collect the full list of ROOT files per dataset.</p> <p>\u2022 The result is stored in a new dictionary <code>ntuples_simple</code>, which maps dataset \u2192 list of ROOT file paths.</p> <p>\u2022 Finally, we save this as a JSON file (<code>ntuples_simple.json</code>) for later reuse.</p> <pre><code>import os, json, requests\nfrom dpoa_workshop import nanoaod_filenames\n\ndef download_and_parse_fileindex(url):\n    \"\"\"Download a file_index.txt and return list of ROOT paths.\"\"\"\n    r = requests.get(url)\n    lines = [ln.strip() for ln in r.text.splitlines() if ln.strip()]\n    # Each line is: root://... nevts=N\n    paths = [ln.split()[0] for ln in lines]\n    return paths\n\nntuples_simple = {}\n\n# Loop through nanoaod_filenames and save only paths\nfor dataset, urls in nanoaod_filenames.items():\n    all_paths = []\n    for url in urls:\n        try:\n            all_paths.extend(download_and_parse_fileindex(url))\n        except Exception as e:\n            print(f\"[warn] {dataset} {url} -&gt; {e}\")\n\n    ntuples_simple[dataset] = all_paths\n\n# Save the JSON\nwith open(\"ntuples_simple.json\", \"w\") as f:\n    json.dump(ntuples_simple, f, indent=2)\n\nprint(\"ntuples_simple.json created with datasets:\", list(ntuples_simple.keys()))\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#analysis-configuration","title":"Analysis Configuration","text":"<p>We first define which datasets to analyze. This includes data (<code>MET</code>) and several MC backgrounds (ttbar, single top, W+jets, dibosons, Z\u2192\u03bd\u03bd, etc.).</p> <p>We also set controls for testing:</p> <p>\u2022 <code>N_FILES_MAX_PER_SAMPLE</code>: how many ROOT files per dataset. \u2022 <code>MAX_EVENTS_PER_FILE</code>: how many events to read per file.</p> <p>This allows us to run quickly on subsets of data before scaling to the full analysis.</p> <pre><code># ================================\n# Initial configuration\n# ================================\nDATASETS_TO_RUN = [\n    \"MET\",\n    \"ttbar-hadronic\",\n    \"ttbar-semileptonic\",\n    \"t-channel-top\",\n    \"ttW\",\n    \"WJets-HT400to600\",\n    \"WJets-2J-FxFx\",\n    \"DYJets-Zpt200\",\n    \"Zvv\",\n    \"ZZ\",\n    \"WW\",\n]\n\n# Limit ROOTs and events\nN_FILES_MAX_PER_SAMPLE = 1  # use only 1 ROOT file per dataset\nMAX_EVENTS_PER_FILE = 50000  # None = use all events\n\n# Luminosity (fb^-1)\nLUMI_FB = 1.0\n\n# JSON with paths to the ROOTs\nNTUPLES_JSON = \"ntuples_simple.json\"\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#constructing-the-fileset","title":"Constructing the Fileset","text":"<p>We construct a fileset, which is a dictionary that maps each dataset name to:</p> <p>\u2022 A list of ROOT file paths. \u2022 Metadata such as process name, cross section, and variation.</p> <p>This abstraction allows us to loop over datasets uniformly later, regardless of whether they are data or MC.</p> <pre><code>import json\n\ndef construct_fileset_simple(\n    n_files_max_per_sample=1,\n    ntuples_json=\"ntuples_simple.json\"\n):\n    with open(ntuples_json) as f:\n        info = json.load(f)\n\n    fileset = {}\n    for key, files in info.items():\n        if key not in DATASETS_TO_RUN:\n            continue\n\n        # limit number of files\n        if n_files_max_per_sample == -1:\n            use_files = files\n        else:\n            use_files = files[:n_files_max_per_sample]\n\n        # normalize file list\n        file_list = [f[\"path\"] if isinstance(f, dict) else f for f in use_files]\n\n        fileset[key] = {\n            \"files\": file_list,\n            \"metadata\": {\n                \"process\": key,\n                \"dataset\": key,\n                \"variation\": \"nominal\",\n            }\n        }\n\n    return fileset\n\n# --- construct fileset ---\nfileset = construct_fileset_simple(\n    n_files_max_per_sample=N_FILES_MAX_PER_SAMPLE,\n    ntuples_json=NTUPLES_JSON\n)\n\nprint(f\"[OK] Fileset constructed with {len(fileset)} datasets\")\nfor k, pack in fileset.items():\n    print(f\"  {k}: {len(pack['files'])} file(s)\")\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#baseline-event-selection-all-hadronic-channel","title":"Baseline Event Selection: All-Hadronic Channel","text":"<p>We now define the baseline cuts for the all-hadronic channel with no isolated leptons. These are motivated by CMS dark matter searches and are designed to suppress dominant Standard Model backgrounds (mainly QCD multijet, \\(t\\bar{t}\\), and W/Z+jets) while enhancing sensitivity to dark matter signals with real \\(p_T^{\\text{miss}}\\).</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#1-trigger-requirements","title":"1. Trigger Requirements","text":"<p>Events are selected if they fire any of the following HLT paths:</p> <p>\u2022 <code>HLT_PFMETNoMu120</code> \u2022 <code>HLT_PFMETNoMu90_PFMHTNoMu90_IDTight</code> \u2022 <code>HLT_PFMETNoMu110_PFMHTNoMu110_IDTight</code> \u2022 <code>HLT_PFMETNoMu120_PFMHTNoMu120_IDTight</code> \u2022 <code>HLT_PFMETNoMu90_JetIdCleaned_PFMHTNoMu90_IDTight</code> \u2022 <code>HLT_PFMETNoMu120_JetIdCleaned_PFMHTNoMu120_IDTight</code> \u2022 <code>HLT_PFMET120_PFMHT120</code> \u2022 <code>HLT_PFMET110_PFMHT110_IDTight</code> \u2022 <code>HLT_PFMET120_PFMHT120_IDTight</code> \u2022 <code>HLT_PFMET170</code> \u2022 <code>HLT_PFMET170_NoiseCleaned</code> \u2022 <code>HLT_PFMET170_HBHECleaned</code> \u2022 <code>HLT_PFMET170_HBHE_BeamHaloCleaned</code></p> <p>Motivation: These MET-based triggers are efficient for hadronic final states with genuine missing energy.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#2-event-cleaning-flags","title":"2. Event Cleaning Flags","text":"<p>The following filters must be applied to both data and simulation:</p> <p>\u2022 <code>HBHENoiseFilter</code> \u2022 <code>HBHENoiseIsoFilter</code> \u2022 <code>ECALDeadCellFilter</code> \u2022 <code>GlobalTightHalo2016Filter</code> \u2022 <code>BadPFMuonFilter</code> \u2022 <code>BadChargedHadronFilter</code></p> <p>The following filters are applied to data only:</p> <p>\u2022 <code>EEBadScFilter</code> \u2022 <code>BadMuons</code> \u2022 <code>DuplicateMuons</code></p> <p>Motivation: Remove events with detector noise and instrumental fake MET.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#3-lepton-veto","title":"3. Lepton Veto","text":"<p>\u2022 No \"Veto\" Leptons: Events must contain no \"Veto\" leptons.   - Electrons: No electrons with \\(p_T &gt; 10\\) GeV, \\(|\\eta| &lt; 2.5\\), and loose ID.   - Muons: No muons with \\(p_T &gt; 10\\) GeV, \\(|\\eta| &lt; 2.4\\), and loose ID.</p> <p>Motivation: Suppress semileptonic \\(t\\bar{t}\\), W+jets, and dilepton backgrounds.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#4-jet-selection","title":"4. Jet Selection","text":"<p>\u2022 Jet \\(p_T\\): &gt; 30 GeV \u2022 Jet Eta:   - Central jets: \\(|\\eta| &lt; 2.4\\)   - Forward jets: \\(2.4 &lt; |\\eta| &lt; 5\\) (optional, depending on analysis) \u2022 Jet ID: Loose jet ID requirements \u2022 Overlap Removal: Jet objects are not considered if they are within \\(\\Delta R &lt; 0.4\\) of a \"Tight\" electron or muon.</p> <p>Motivation: Ensure well-reconstructed jets while avoiding double-counting with leptons.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#5-minimum-number-of-jets","title":"5. Minimum Number of Jets","text":"<p>\u2022 Baseline: \u2265 3 jets</p> <p>Motivation: Top decays produce at least 6 quarks in the \\(t\\bar{t}\\) fully hadronic channel (2 b-quarks + 4 light quarks from W decays). Requiring \u22653 jets ensures we capture the event topology.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#6-minimum-number-of-b-tagged-jets","title":"6. Minimum Number of b-tagged Jets","text":"<p>\u2022 Baseline: \u2265 1 b-tagged jet (CSVM working point) with \\(p_T &gt; 30\\) GeV \u2022 Categorization:   - \\(n_b = 1\\) (for single top + DM events)   - \\(n_b \u2265 2\\) (for \\(t\\bar{t}\\) + DM events)</p> <p>Motivation: Top-quark decays always produce b-jets, so this suppresses W+light-flavor jets and QCD.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#7-met-requirements","title":"7. MET Requirements","text":"<p>\u2022 Baseline: \\(p_T^{\\text{miss}} \u2265 250\\) GeV</p> <p>Motivation: Dark matter escapes undetected \u2192 large genuine MET. This high threshold strongly suppresses QCD multijet background.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#8-angular-separation-cuts","title":"8. Angular Separation: \u0394\u03c6 Cuts","text":"<p>\u2022 Baseline: \\(\\min\\Delta\\phi(j_{1,2}, p_T^{\\text{miss}}) &gt; 0.4\\) \u2022 Optimized Selection: \\(\\min\\Delta\\phi(j_{1,2}, p_T^{\\text{miss}}) &gt; 1.0\\)</p> <p>Motivation: Reduces QCD multijet events with mismeasured MET aligned with jets. True MET from dark matter is typically more isotropic.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#9-additional-kinematic-variables","title":"9. Additional Kinematic Variables","text":"<p>\u2022 b\u2013MET Transverse Mass (\\(M_{bT}\\)):   - \\(M_{bT} &gt; 180\\) GeV   - Motivation: Further discriminates signal from \\(t\\bar{t}\\) backgrounds by exploiting the kinematics of b-quarks and MET.</p> <p>\u2022 Jet 1 \\(p_T\\) / \\(H_T\\):   - \u2264 0.5 (specifically for \\(n_b \u2265 2\\) category)   - Motivation: Reduces events where a single jet dominates the hadronic activity, which is characteristic of QCD.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#summary","title":"Summary","text":"<p>The baseline selection region is defined by:</p> <p>\u2022 MET-based triggers (no leptons required) \u2022 Event cleaning filters \u2022 No veto leptons (electrons or muons with \\(p_T &gt; 10\\) GeV) \u2022 \u2265 3 jets with \u2265 1 b-tag \u2022 Large MET (\\(p_T^{\\text{miss}} \u2265 250\\) GeV) \u2022 Angular separation to suppress QCD (\\(\\min\\Delta\\phi(j_{1,2}, p_T^{\\text{miss}}) &gt; 0.4\\) or \\(&gt; 1.0\\)) \u2022 Additional kinematic cuts (\\(M_{bT} &gt; 180\\) GeV, jet \\(p_T\\) / \\(H_T\\) \u2264 0.5)</p> <p>Together, these cuts target signal-like topologies while removing the bulk of Standard Model backgrounds, especially the overwhelming QCD multijet contamination.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#analysis-implementation","title":"Analysis Implementation","text":"<p>This class (<code>DMAnalysisAllHadronic</code>) encodes the physics selection of the analysis for the all-hadronic channel:</p> <p>\u2022 Object-level cuts:   - Veto loose electrons: \\(p_T &gt; 10\\) GeV, \\(|\\eta| &lt; 2.5\\), loose ID.   - Veto loose muons: \\(p_T &gt; 10\\) GeV, \\(|\\eta| &lt; 2.4\\), loose ID.   - Jets: \\(p_T &gt; 30\\) GeV, \\(|\\eta| &lt; 2.4\\), good jetID.   - b-jets: tagged with DeepCSV/DeepFlav WP.</p> <p>\u2022 Event-level cuts:   - Pass MET-based triggers (HLT).   - No veto leptons (0 electrons, 0 muons).   - \u22653 jets, \u22651 b-jet.   - \\(p_T^{\\text{miss}} \u2265 250\\) GeV.   - \\(\\min\\Delta\\phi(j_{1,2}, p_T^{\\text{miss}}) &gt; 0.4\\).</p> <p>\u2022 Outputs:   - Histograms for physics variables.   - Cutflow (number of events passing each step).</p> <pre><code>import time, pickle\nfrom pathlib import Path\nimport numpy as np\nimport awkward as ak\nimport hist\nfrom coffea.nanoevents import NanoAODSchema, NanoEventsFactory\n\n# === Function to read events from a ROOT file with optional limit ===\ndef events_from_file(path, metadata=None, schemaclass=NanoAODSchema, max_events=None):\n    \"\"\"\n    Load events from a ROOT file.\n    If max_events is not None, only read up to that number of events (useful for quick tests).\n    \"\"\"\n    factory = NanoEventsFactory.from_root(\n        {path: \"Events\"},\n        schemaclass=schemaclass,\n        metadata=(metadata or {}),\n        entry_stop=max_events,\n    )\n    return factory.events()\n\n# === Main class for Dark Matter analysis (All-Hadronic channel) ===\nclass DMAnalysisAllHadronic:\n    def __init__(self, DATASET, lumi_fb):\n        self.DATASET = DATASET\n        self.lumi_fb = float(lumi_fb)\n\n        # Histogram axes\n        self.process_cat = hist.axis.StrCategory([], name=\"process\", label=\"Process\", growth=True)\n        self.variation_cat = hist.axis.StrCategory([], name=\"variation\", label=\"Variation\", growth=True)\n\n        # Physics variables\n        num_axis = hist.axis.Regular(15, 0, 15, name=\"var\")      # njets, nbjets\n        met_axis = hist.axis.Regular(30, 0, 600, name=\"var\")     # MET\n        dphi_axis = hist.axis.Regular(32, 0, 3.2, name=\"var\")    # min deltaphi\n        ht_axis = hist.axis.Regular(40, 0, 2000, name=\"var\")     # HT\n        mbt_axis = hist.axis.Regular(30, 0, 600, name=\"var\")     # M_bT\n\n        # Histograms\n        self.h = {\n            'njets':    hist.Hist(num_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'nbjets':   hist.Hist(num_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'met':      hist.Hist(met_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'min_dphi': hist.Hist(dphi_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'ht':       hist.Hist(ht_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'mbt':      hist.Hist(mbt_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n        }\n\n        # Cutflow\n        self.cut_flow = {\n            \"All\": 0,\n            \"HLT_MET\": 0,\n            \"Filters\": 0,\n            \"LeptonVeto\": 0,\n            \"&gt;=3jets\": 0,\n            \"&gt;=1btag\": 0,\n            \"MET&gt;=250\": 0,\n            \"minDPhi&gt;0.4\": 0,\n        }\n\n    # ---------- helpers ----------\n    def _pass_met_hlt(self, events):\n        \"\"\"Apply MET-based HLT paths.\"\"\"\n        hlt = getattr(events, \"HLT\", None)\n        if hlt is None:\n            return ak.ones_like(events.event, dtype=bool)\n\n        hlt_paths = [\n            \"PFMETNoMu120\",\n            \"PFMETNoMu90_PFMHTNoMu90_IDTight\",\n            \"PFMETNoMu110_PFMHTNoMu110_IDTight\",\n            \"PFMETNoMu120_PFMHTNoMu120_IDTight\",\n            \"PFMET120_PFMHT120\",\n            \"PFMET110_PFMHT110_IDTight\",\n            \"PFMET120_PFMHT120_IDTight\",\n            \"PFMET170\",\n        ]\n\n        masks = []\n        for name in hlt_paths:\n            if hasattr(hlt, name):\n                masks.append(ak.values_astype(getattr(hlt, name), bool))\n\n        if not masks:\n            return ak.ones_like(events.event, dtype=bool)\n\n        # Logical OR of all triggers\n        combined = masks[0]\n        for m in masks[1:]:\n            combined = combined | m\n        return combined\n\n    def _pass_event_filters(self, events):\n        \"\"\"Apply event cleaning flags.\"\"\"\n        flag = getattr(events, \"Flag\", None)\n        if flag is None:\n            return ak.ones_like(events.event, dtype=bool)\n\n        required_flags = [\n            \"HBHENoiseFilter\",\n            \"HBHENoiseIsoFilter\",\n            \"EcalDeadCellTriggerPrimitiveFilter\",\n            \"globalSuperTightHalo2016Filter\",\n            \"BadPFMuonFilter\",\n        ]\n\n        mask = ak.ones_like(events.event, dtype=bool)\n        for fname in required_flags:\n            if hasattr(flag, fname):\n                mask = mask &amp; ak.values_astype(getattr(flag, fname), bool)\n\n        return mask\n\n    def _select_veto_electrons(self, events):\n        \"\"\"Veto loose electrons: pt &gt; 10, |eta| &lt; 2.5.\"\"\"\n        el = events.Electron\n        if hasattr(el, \"cutBased\"):\n            return el[(el.pt &gt; 10) &amp; (abs(el.eta) &lt; 2.5) &amp; (el.cutBased &gt;= 1)]\n        return el[(el.pt &gt; 10) &amp; (abs(el.eta) &lt; 2.5)]\n\n    def _select_veto_muons(self, events):\n        \"\"\"Veto loose muons: pt &gt; 10, |eta| &lt; 2.4.\"\"\"\n        mu = events.Muon\n        if hasattr(mu, \"looseId\"):\n            return mu[(mu.pt &gt; 10) &amp; (abs(mu.eta) &lt; 2.4) &amp; (mu.looseId == True)]\n        return mu[(mu.pt &gt; 10) &amp; (abs(mu.eta) &lt; 2.4)]\n\n    def _select_good_jets(self, events):\n        \"\"\"Jets: pt &gt; 30, |eta| &lt; 2.4, jetId &gt;= 2.\"\"\"\n        jets = events.Jet\n        jetid = getattr(jets, \"jetId\", ak.zeros_like(jets.pt, dtype=np.int32) + 2)\n        return jets[(jets.pt &gt; 30) &amp; (abs(jets.eta) &lt; 2.4) &amp; (jetid &gt;= 2)]\n\n    def _count_bjets(self, jets):\n        \"\"\"Count b-tagged jets (DeepFlav or DeepCSV medium WP).\"\"\"\n        if hasattr(jets, \"btagDeepFlavB\"):\n            mask = jets.btagDeepFlavB &gt; 0.3093  # medium WP\n        elif hasattr(jets, \"btagDeepB\"):\n            mask = jets.btagDeepB &gt; 0.6321      # medium WP\n        else:\n            mask = ak.zeros_like(jets.pt, dtype=bool)\n        return ak.sum(mask, axis=1), mask\n\n    def _compute_min_dphi(self, jets, met):\n        \"\"\"Compute min delta phi between leading 2 jets and MET.\"\"\"\n        if ak.any(ak.num(jets) &gt;= 2):\n            jet1 = jets[:, 0]\n            jet2 = jets[:, 1]\n            dphi1 = abs(jet1.phi - met.phi)\n            dphi2 = abs(jet2.phi - met.phi)\n            dphi1 = ak.where(dphi1 &gt; np.pi, 2*np.pi - dphi1, dphi1)\n            dphi2 = ak.where(dphi2 &gt; np.pi, 2*np.pi - dphi2, dphi2)\n            min_dphi = ak.where(dphi1 &lt; dphi2, dphi1, dphi2)\n        else:\n            min_dphi = ak.zeros_like(met.pt) + 999.0\n        return min_dphi\n\n    def _compute_ht(self, jets):\n        \"\"\"Compute HT = scalar sum of jet pT.\"\"\"\n        return ak.sum(jets.pt, axis=1)\n\n    def _compute_mbt(self, bjets, met):\n        \"\"\"Compute transverse mass between leading b-jet and MET.\"\"\"\n        if ak.any(ak.num(bjets) &gt;= 1):\n            bjet_lead = bjets[:, 0]\n            mbt = np.sqrt(2.0 * bjet_lead.pt * met.pt * \n                         (1.0 - np.cos(bjet_lead.phi - met.phi)))\n        else:\n            mbt = ak.zeros_like(met.pt)\n        return mbt\n\n    def _weights(self, events):\n        \"\"\"Weights disabled: return 1 for each event.\"\"\"\n        return np.ones(len(events), dtype=\"float64\")\n\n    def _labels_array(self, label, n):\n        return np.array([label] * int(n), dtype=object)\n\n    def _fill_event_hist(self, H, var_np, process, variation, w_np):\n        H.fill(\n            var=var_np,\n            process=self._labels_array(process, len(var_np)),\n            variation=self._labels_array(variation, len(var_np)),\n            weight=w_np\n        )\n\n    # ---------- main pipeline ----------\n    def process(self, events):\n        process = events.metadata.get(\"process\", \"unknown\")\n        variation = events.metadata.get(\"variation\", \"nominal\")\n\n        w_evt = self._weights(events)\n        self.cut_flow[\"All\"] += len(events)\n\n        # HLT\n        hltmask = self._pass_met_hlt(events)\n        events = events[hltmask]\n        w_evt = w_evt[ak.to_numpy(hltmask)]\n```python\n        self.cut_flow[\"HLT_MET\"] += len(events)\n\n        # Event filters\n        filtermask = self._pass_event_filters(events)\n        events = events[filtermask]\n        w_evt = w_evt[ak.to_numpy(filtermask)]\n        self.cut_flow[\"Filters\"] += len(events)\n\n        # Lepton veto (no veto electrons, no veto muons)\n        el_veto = self._select_veto_electrons(events)\n        mu_veto = self._select_veto_muons(events)\n        mask_lep = (ak.num(el_veto) == 0) &amp; (ak.num(mu_veto) == 0)\n        events = events[mask_lep]\n        w_evt = w_evt[ak.to_numpy(mask_lep)]\n        self.cut_flow[\"LeptonVeto\"] += len(events)\n\n        # Jets\n        jets = self._select_good_jets(events)\n        nj = ak.num(jets)\n\n        # &gt;= 3 jets\n        jets_ok = (nj &gt;= 3)\n        events = events[jets_ok]\n        jets = jets[jets_ok]\n        nj = nj[jets_ok]\n        w_evt = w_evt[ak.to_numpy(jets_ok)]\n        self.cut_flow[\"&gt;=3jets\"] += len(events)\n\n        if len(events) == 0:\n            return {\"nevents\": {process: 0}, \"hists\": self.h}\n\n        # b-jets\n        nb, btag_mask = self._count_bjets(jets)\n        bjets = jets[btag_mask]\n\n        # &gt;= 1 b-tag\n        btag_ok = (nb &gt;= 1)\n        events = events[btag_ok]\n        jets = jets[btag_ok]\n        bjets = bjets[btag_ok]\n        nj = nj[btag_ok]\n        nb = nb[btag_ok]\n        w_evt = w_evt[ak.to_numpy(btag_ok)]\n        self.cut_flow[\"&gt;=1btag\"] += len(events)\n\n        if len(events) == 0:\n            return {\"nevents\": {process: 0}, \"hists\": self.h}\n\n        # MET\n        met = getattr(events, \"MET\", None)\n        met_pt = met.pt if (met is not None and hasattr(met, \"pt\")) else ak.zeros_like(events.event, dtype=float)\n\n        # MET &gt;= 250 GeV\n        met_ok = (met_pt &gt;= 250)\n        events = events[met_ok]\n        jets = jets[met_ok]\n        bjets = bjets[met_ok]\n        nj = nj[met_ok]\n        nb = nb[met_ok]\n        met_pt = met_pt[met_ok]\n        met = met[met_ok]\n        w_evt = w_evt[ak.to_numpy(met_ok)]\n        self.cut_flow[\"MET&gt;=250\"] += len(events)\n\n        if len(events) == 0:\n            return {\"nevents\": {process: 0}, \"hists\": self.h}\n\n        # min delta phi\n        min_dphi = self._compute_min_dphi(jets, met)\n\n        # min delta phi &gt; 0.4\n        dphi_ok = (min_dphi &gt; 0.4)\n        events = events[dphi_ok]\n        jets = jets[dphi_ok]\n        bjets = bjets[dphi_ok]\n        nj = nj[dphi_ok]\n        nb = nb[dphi_ok]\n        met_pt = met_pt[dphi_ok]\n        met = met[dphi_ok]\n        min_dphi = min_dphi[dphi_ok]\n        w_evt = w_evt[ak.to_numpy(dphi_ok)]\n        self.cut_flow[\"minDPhi&gt;0.4\"] += len(events)\n\n        if len(events) == 0:\n            return {\"nevents\": {process: 0}, \"hists\": self.h}\n\n        # Additional kinematic variables\n        ht = self._compute_ht(jets)\n        mbt = self._compute_mbt(bjets, met)\n\n        # Fill histograms\n        w_evt_np = ak.to_numpy(w_evt)\n        self._fill_event_hist(self.h['njets'], ak.to_numpy(nj), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['nbjets'], ak.to_numpy(nb), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['met'], ak.to_numpy(met_pt), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['min_dphi'], ak.to_numpy(min_dphi), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['ht'], ak.to_numpy(ht), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['mbt'], ak.to_numpy(mbt), process, variation, w_evt_np)\n\n        # Return both histos and filtered events\n        return {\n            \"nevents\": {process: int(len(w_evt_np))},\n            \"hists\": self.h,\n            \"selected_events\": events,\n            \"jets\": jets,\n            \"bjets\": bjets,\n            \"njets\": nj,\n            \"nbjets\": nb,\n            \"met\": met_pt,\n            \"min_dphi\": min_dphi,\n            \"ht\": ht,\n            \"mbt\": mbt,\n        }\n\n    def postprocess(self, accumulator):\n        return accumulator\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#running-the-analysis","title":"Running the Analysis","text":"<p>Now we run the main loop:</p> <ol> <li>Iterate over each dataset in the fileset.</li> <li>Read events from ROOT files with <code>events_from_file</code>.</li> <li>Apply the physics cuts with <code>analysis.process</code>.</li> <li>Save results:    \u2022 Per-dataset CSVs with selected event variables.    \u2022 Histograms (pickled) for plotting later.    \u2022 Cutflow table to verify event selection efficiency.    \u2022 Event counts for normalization.</li> </ol> <p>This is the driver stage where the analysis is executed.</p> <pre><code>from pathlib import Path\nimport pandas as pd\nimport awkward as ak\nimport pickle, time\nimport numpy as np\n\nOUT_DIR = Path(\"./outputs_ah\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nOUT_HISTOS = OUT_DIR / \"dm_histograms_ah.pkl\"\nOUT_CUTFLOW = OUT_DIR / \"dm_cutflow_ah.csv\"\nOUT_NEVENTS = OUT_DIR / \"dm_nevents_ah.csv\"\n\nanalysis = DMAnalysisAllHadronic(DATASET=\"MET\", lumi_fb=LUMI_FB)\n\nentries_total = 0\nnevents_rows = []\n\nt0 = time.monotonic()\n\nfor key, pack in fileset.items():\n    flist = pack[\"files\"]\n    meta = pack[\"metadata\"]\n\n    if not flist:\n        continue\n\n    events_all = []\n\n    for path in flist:\n        md = dict(meta)\n\n        try:\n            ev = events_from_file(path, metadata=md, max_events=MAX_EVENTS_PER_FILE)\n        except Exception as e:\n            print(f\"[warn] skip {path} -&gt; {e}\")\n            continue\n\n        entries_total += len(ev)\n\n        # --- run analysis ---\n        out = analysis.process(ev)\n        nsel = list(out[\"nevents\"].values())[0]\n\n        if nsel == 0:\n            continue\n\n        # --- use filtered events for CSV ---\n        ev_sel = out[\"selected_events\"]\n        jets = out[\"jets\"]\n        bjets = out[\"bjets\"]\n        nj = out[\"njets\"]\n        nb = out[\"nbjets\"]\n        met_pt = out[\"met\"]\n        min_dphi = out[\"min_dphi\"]\n        ht = out[\"ht\"]\n        mbt = out[\"mbt\"]\n\n        df_ev = pd.DataFrame({\n            \"process\": [md[\"process\"]]*len(ev_sel),\n            \"dataset\": [md[\"dataset\"]]*len(ev_sel),\n            \"njets\": ak.to_numpy(nj),\n            \"nbjets\": ak.to_numpy(nb),\n            \"met\": ak.to_numpy(met_pt),\n            \"min_dphi\": ak.to_numpy(min_dphi),\n            \"ht\": ak.to_numpy(ht),\n            \"mbt\": ak.to_numpy(mbt),\n        })\n\n        events_all.append(df_ev)\n\n        nevents_rows.append({\n            \"key\": key,\n            \"process\": md[\"process\"],\n            \"dataset\": md[\"dataset\"],\n            \"file\": path,\n            \"selected_events\": int(len(df_ev)),\n            \"entries_in_file\": int(len(ev)),\n            \"xsec\": md.get(\"xsec\", None),\n        })\n\n    # Export CSV for this dataset\n    if events_all:\n        df_all = pd.concat(events_all, ignore_index=True)\n        outfile = OUT_DIR / f\"{key}_processed.csv\"\n        df_all.to_csv(outfile, index=False)\n        print(f\"[OK] Wrote {len(df_all)} events -&gt; {outfile}\")\n\n# Save global results\nelapsed = time.monotonic() - t0\n\nwith open(OUT_HISTOS, \"wb\") as f:\n    pickle.dump(analysis.h, f, protocol=pickle.HIGHEST_PROTOCOL)\n\npd.DataFrame(list(analysis.cut_flow.items()), columns=[\"cut\",\"events\"]).to_csv(OUT_CUTFLOW, index=False)\npd.DataFrame(nevents_rows).to_csv(OUT_NEVENTS, index=False)\n\nprint(f\"\\n[OK] Total entries processed: {entries_total}\")\nprint(f\"[OK] Global cutflow:\\n{analysis.cut_flow}\")\nprint(f\"[timing] Elapsed: {elapsed:.2f} s\")\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#visualization-raw-event-counts","title":"Visualization: Raw Event Counts","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport pandas as pd\nimport mplhep as hep\n\n# Apply CMS style\nplt.style.use(hep.style.CMS)\n\n# Load histograms and nevents\ndf_nevents = pd.read_csv(OUT_NEVENTS)\n\nwith open(OUT_HISTOS, \"rb\") as f:\n    hdict = pickle.load(f)\n\ndef plot_all_variables_raw(hdict, lumi_fb):\n    \"\"\"\n    Plot all variables without any normalization:\n    - MET as data (points with error bars).\n    - All other datasets as MC (stacked histograms).\n    \"\"\"\n    variables = list(hdict.keys())\n    print(\"Available variables:\", variables)\n\n    for var in variables:\n        h = hdict[var]\n        processes = list(h.axes[\"process\"])\n        edges = h.axes[\"var\"].edges\n        centers = 0.5*(edges[:-1] + edges[1:])\n        width = np.diff(edges)\n\n        # Explicitly define data vs MC\n        data_procs = [\"MET\"]\n        mc_procs = [p for p in processes if p not in data_procs]\n\n        plt.figure(figsize=(7,5))\n        bottom = np.zeros(len(edges)-1)\n\n        # --- MC stacked ---\n        for proc in mc_procs:\n            vals = h[{\"process\": proc}].values().ravel()\n            if np.any(vals):\n                plt.bar(edges[:-1], vals, width=width, bottom=bottom,\n                       align=\"edge\", alpha=0.7, label=proc)\n                bottom += vals\n\n        # --- Data as points ---\n        for proc in data_procs:\n            if proc in processes:\n                vals = h[{\"process\": proc}].values().ravel()\n                if np.any(vals):\n                    yerr = np.sqrt(vals)\n                    plt.errorbar(centers, vals, yerr=yerr,\n                               fmt=\"o\", color=\"black\", label=proc)\n\n        plt.xlabel(var)\n        plt.ylabel(\"Raw events\")\n        plt.legend(fontsize=8, frameon=False)\n        plt.grid(True, linestyle=\"--\", alpha=0.4)\n        plt.tight_layout()\n        plt.show()\n\n# === Run ===\nplot_all_variables_raw(hdict, LUMI_FB)\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#cross-sections-and-normalization","title":"Cross-Sections and Normalization","text":""},{"location":"All_Hadronic/all%20hadronic%20si/#why-normalization-is-necessary","title":"Why Normalization is Necessary","text":"<p>When comparing data and Monte Carlo (MC) simulations, the raw event counts are not directly comparable:</p> <p>\u2022 Data:   - Events are collected with a detector during a given time period.   - The \"size\" of the dataset is controlled by the integrated luminosity (L).   - Example: UL2016 MET dataset corresponds to ~35.9 fb\u207b\u00b9.   - There is no cross section attached \u2014 it is just what was recorded.</p> <p>\u2022 MC (simulations):   - Each simulated dataset corresponds to a particular physics process (e.g. \\(t\\bar{t}\\), W+jets).   - Generators simulate a finite number of events (N_gen) with a known theoretical cross section (\u03c3).   - By construction, MC samples may represent more or fewer events than would be seen in real data.   - Therefore, they must be scaled.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#the-normalization-formula","title":"The Normalization Formula","text":"<p>To make MC comparable to data, we apply a per-event weight:</p> \\[ w = \\frac{\\sigma \\cdot L}{N_\\text{gen}} \\] <p>Where:</p> <p>\u2022 \u03c3 (pb) = process cross section (from theory/measurements). \u2022 L (fb\u207b\u00b9) = integrated luminosity of the data sample.   - Convert to pb\u207b\u00b9 by multiplying by 1000. \u2022 N_gen = total number of generated MC events (before cuts).</p> <p>This weight ensures that when we sum the MC events after applying cuts, the histograms reflect the expected yield in the same luminosity as the data.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#step-1-define-cross-sections","title":"Step 1 \u2014 Define Cross-Sections","text":"<p>For each simulated process (MC), we need the theoretical cross section (\u03c3) in pb. This will later be combined with the luminosity (L) and the number of generated events (N_gen) to normalize MC histograms.</p> <pre><code># ================================\n# Cross-sections (pb) \u2014 official values\n# ================================\nXSEC_PB = {\n    # Data-like (no cross section)\n    (\"MET\", None): None,\n    (\"SingleMuon\", None): None,\n    (\"SingleElectron\", None): None,\n\n    # ttbar\n    (\"ttbar-semileptonic\", None): 831.76,\n    (\"ttbar-hadronic\", None): 831.76,\n\n    # single top\n    (\"t-channel-top\", None): 136.0,\n    (\"t-channel-antitop\", None): 81.0,\n    (\"tW-top\", None): 71.7,\n\n    # ttV\n    (\"ttW\", None): 0.204,\n    (\"ttZ\", None): 0.252,\n\n    # W+jets\n    (\"WJets-HT400to600\", None): 48.9,\n    (\"WJets-2J-FxFx\", None): 615.7,\n\n    # DY+jets\n    (\"DYJets-inclusive\", None): 6025.0,\n    (\"DYJets-Zpt200\", None): 1.27,\n\n    # Diboson + Z\u2192\u03bd\u03bd\n    (\"Zvv\", None): 77.3,\n    (\"WW\", None): 118.7,\n    (\"ZZ\", None): 16.6,\n}\n\ndef get_xsec(proc: str, subgroup: str | None = None):\n    \"\"\"\n    Return cross-section (pb) for (proc, subgroup).\n    Data-like samples (MET, SingleMuon, SingleElectron) return None.\n    \"\"\"\n    key = (proc, subgroup)\n    if key in XSEC_PB:\n        return XSEC_PB[key]\n    return XSEC_PB.get((proc, None), None)\n\nprint(\"=== Cross-sections (pb) ===\")\nfor (proc, subgroup), xsec in XSEC_PB.items():\n    name = proc if subgroup is None else f\"{proc} ({subgroup})\"\n    print(f\"{name:20s} : {xsec}\")\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#step-2-count-generated-events","title":"Step 2 \u2014 Count Generated Events","text":"<p>We need to know how many events were generated (N_gen) for each dataset. This is critical to compute normalization factors.</p> <pre><code>import uproot\n\ndef count_events_one_file(root_path):\n    try:\n        with uproot.open(root_path) as f:\n            return f[\"Events\"].num_entries\n    except Exception as e:\n        print(f\"[warn] could not open {root_path} -&gt; {e}\")\n        return 0\n\n# Dictionary to store counts\nevents_count = {}\n\nfor key, pack in fileset.items():\n    fpaths = pack[\"files\"][:N_FILES_MAX_PER_SAMPLE]\n    if not fpaths:\n        continue\n\n    total_events = 0\n    file_events = []\n\n    for path in fpaths:\n        nevts = count_events_one_file(path)\n        total_events += nevts\n        file_events.append((path, nevts))\n        print(f\"{key}: {path}, events={nevts}\")\n\n    events_count[key] = {\n        \"files\": fpaths,\n        \"file_events\": file_events,\n        \"total_events\": total_events,\n    }\n\n    print(f\"--&gt; {key}: total_events={total_events}\\n\")\n\n# --- Final summary ---\nprint(\"\\nSummary (per dataset):\")\ngrand_total = 0\nfor k, v in events_count.items():\n    print(f\"  {k:25s} \u2192 total_events={v['total_events']} (from {len(v['files'])} file(s))\")\n    grand_total += v[\"total_events\"]\n\nprint(f\"\\n[OK] Grand total across datasets = {grand_total}\")\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#normalized-plots","title":"Normalized Plots","text":"<pre><code># ================================\n# Normalization + CMS-style plots\n# ================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\n# Set your analysis luminosity (fb^-1)\nLUMI_FB = 0.01\n\n# Load histograms\nwith open(OUT_HISTOS, \"rb\") as f:\n    hdict = pickle.load(f)\n\n# Explicit N_gen dictionary (truth-level generated events)\nN_EVENTS_GEN = {\n    \"MET\": 1654969,\n    \"SingleMuon\": 14113,\n    \"SingleElectron\": 2338304,\n    \"ttbar-semileptonic\": 1233000,\n    \"ttbar-hadronic\": 1344000,\n    \"t-channel-top\": 168000,\n    \"ttW\": 9451,\n    \"WJets-HT400to600\": 41364,\n    \"WJets-2J-FxFx\": 1766801,\n    \"DYJets-Zpt200\": 9748,\n    \"WW\": 2016000,\n    \"ZZ\": 4000,\n    \"Zvv\": 932,\n}\n\ndef get_neq_gen(proc: str) -&gt; int|None:\n    \"\"\"N_gen = from fixed dictionary.\"\"\"\n    return N_EVENTS_GEN.get(proc, None)\n\ndef norm_factor(proc: str, lumi_fb: float) -&gt; float:\n    if proc in (\"SingleMuon\", \"SingleElectron\", \"MET\"):\n        return 1.0\n\n    xsec = get_xsec(proc)\n    ngen = get_neq_gen(proc)\n\n    if (xsec is None) or (ngen is None) or (ngen &lt;= 0):\n        return 1.0\n\n    return (xsec * lumi_fb * 1e3) / float(ngen)\n\ndef build_norm_report(hdict, lumi_fb):\n    some_var = next(iter(hdict.keys()))\n    processes = list(hdict[some_var].axes[\"process\"])\n\n    rows = []\n    for proc in processes:\n        xsec = get_xsec(proc)\n        ngen = get_neq_gen(proc)\n        factor = norm_factor(proc, lumi_fb)\n\n        rows.append({\n            \"process\": proc,\n            \"xsec_pb\": xsec,\n            \"lumi_fb\": lumi_fb if xsec is not None else None,\n            \"N_gen_used\": ngen,\n            \"scale_factor\": factor,\n            \"is_data\": proc in (\"SingleMuon\", \"SingleElectron\", \"MET\"),\n        })\n\n    return pd.DataFrame(rows)\n\ndef plot_all_variables_normalized(hdict, lumi_fb):\n    rep = build_norm_report(hdict, lumi_fb)\n    print(\"\\n=== Normalization report (All-Hadronic Channel) ===\")\n    print(rep.to_string(index=False))\n\n    variables = list(hdict.keys())\n\n    for var in variables:\n        h = hdict[var]\n        processes = list(h.axes[\"process\"])\n        edges = h.axes[\"var\"].edges\n        centers = 0.5*(edges[:-1] + edges[1:])\n        width = np.diff(edges)\n\n        data_procs = [p for p in processes if p in (\"MET\", \"SingleMuon\", \"SingleElectron\")]\n        mc_procs = [p for p in processes if p not in data_procs]\n\n        plt.figure(figsize=(7,5))\n        bottom = np.zeros(len(edges)-1)\n\n        for proc in mc_procs:\n            vals = h[{\"process\": proc}].values().ravel()\n            vals *= norm_factor(proc, lumi_fb)\n            if np.any(vals):\n                plt.bar(edges[:-1], vals, width=width, bottom=bottom,\n                       align=\"edge\", alpha=0.7, label=proc)\n                bottom += vals\n\n        for proc in data_procs:\n            vals = h[{\"process\": proc}].values().ravel()\n            if np.any(vals):\n                yerr = np.sqrt(vals)\n                plt.errorbar(centers, vals, yerr=yerr, fmt=\"o\", color=\"black\", label=proc)\n\n        plt.xlabel(var)\n        plt.ylabel(\"Events (MC scaled, Data raw)\")\n        plt.legend(fontsize=8, frameon=False)\n        plt.grid(True, linestyle=\"--\", alpha=0.4)\n        plt.tight_layout()\n        plt.show()\n\n# === Run ===\nplot_all_variables_normalized(hdict, LUMI_FB)\n</code></pre> <p>This completes the all-hadronic channel analysis workflow with proper event selection, MET-based triggers, lepton veto, angular cuts to suppress QCD, and normalized MC-to-data comparisons.  </p>"},{"location":"Single_Lepton/Single_lepton_comment/","title":"Single Lepton","text":""},{"location":"Single_Lepton/Single_lepton_comment/#physics-motivation-and-channel-strategy","title":"Physics Motivation and Channel Strategy","text":"<p>The Large Hadron Collider (LHC) collides protons at center-of-mass energies high enough to probe physics beyond the Standard Model. Although the protons are composite objects, the relevant hard scatterings occur between their constituents \u2014 quarks and gluons. In the context of simplified dark matter models, these partonic interactions can produce top quarks together with a new mediator particle (commonly denoted \u03c6 for scalar or a for pseudoscalar). The mediator then decays invisibly into a pair of dark matter candidates (\\(\\chi \\bar{\\chi}\\)). At the detector level, this results in events with multiple top quarks plus significant missing transverse momentum (\\(p_T^{\\text{miss}}\\)), the latter coming from both neutrinos and the invisible \u03c7 particles.</p> <p>The production mechanisms of interest include:  </p> <ul> <li> <p>Gluon fusion:   $$ gg \\to t \\bar{t}\\,\\phi \\to t \\bar{t} + \\chi \\bar{\\chi} $$  </p> </li> <li> <p>Single top associated production:   $$ gb \\to t \\phi \\to t + \\chi \\bar{\\chi} $$  </p> </li> <li> <p>t\u2013channel production:   $$ qq' \\to tb \\phi \\to tb + \\chi \\bar{\\chi} $$  </p> </li> </ul> <p>In all cases, the top quarks decay via \\(t \\to W b\\). Each W boson subsequently decays either leptonically (\\(W \\to \\ell \\nu\\)) or hadronically (\\(W \\to q \\bar{q}'\\)). Thus, the final states contain a mixture of b-tagged jets, light-flavor jets, charged leptons (electrons or muons), and genuine \\(p_T^{\\text{miss}}\\). The specific experimental signature depends strongly on the decay mode of the W bosons.</p> <p>Because of this, analyses are divided into channels, each defined by the number of isolated charged leptons:</p> <ul> <li>Single-lepton (SL): one isolated electron or muon, several jets (including \u22651 b-tag), and nonzero \\(p_T^{\\text{miss}}\\). This channel is statistically powerful and relatively clean, striking a balance between signal sensitivity and manageable backgrounds.  </li> <li>All-hadronic (AH): no isolated leptons, many jets including b-tagged jets, and \\(p_T^{\\text{miss}}\\). While it has the largest raw yield, it suffers from overwhelming QCD multijet background, which can fake \\(p_T^{\\text{miss}}\\).  </li> <li>Dilepton: two isolated leptons, large \\(p_T^{\\text{miss}}\\), and multiple jets. It provides a very clean signal region but is limited by low branching fraction, hence low statistics.</li> </ul> <p>In this notebook, we concentrate on the single-lepton channel with exactly one muon. There are both theoretical and practical reasons for this choice. From the physics side, the SL channel has the right compromise: it suppresses pure QCD while retaining enough events to make meaningful comparisons. From the experimental side, single-muon triggers are robust, well understood in CMS, and ensure efficient data collection. This focus allows us to demonstrate the full workflow \u2014 from event selection to histograms \u2014 in a setting where the interplay between signal characteristics and background processes can be clearly explained. Splitting into channels is therefore not a stylistic decision but a physics necessity: each final state probes the same underlying processes under different background conditions and detector signatures.</p> <p>After defining the objective of the project (Reproducible Analysis of CMS Open Data: Search for Dark Matter in Association with Top Quarks), we discussed in which data format to work \u2014 NanoAOD or MiniAOD. We decided to use NanoAOD, because it is lighter and optimized for analysis tasks.</p> <p>For this reason, the environment is built with Docker, using the image:</p> <p>cmscloud/python-vnc</p> <p>and all development is done inside JupyterLab.</p>"},{"location":"Single_Lepton/Single_lepton_comment/#single-lepton-channel","title":"Single lepton channel","text":""},{"location":"Single_Lepton/Single_lepton_comment/#install-and-upgrade-libraries","title":"Install and upgrade libraries","text":"<p>First we install and upgrade the libraries that would be used to analyze the data.</p> <pre><code>!pip install --upgrade pip\n\n!pip install --upgrade awkward\n!pip install --upgrade uproot\n\n!pip install fsspec-xrootd\n\n!pip install vector\n\n!pip install --upgrade pandas\n\n\n!pip install --upgrade matplotlib\n\n</code></pre> <pre><code>Requirement already satisfied: pip in /usr/local/venv/lib/python3.10/site-packages (25.2)\nRequirement already satisfied: awkward in /usr/local/venv/lib/python3.10/site-packages (2.8.8)\nCollecting awkward\n  Downloading awkward-2.8.9-py3-none-any.whl.metadata (7.5 kB)\nCollecting awkward-cpp==50 (from awkward)\n  Downloading awkward_cpp-50-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: fsspec&gt;=2022.11.0 in /usr/local/venv/lib/python3.10/site-packages (from awkward) (2025.2.0)\nRequirement already satisfied: importlib-metadata&gt;=4.13.0 in /usr/local/venv/lib/python3.10/site-packages (from awkward) (8.6.1)\nRequirement already satisfied: numpy&gt;=1.18.0 in /usr/local/venv/lib/python3.10/site-packages (from awkward) (2.2.3)\nRequirement already satisfied: packaging in /usr/local/venv/lib/python3.10/site-packages (from awkward) (24.2)\nRequirement already satisfied: typing-extensions&gt;=4.1.0 in /usr/local/venv/lib/python3.10/site-packages (from awkward) (4.12.2)\nRequirement already satisfied: zipp&gt;=3.20 in /usr/local/venv/lib/python3.10/site-packages (from importlib-metadata&gt;=4.13.0-&gt;awkward) (3.21.0)\nDownloading awkward-2.8.9-py3-none-any.whl (903 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m903.5/903.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\nDownloading awkward_cpp-50-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (653 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m653.7/653.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\nInstalling collected packages: awkward-cpp, awkward\n\u001b[2K  Attempting uninstall: awkward-cpp\n\u001b[2K    Found existing installation: awkward_cpp 49\n\u001b[2K    Uninstalling awkward_cpp-49:\n\u001b[2K      Successfully uninstalled awkward_cpp-49\n\u001b[2K  Attempting uninstall: awkward\n\u001b[2K    Found existing installation: awkward 2.8.8\n\u001b[2K    Uninstalling awkward-2.8.8:\n\u001b[2K      Successfully uninstalled awkward-2.8.8\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2/2\u001b[0m [awkward]m1/2\u001b[0m [awkward]\nSuccessfully installed awkward-2.8.9 awkward-cpp-50\nRequirement already satisfied: uproot in /usr/local/venv/lib/python3.10/site-packages (5.6.5)\nCollecting uproot\n  Downloading uproot-5.6.6-py3-none-any.whl.metadata (34 kB)\nRequirement already satisfied: awkward&gt;=2.4.6 in /usr/local/venv/lib/python3.10/site-packages (from uproot) (2.8.9)\nRequirement already satisfied: cramjam&gt;=2.5.0 in /usr/local/venv/lib/python3.10/site-packages (from uproot) (2.9.1)\nRequirement already satisfied: fsspec!=2025.7.0 in /usr/local/venv/lib/python3.10/site-packages (from uproot) (2025.2.0)\nRequirement already satisfied: numpy in /usr/local/venv/lib/python3.10/site-packages (from uproot) (2.2.3)\nRequirement already satisfied: packaging in /usr/local/venv/lib/python3.10/site-packages (from uproot) (24.2)\nRequirement already satisfied: typing-extensions&gt;=4.1.0 in /usr/local/venv/lib/python3.10/site-packages (from uproot) (4.12.2)\nRequirement already satisfied: xxhash in /usr/local/venv/lib/python3.10/site-packages (from uproot) (3.5.0)\nRequirement already satisfied: awkward-cpp==50 in /usr/local/venv/lib/python3.10/site-packages (from awkward&gt;=2.4.6-&gt;uproot) (50)\nRequirement already satisfied: importlib-metadata&gt;=4.13.0 in /usr/local/venv/lib/python3.10/site-packages (from awkward&gt;=2.4.6-&gt;uproot) (8.6.1)\nRequirement already satisfied: zipp&gt;=3.20 in /usr/local/venv/lib/python3.10/site-packages (from importlib-metadata&gt;=4.13.0-&gt;awkward&gt;=2.4.6-&gt;uproot) (3.21.0)\nDownloading uproot-5.6.6-py3-none-any.whl (385 kB)\nInstalling collected packages: uproot\n  Attempting uninstall: uproot\n    Found existing installation: uproot 5.6.5\n    Uninstalling uproot-5.6.5:\n      Successfully uninstalled uproot-5.6.5\nSuccessfully installed uproot-5.6.6\nRequirement already satisfied: fsspec-xrootd in /usr/local/venv/lib/python3.10/site-packages (0.5.0)\nRequirement already satisfied: fsspec in /usr/local/venv/lib/python3.10/site-packages (from fsspec-xrootd) (2025.2.0)\nRequirement already satisfied: vector in /usr/local/venv/lib/python3.10/site-packages (1.6.3)\nRequirement already satisfied: numpy&gt;=1.13.3 in /usr/local/venv/lib/python3.10/site-packages (from vector) (2.2.3)\nRequirement already satisfied: packaging&gt;=19 in /usr/local/venv/lib/python3.10/site-packages (from vector) (24.2)\nRequirement already satisfied: pandas in /usr/local/venv/lib/python3.10/site-packages (2.3.2)\nCollecting pandas\n  Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\nRequirement already satisfied: numpy&gt;=1.22.4 in /usr/local/venv/lib/python3.10/site-packages (from pandas) (2.2.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/venv/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/venv/lib/python3.10/site-packages (from pandas) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/venv/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\nDownloading pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\nInstalling collected packages: pandas\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.3.2\n    Uninstalling pandas-2.3.2:\n      Successfully uninstalled pandas-2.3.2\nSuccessfully installed pandas-2.3.3\nRequirement already satisfied: matplotlib in /usr/local/venv/lib/python3.10/site-packages (3.10.6)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy&gt;=1.23 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (2.2.3)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow&gt;=8 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/venv/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.17.0)\n</code></pre> <p>We import all the libraries that we are going to use.</p> <pre><code>%load_ext autoreload\n%autoreload 2\n\n# The classics\nimport numpy as np\nimport matplotlib.pylab as plt\nimport matplotlib # To get the version\n\nimport pandas as pd\n\n# The newcomers\nimport awkward as ak\nimport uproot\n\nimport vector\nvector.register_awkward()\n\nimport requests\nimport os\n\nimport time\n\nimport json\n\n\n%load_ext autoreload\n%autoreload 2\n\n# The classics\nimport numpy as np\nimport matplotlib.pylab as plt\nimport matplotlib # To get the version\n\nimport pandas as pd\n\nimport hist\nfrom hist import Hist\n\n# To read file names using the OS (operating system)\nimport glob\nimport os\n\nimport dpoa_workshop_utilities\nfrom dpoa_workshop_utilities import nanoaod_filenames\nfrom dpoa_workshop_utilities import get_files_for_dataset\nfrom dpoa_workshop_utilities import pretty_print\nfrom dpoa_workshop_utilities import build_lumi_mask\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <p>We are going to use the <code>dpoa_workshop_utilities</code> file to help you access the datasets.</p> <p>The <code>nanoaod_filenames</code> is a dictionary with the urls to the file indexes of the root files for every dataset that we will use in the analysis.</p> <p>The <code>get_files_for_dataset(dataset,radom,n)</code> is a function that returs the <code>n</code> root directions of the specified <code>dataset</code>, the <code>random</code> is set <code>False</code> by default and it allows you to select random urls.</p> <p>The <code>pretty_print(fields, fmt='40s', require=None, ignore=None)</code> function allows you to print subsets of keys based on strings that you require or ignore. It will also format that output based on how many characters you want in a column (you are limited to 80 characters per line).</p> <p>The <code>build_lumi_mask(lumifile, tree, verbose=False)</code> function helps you mask (select) the data that's collected from collisions.</p> <p>In the next seccion will download important files like the luminosity file.</p> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(filename=\"dataset_2016.png\"))\n\n</code></pre> <p></p> <pre><code>\ndisplay(Image(filename=\"MC_data.png\"))\n\n</code></pre> <p></p>"},{"location":"Single_Lepton/Single_lepton_comment/#download-the-essential-files","title":"Download the essential files","text":"<p>The following code uses these utilities to access the location of the ROOT files of every dataset used in the analysis. We download and create a file with all the ROOT's urls for every dataset.</p> <p>We assign a name similar to <code>FILE_LIST_ttsemilep.txt</code> to these files.</p> <pre><code># Descarga de las direcciones root para cada grupo de datos\n\nfor datasetname in nanoaod_filenames.keys():\n\n    print(datasetname)\n\n    outfilename = f'FILE_LIST_{datasetname}.txt'\n\n    # Remove the file if it exists\n    try:\n        os.remove(outfilename)\n    except OSError:\n        pass\n\n    for url in nanoaod_filenames[datasetname]:\n        print(url)\n\n        r = requests.get(url, allow_redirects=True)\n\n        open(outfilename, 'a').write(r.text)\n\n</code></pre> <pre><code>ttsemilep\nhttps://opendata.cern.ch/record/67993/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_120000_file_index.txt\nhttps://opendata.cern.ch/record/67993/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_130000_file_index.txt\nhttps://opendata.cern.ch/record/67993/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_270000_file_index.txt\nhttps://opendata.cern.ch/record/67993/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_280000_file_index.txt\nhttps://opendata.cern.ch/record/67993/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_70000_file_index.txt\nttop\nhttps://opendata.cern.ch/record/64763/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_ST_t-channel_top_4f_InclusiveDecays_TuneCP5CR2_13TeV-powheg-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_100000_file_index.txt\nhttps://opendata.cern.ch/record/64763/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_ST_t-channel_top_4f_InclusiveDecays_TuneCP5CR2_13TeV-powheg-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_230000_file_index.txt\nhttps://opendata.cern.ch/record/64763/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_ST_t-channel_top_4f_InclusiveDecays_TuneCP5CR2_13TeV-powheg-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2430000_file_index.txt\nhttps://opendata.cern.ch/record/64763/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_ST_t-channel_top_4f_InclusiveDecays_TuneCP5CR2_13TeV-powheg-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_2500000_file_index.txt\nhttps://opendata.cern.ch/record/64763/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_ST_t-channel_top_4f_InclusiveDecays_TuneCP5CR2_13TeV-powheg-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_250000_file_index.txt\nWjets\nhttps://opendata.cern.ch/record/69729/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_HT-400To600_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_120000_file_index.txt\nhttps://opendata.cern.ch/record/69729/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_HT-400To600_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_130000_file_index.txt\nhttps://opendata.cern.ch/record/69729/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_HT-400To600_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_270000_file_index.txt\nhttps://opendata.cern.ch/record/69729/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_HT-400To600_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_280000_file_index.txt\nhttps://opendata.cern.ch/record/69729/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_WJetsToLNu_HT-400To600_TuneCP5_13TeV-madgraphMLM-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_70000_file_index.txt\nttv\nhttps://opendata.cern.ch/record/68073/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTWJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_130000_file_index.txt\nhttps://opendata.cern.ch/record/68073/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTWJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_270000_file_index.txt\nhttps://opendata.cern.ch/record/68073/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTWJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_280000_file_index.txt\nhttps://opendata.cern.ch/record/68073/file_index/CMS_mc_RunIISummer20UL16NanoAODv9_TTWJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-madspin-pythia8_NANOAODSIM_106X_mcRun2_asymptotic_v17-v1_70000_file_index.txt\ncollmet\nhttps://opendata.cern.ch/record/30559/file_index/CMS_Run2016H_MET_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_120000_file_index.txt\nhttps://opendata.cern.ch/record/30559/file_index/CMS_Run2016H_MET_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_280000_file_index.txt\nhttps://opendata.cern.ch/record/30559/file_index/CMS_Run2016H_MET_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_70000_file_index.txt\ncollsm\nhttps://opendata.cern.ch/record/30563/file_index/CMS_Run2016H_SingleMuon_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_120000_file_index.txt\nhttps://opendata.cern.ch/record/30563/file_index/CMS_Run2016H_SingleMuon_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_1210000_file_index.txt\nhttps://opendata.cern.ch/record/30563/file_index/CMS_Run2016H_SingleMuon_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_130000_file_index.txt\nhttps://opendata.cern.ch/record/30563/file_index/CMS_Run2016H_SingleMuon_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_280000_file_index.txt\nhttps://opendata.cern.ch/record/30563/file_index/CMS_Run2016H_SingleMuon_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_70000_file_index.txt\ncollse\nhttps://opendata.cern.ch/record/30562/file_index/CMS_Run2016H_SingleElectron_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_120000_file_index.txt\nhttps://opendata.cern.ch/record/30562/file_index/CMS_Run2016H_SingleElectron_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_270000_file_index.txt\nhttps://opendata.cern.ch/record/30562/file_index/CMS_Run2016H_SingleElectron_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_280000_file_index.txt\nhttps://opendata.cern.ch/record/30562/file_index/CMS_Run2016H_SingleElectron_NANOAOD_UL2016_MiniAODv2_NanoAODv9-v1_70000_file_index.txt\n</code></pre> <p>By running the next code we can count the number of ROOT files that are in every dataset.</p> <pre><code>!wc -l FILE_LIST_*.txt\n\n</code></pre> <pre><code>   11 FILE_LIST_Wjets.txt\n   32 FILE_LIST_collmet.txt\n   80 FILE_LIST_collse.txt\n   82 FILE_LIST_collsm.txt\n   25 FILE_LIST_ttop.txt\n  138 FILE_LIST_ttsemilep.txt\n   12 FILE_LIST_ttv.txt\n  380 total\n</code></pre> <p>Using the <code>get_files_for_dataset</code> function we can get <code>n</code> number of ROOT files of a dataset, in this case the <code>ttsemilep</code> dataset.</p> <pre><code># Pick 2 tt\u0304 semileptonic files\nfilenames = get_files_for_dataset(\"ttsemilep\",random=True, n=2)\n\nfor f in filenames:\n    print(f)\n\n</code></pre> <pre><code>root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/270000/CBA39E34-6BAD-B740-AB89-ECD745490CE5.root\n\nroot://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/70000/7C94DA4D-C0B5-8747-B6F8-E4903C610777.root\n</code></pre> <p>In order to work with the collisions datasets it is necesary to apply a luminosity filter which you can download running the next cell.</p> <pre><code>!wget https://opendata.cern.ch/record/14220/files/Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\n\n</code></pre> <pre><code>--2025-10-02 21:34:08--  https://opendata.cern.ch/record/14220/files/Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\n137.138.6.31, 2001:1458:201:8b::100:1c8rn.ch)... \nconnected. to opendata.cern.ch (opendata.cern.ch)|137.138.6.31|:443... \nHTTP request sent, awaiting response... 200 OK\nLength: 11686 (11K) [text/plain]\nSaving to: \u2018Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\u2019\n\nCert_271036-284044_ 100%[===================&gt;]  11.41K  --.-KB/s    in 0s\n\n2025-10-02 21:34:09 (91.7 MB/s) - \u2018Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\u2019 saved [11686/11686]\n</code></pre>"},{"location":"Single_Lepton/Single_lepton_comment/#selecting-the-data","title":"Selecting the data","text":"<p>We will work with a random file from the <code>ttsemilp</code> dataset.</p> <pre><code>filename = filenames[0]\nprint(f\"Opening...{filename}\")\nf = uproot.open(filename)\n\nevents = f['Events']\nnevents = events.num_entries\n\nprint(f\"{nevents = }\")\n</code></pre> <pre><code>Opening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/270000/CBA39E34-6BAD-B740-AB89-ECD745490CE5.root\n\nnevents = 810000\n</code></pre> <p>We can see the keys with</p> <pre><code>print(events.keys())\n</code></pre> <pre><code>['run', 'luminosityBlock', 'event', 'HTXS_Higgs_pt', 'HTXS_Higgs_y', 'HTXS_stage1_1_cat_pTjet25GeV', 'HTXS_stage1_1_cat_pTjet30GeV', 'HTXS_stage1_1_fine_cat_pTjet25GeV', 'HTXS_stage1_1_fine_cat_pTjet30GeV', 'HTXS_stage1_2_cat_pTjet25GeV', 'HTXS_stage1_2_cat_pTjet30GeV', 'HTXS_stage1_2_fine_cat_pTjet25GeV', 'HTXS_stage1_2_fine_cat_pTjet30GeV', 'HTXS_stage_0', 'HTXS_stage_1_pTjet25', 'HTXS_stage_1_pTjet30', 'HTXS_njets25', 'HTXS_njets30', 'nboostedTau', 'boostedTau_chargedIso', 'boostedTau_eta', 'boostedTau_leadTkDeltaEta', 'boostedTau_leadTkDeltaPhi', 'boostedTau_leadTkPtOverTauPt', 'boostedTau_mass', 'boostedTau_neutralIso', 'boostedTau_phi', 'boostedTau_photonsOutsideSignalCone', 'boostedTau_pt', 'boostedTau_puCorr', 'boostedTau_rawAntiEle2018', 'boostedTau_rawIso', 'boostedTau_rawIsodR03', 'boostedTau_rawMVAnewDM2017v2', 'boostedTau_rawMVAoldDM2017v2', 'boostedTau_rawMVAoldDMdR032017v2', 'boostedTau_charge', 'boostedTau_decayMode', 'boostedTau_jetIdx', 'boostedTau_rawAntiEleCat2018', 'boostedTau_idAntiEle2018', 'boostedTau_idAntiMu', 'boostedTau_idMVAnewDM2017v2', 'boostedTau_idMVAoldDM2017v2', 'boostedTau_idMVAoldDMdR032017v2', 'btagWeight_CSVV2', 'btagWeight_DeepCSVB', 'CaloMET_phi', 'CaloMET_pt', 'CaloMET_sumEt', 'ChsMET_phi', 'ChsMET_pt', 'ChsMET_sumEt', 'nCorrT1METJet', 'CorrT1METJet_area', 'CorrT1METJet_eta', 'CorrT1METJet_muonSubtrFactor', 'CorrT1METJet_phi', 'CorrT1METJet_rawPt', 'DeepMETResolutionTune_phi', 'DeepMETResolutionTune_pt', 'DeepMETResponseTune_phi', 'DeepMETResponseTune_pt', 'nElectron', 'Electron_dEscaleDown', 'Electron_dEscaleUp', 'Electron_dEsigmaDown', 'Electron_dEsigmaUp', 'Electron_deltaEtaSC', 'Electron_dr03EcalRecHitSumEt', 'Electron_dr03HcalDepth1TowerSumEt', 'Electron_dr03TkSumPt', 'Electron_dr03TkSumPtHEEP', 'Electron_dxy', 'Electron_dxyErr', 'Electron_dz', 'Electron_dzErr', 'Electron_eCorr', 'Electron_eInvMinusPInv', 'Electron_energyErr', 'Electron_eta', 'Electron_hoe', 'Electron_ip3d', 'Electron_jetPtRelv2', 'Electron_jetRelIso', 'Electron_mass', 'Electron_miniPFRelIso_all', 'Electron_miniPFRelIso_chg', 'Electron_mvaFall17V2Iso', 'Electron_mvaFall17V2noIso', 'Electron_pfRelIso03_all', 'Electron_pfRelIso03_chg', 'Electron_phi', 'Electron_pt', 'Electron_r9', 'Electron_scEtOverPt', 'Electron_sieie', 'Electron_sip3d', 'Electron_mvaTTH', 'Electron_charge', 'Electron_cutBased', 'Electron_jetIdx', 'Electron_pdgId', 'Electron_photonIdx', 'Electron_tightCharge', 'Electron_vidNestedWPBitmap', 'Electron_vidNestedWPBitmapHEEP', 'Electron_convVeto', 'Electron_cutBased_HEEP', 'Electron_isPFcand', 'Electron_jetNDauCharged', 'Electron_lostHits', 'Electron_mvaFall17V2Iso_WP80', 'Electron_mvaFall17V2Iso_WP90', 'Electron_mvaFall17V2Iso_WPL', 'Electron_mvaFall17V2noIso_WP80', 'Electron_mvaFall17V2noIso_WP90', 'Electron_mvaFall17V2noIso_WPL', 'Electron_seedGain', 'nFatJet', 'FatJet_area', 'FatJet_btagCSVV2', 'FatJet_btagDDBvLV2', 'FatJet_btagDDCvBV2', 'FatJet_btagDDCvLV2', 'FatJet_btagDeepB', 'FatJet_btagHbb', 'FatJet_deepTagMD_H4qvsQCD', 'FatJet_deepTagMD_HbbvsQCD', 'FatJet_deepTagMD_TvsQCD', 'FatJet_deepTagMD_WvsQCD', 'FatJet_deepTagMD_ZHbbvsQCD', 'FatJet_deepTagMD_ZHccvsQCD', 'FatJet_deepTagMD_ZbbvsQCD', 'FatJet_deepTagMD_ZvsQCD', 'FatJet_deepTagMD_bbvsLight', 'FatJet_deepTagMD_ccvsLight', 'FatJet_deepTag_H', 'FatJet_deepTag_QCD', 'FatJet_deepTag_QCDothers', 'FatJet_deepTag_TvsQCD', 'FatJet_deepTag_WvsQCD', 'FatJet_deepTag_ZvsQCD', 'FatJet_eta', 'FatJet_mass', 'FatJet_msoftdrop', 'FatJet_n2b1', 'FatJet_n3b1', 'FatJet_particleNetMD_QCD', 'FatJet_particleNetMD_Xbb', 'FatJet_particleNetMD_Xcc', 'FatJet_particleNetMD_Xqq', 'FatJet_particleNet_H4qvsQCD', 'FatJet_particleNet_HbbvsQCD', 'FatJet_particleNet_HccvsQCD', 'FatJet_particleNet_QCD', 'FatJet_particleNet_TvsQCD', 'FatJet_particleNet_WvsQCD', 'FatJet_particleNet_ZvsQCD', 'FatJet_particleNet_mass', 'FatJet_phi', 'FatJet_pt', 'FatJet_rawFactor', 'FatJet_tau1', 'FatJet_tau2', 'FatJet_tau3', 'FatJet_tau4', 'FatJet_lsf3', 'FatJet_jetId', 'FatJet_subJetIdx1', 'FatJet_subJetIdx2', 'FatJet_electronIdx3SJ', 'FatJet_muonIdx3SJ', 'FatJet_nConstituents', 'nFsrPhoton', 'FsrPhoton_dROverEt2', 'FsrPhoton_eta', 'FsrPhoton_phi', 'FsrPhoton_pt', 'FsrPhoton_relIso03', 'FsrPhoton_muonIdx', 'nGenJetAK8', 'GenJetAK8_eta', 'GenJetAK8_mass', 'GenJetAK8_phi', 'GenJetAK8_pt', 'nGenJet', 'GenJet_eta', 'GenJet_mass', 'GenJet_phi', 'GenJet_pt', 'nGenPart', 'GenPart_eta', 'GenPart_mass', 'GenPart_phi', 'GenPart_pt', 'GenPart_genPartIdxMother', 'GenPart_pdgId', 'GenPart_status', 'GenPart_statusFlags', 'nSubGenJetAK8', 'SubGenJetAK8_eta', 'SubGenJetAK8_mass', 'SubGenJetAK8_phi', 'SubGenJetAK8_pt', 'Generator_binvar', 'Generator_scalePDF', 'Generator_weight', 'Generator_x1', 'Generator_x2', 'Generator_xpdf1', 'Generator_xpdf2', 'Generator_id1', 'Generator_id2', 'GenVtx_x', 'GenVtx_y', 'GenVtx_z', 'nGenVisTau', 'GenVisTau_eta', 'GenVisTau_mass', 'GenVisTau_phi', 'GenVisTau_pt', 'GenVisTau_charge', 'GenVisTau_genPartIdxMother', 'GenVisTau_status', 'genWeight', 'LHEWeight_originalXWGTUP', 'nLHEPdfWeight', 'LHEPdfWeight', 'nLHEReweightingWeight', 'LHEReweightingWeight', 'nLHEScaleWeight', 'LHEScaleWeight', 'nPSWeight', 'PSWeight', 'nIsoTrack', 'IsoTrack_dxy', 'IsoTrack_dz', 'IsoTrack_eta', 'IsoTrack_pfRelIso03_all', 'IsoTrack_pfRelIso03_chg', 'IsoTrack_phi', 'IsoTrack_pt', 'IsoTrack_miniPFRelIso_all', 'IsoTrack_miniPFRelIso_chg', 'IsoTrack_charge', 'IsoTrack_fromPV', 'IsoTrack_pdgId', 'IsoTrack_isHighPurityTrack', 'IsoTrack_isPFcand', 'IsoTrack_isFromLostTrack', 'nJet', 'Jet_area', 'Jet_btagCSVV2', 'Jet_btagDeepB', 'Jet_btagDeepCvB', 'Jet_btagDeepCvL', 'Jet_btagDeepFlavB', 'Jet_btagDeepFlavCvB', 'Jet_btagDeepFlavCvL', 'Jet_btagDeepFlavQG', 'Jet_chEmEF', 'Jet_chFPV0EF', 'Jet_chHEF', 'Jet_eta', 'Jet_hfsigmaEtaEta', 'Jet_hfsigmaPhiPhi', 'Jet_mass', 'Jet_muEF', 'Jet_muonSubtrFactor', 'Jet_neEmEF', 'Jet_neHEF', 'Jet_phi', 'Jet_pt', 'Jet_puIdDisc', 'Jet_qgl', 'Jet_rawFactor', 'Jet_bRegCorr', 'Jet_bRegRes', 'Jet_cRegCorr', 'Jet_cRegRes', 'Jet_electronIdx1', 'Jet_electronIdx2', 'Jet_hfadjacentEtaStripsSize', 'Jet_hfcentralEtaStripSize', 'Jet_jetId', 'Jet_muonIdx1', 'Jet_muonIdx2', 'Jet_nElectrons', 'Jet_nMuons', 'Jet_puId', 'Jet_nConstituents', 'L1PreFiringWeight_Dn', 'L1PreFiringWeight_ECAL_Dn', 'L1PreFiringWeight_ECAL_Nom', 'L1PreFiringWeight_ECAL_Up', 'L1PreFiringWeight_Muon_Nom', 'L1PreFiringWeight_Muon_StatDn', 'L1PreFiringWeight_Muon_StatUp', 'L1PreFiringWeight_Muon_SystDn', 'L1PreFiringWeight_Muon_SystUp', 'L1PreFiringWeight_Nom', 'L1PreFiringWeight_Up', 'LHE_HT', 'LHE_HTIncoming', 'LHE_Vpt', 'LHE_AlphaS', 'LHE_Njets', 'LHE_Nb', 'LHE_Nc', 'LHE_Nuds', 'LHE_Nglu', 'LHE_NpNLO', 'LHE_NpLO', 'nLHEPart', 'LHEPart_pt', 'LHEPart_eta', 'LHEPart_phi', 'LHEPart_mass', 'LHEPart_incomingpz', 'LHEPart_pdgId', 'LHEPart_status', 'LHEPart_spin', 'nLowPtElectron', 'LowPtElectron_ID', 'LowPtElectron_convVtxRadius', 'LowPtElectron_deltaEtaSC', 'LowPtElectron_dxy', 'LowPtElectron_dxyErr', 'LowPtElectron_dz', 'LowPtElectron_dzErr', 'LowPtElectron_eInvMinusPInv', 'LowPtElectron_embeddedID', 'LowPtElectron_energyErr', 'LowPtElectron_eta', 'LowPtElectron_hoe', 'LowPtElectron_mass', 'LowPtElectron_miniPFRelIso_all', 'LowPtElectron_miniPFRelIso_chg', 'LowPtElectron_phi', 'LowPtElectron_pt', 'LowPtElectron_ptbiased', 'LowPtElectron_r9', 'LowPtElectron_scEtOverPt', 'LowPtElectron_sieie', 'LowPtElectron_unbiased', 'LowPtElectron_charge', 'LowPtElectron_convWP', 'LowPtElectron_pdgId', 'LowPtElectron_convVeto', 'LowPtElectron_lostHits', 'GenMET_phi', 'GenMET_pt', 'MET_MetUnclustEnUpDeltaX', 'MET_MetUnclustEnUpDeltaY', 'MET_covXX', 'MET_covXY', 'MET_covYY', 'MET_phi', 'MET_pt', 'MET_significance', 'MET_sumEt', 'MET_sumPtUnclustered', 'nMuon', 'Muon_dxy', 'Muon_dxyErr', 'Muon_dxybs', 'Muon_dz', 'Muon_dzErr', 'Muon_eta', 'Muon_ip3d', 'Muon_jetPtRelv2', 'Muon_jetRelIso', 'Muon_mass', 'Muon_miniPFRelIso_all', 'Muon_miniPFRelIso_chg', 'Muon_pfRelIso03_all', 'Muon_pfRelIso03_chg', 'Muon_pfRelIso04_all', 'Muon_phi', 'Muon_pt', 'Muon_ptErr', 'Muon_segmentComp', 'Muon_sip3d', 'Muon_softMva', 'Muon_tkRelIso', 'Muon_tunepRelPt', 'Muon_mvaLowPt', 'Muon_mvaTTH', 'Muon_charge', 'Muon_jetIdx', 'Muon_nStations', 'Muon_nTrackerLayers', 'Muon_pdgId', 'Muon_tightCharge', 'Muon_fsrPhotonIdx', 'Muon_highPtId', 'Muon_highPurity', 'Muon_inTimeMuon', 'Muon_isGlobal', 'Muon_isPFcand', 'Muon_isStandalone', 'Muon_isTracker', 'Muon_jetNDauCharged', 'Muon_looseId', 'Muon_mediumId', 'Muon_mediumPromptId', 'Muon_miniIsoId', 'Muon_multiIsoId', 'Muon_mvaId', 'Muon_mvaLowPtId', 'Muon_pfIsoId', 'Muon_puppiIsoId', 'Muon_softId', 'Muon_softMvaId', 'Muon_tightId', 'Muon_tkIsoId', 'Muon_triggerIdLoose', 'nPhoton', 'Photon_dEscaleDown', 'Photon_dEscaleUp', 'Photon_dEsigmaDown', 'Photon_dEsigmaUp', 'Photon_eCorr', 'Photon_energyErr', 'Photon_eta', 'Photon_hoe', 'Photon_mass', 'Photon_mvaID', 'Photon_mvaID_Fall17V1p1', 'Photon_pfRelIso03_all', 'Photon_pfRelIso03_chg', 'Photon_phi', 'Photon_pt', 'Photon_r9', 'Photon_sieie', 'Photon_charge', 'Photon_cutBased', 'Photon_cutBased_Fall17V1Bitmap', 'Photon_electronIdx', 'Photon_jetIdx', 'Photon_pdgId', 'Photon_vidNestedWPBitmap', 'Photon_electronVeto', 'Photon_isScEtaEB', 'Photon_isScEtaEE', 'Photon_mvaID_WP80', 'Photon_mvaID_WP90', 'Photon_pixelSeed', 'Photon_seedGain', 'Pileup_nTrueInt', 'Pileup_pudensity', 'Pileup_gpudensity', 'Pileup_nPU', 'Pileup_sumEOOT', 'Pileup_sumLOOT', 'PuppiMET_phi', 'PuppiMET_phiJERDown', 'PuppiMET_phiJERUp', 'PuppiMET_phiJESDown', 'PuppiMET_phiJESUp', 'PuppiMET_phiUnclusteredDown', 'PuppiMET_phiUnclusteredUp', 'PuppiMET_pt', 'PuppiMET_ptJERDown', 'PuppiMET_ptJERUp', 'PuppiMET_ptJESDown', 'PuppiMET_ptJESUp', 'PuppiMET_ptUnclusteredDown', 'PuppiMET_ptUnclusteredUp', 'PuppiMET_sumEt', 'RawMET_phi', 'RawMET_pt', 'RawMET_sumEt', 'RawPuppiMET_phi', 'RawPuppiMET_pt', 'RawPuppiMET_sumEt', 'fixedGridRhoFastjetAll', 'fixedGridRhoFastjetCentral', 'fixedGridRhoFastjetCentralCalo', 'fixedGridRhoFastjetCentralChargedPileUp', 'fixedGridRhoFastjetCentralNeutral', 'nGenDressedLepton', 'GenDressedLepton_eta', 'GenDressedLepton_mass', 'GenDressedLepton_phi', 'GenDressedLepton_pt', 'GenDressedLepton_pdgId', 'GenDressedLepton_hasTauAnc', 'nGenIsolatedPhoton', 'GenIsolatedPhoton_eta', 'GenIsolatedPhoton_mass', 'GenIsolatedPhoton_phi', 'GenIsolatedPhoton_pt', 'nSoftActivityJet', 'SoftActivityJet_eta', 'SoftActivityJet_phi', 'SoftActivityJet_pt', 'SoftActivityJetHT', 'SoftActivityJetHT10', 'SoftActivityJetHT2', 'SoftActivityJetHT5', 'SoftActivityJetNjets10', 'SoftActivityJetNjets2', 'SoftActivityJetNjets5', 'nSubJet', 'SubJet_btagCSVV2', 'SubJet_btagDeepB', 'SubJet_eta', 'SubJet_mass', 'SubJet_n2b1', 'SubJet_n3b1', 'SubJet_phi', 'SubJet_pt', 'SubJet_rawFactor', 'SubJet_tau1', 'SubJet_tau2', 'SubJet_tau3', 'SubJet_tau4', 'nTau', 'Tau_chargedIso', 'Tau_dxy', 'Tau_dz', 'Tau_eta', 'Tau_leadTkDeltaEta', 'Tau_leadTkDeltaPhi', 'Tau_leadTkPtOverTauPt', 'Tau_mass', 'Tau_neutralIso', 'Tau_phi', 'Tau_photonsOutsideSignalCone', 'Tau_pt', 'Tau_puCorr', 'Tau_rawDeepTau2017v2p1VSe', 'Tau_rawDeepTau2017v2p1VSjet', 'Tau_rawDeepTau2017v2p1VSmu', 'Tau_rawIso', 'Tau_rawIsodR03', 'Tau_charge', 'Tau_decayMode', 'Tau_jetIdx', 'Tau_idAntiEleDeadECal', 'Tau_idAntiMu', 'Tau_idDecayModeOldDMs', 'Tau_idDeepTau2017v2p1VSe', 'Tau_idDeepTau2017v2p1VSjet', 'Tau_idDeepTau2017v2p1VSmu', 'TkMET_phi', 'TkMET_pt', 'TkMET_sumEt', 'nTrigObj', 'TrigObj_pt', 'TrigObj_eta', 'TrigObj_phi', 'TrigObj_l1pt', 'TrigObj_l1pt_2', 'TrigObj_l2pt', 'TrigObj_id', 'TrigObj_l1iso', 'TrigObj_l1charge', 'TrigObj_filterBits', 'genTtbarId', 'nOtherPV', 'OtherPV_z', 'PV_ndof', 'PV_x', 'PV_y', 'PV_z', 'PV_chi2', 'PV_score', 'PV_npvs', 'PV_npvsGood', 'nSV', 'SV_dlen', 'SV_dlenSig', 'SV_dxy', 'SV_dxySig', 'SV_pAngle', 'SV_charge', 'boostedTau_genPartIdx', 'boostedTau_genPartFlav', 'Electron_genPartIdx', 'Electron_genPartFlav', 'FatJet_genJetAK8Idx', 'FatJet_hadronFlavour', 'FatJet_nBHadrons', 'FatJet_nCHadrons', 'GenJetAK8_partonFlavour', 'GenJetAK8_hadronFlavour', 'GenJet_partonFlavour', 'GenJet_hadronFlavour', 'GenVtx_t0', 'Jet_genJetIdx', 'Jet_hadronFlavour', 'Jet_partonFlavour', 'LowPtElectron_genPartIdx', 'LowPtElectron_genPartFlav', 'Muon_genPartIdx', 'Muon_genPartFlav', 'Photon_genPartIdx', 'Photon_genPartFlav', 'MET_fiducialGenPhi', 'MET_fiducialGenPt', 'Electron_cleanmask', 'Jet_cleanmask', 'Muon_cleanmask', 'Photon_cleanmask', 'Tau_cleanmask', 'SubJet_hadronFlavour', 'SubJet_nBHadrons', 'SubJet_nCHadrons', 'SV_chi2', 'SV_eta', 'SV_mass', 'SV_ndof', 'SV_phi', 'SV_pt', 'SV_x', 'SV_y', 'SV_z', 'SV_ntracks', 'Tau_genPartIdx', 'Tau_genPartFlav', 'L1_AlwaysTrue', 'L1_BRIL_TRIG0_AND', 'L1_BRIL_TRIG0_FstBunchInTrain', 'L1_BRIL_TRIG0_OR', 'L1_BRIL_TRIG0_delayedAND', 'L1_BeamGasB1', 'L1_BeamGasB2', 'L1_BeamGasMinus', 'L1_BeamGasPlus', 'L1_BptxMinus', 'L1_BptxOR', 'L1_BptxPlus', 'L1_BptxXOR', 'L1_DoubleEG6_HTT255', 'L1_DoubleEG_15_10', 'L1_DoubleEG_18_17', 'L1_DoubleEG_20_18', 'L1_DoubleEG_22_10', 'L1_DoubleEG_22_12', 'L1_DoubleEG_22_15', 'L1_DoubleEG_23_10', 'L1_DoubleEG_24_17', 'L1_DoubleEG_25_12', 'L1_DoubleIsoTau28er', 'L1_DoubleIsoTau30er', 'L1_DoubleIsoTau32er', 'L1_DoubleIsoTau33er', 'L1_DoubleIsoTau34er', 'L1_DoubleIsoTau35er', 'L1_DoubleIsoTau36er', 'L1_DoubleJet12_ForwardBackward', 'L1_DoubleJet16_ForwardBackward', 'L1_DoubleJet8_ForwardBackward', 'L1_DoubleJetC100', 'L1_DoubleJetC112', 'L1_DoubleJetC120', 'L1_DoubleJetC40', 'L1_DoubleJetC50', 'L1_DoubleJetC60', 'L1_DoubleJetC60_ETM60', 'L1_DoubleJetC80', 'L1_DoubleMu0', 'L1_DoubleMu0_ETM40', 'L1_DoubleMu0_ETM55', 'L1_DoubleMu0er1p4_dEta_Max1p8_OS', 'L1_DoubleMu0er1p6_dEta_Max1p8', 'L1_DoubleMu0er1p6_dEta_Max1p8_OS', 'L1_DoubleMu7_EG14', 'L1_DoubleMu7_EG7', 'L1_DoubleMuOpen', 'L1_DoubleMu_10_0_dEta_Max1p8', 'L1_DoubleMu_10_3p5', 'L1_DoubleMu_10_Open', 'L1_DoubleMu_11_4', 'L1_DoubleMu_12_5', 'L1_DoubleMu_12_8', 'L1_DoubleMu_13_6', 'L1_DoubleMu_15_5', 'L1_DoubleTau50er', 'L1_EG25er_HTT125', 'L1_EG27er_HTT200', 'L1_ETM100', 'L1_ETM120', 'L1_ETM30', 'L1_ETM40', 'L1_ETM50', 'L1_ETM60', 'L1_ETM70', 'L1_ETM75', 'L1_ETM75_Jet60_dPhi_Min0p4', 'L1_ETM80', 'L1_ETM85', 'L1_ETM90', 'L1_ETM95', 'L1_ETT25', 'L1_ETT35_BptxAND', 'L1_ETT40_BptxAND', 'L1_ETT50_BptxAND', 'L1_ETT60_BptxAND', 'L1_FirstBunchAfterTrain', 'L1_FirstBunchInTrain', 'L1_HTM100', 'L1_HTM120', 'L1_HTM130', 'L1_HTM140', 'L1_HTM150', 'L1_HTM50', 'L1_HTM60_HTT260', 'L1_HTM70', 'L1_HTM80', 'L1_HTM80_HTT220', 'L1_HTT120', 'L1_HTT160', 'L1_HTT200', 'L1_HTT220', 'L1_HTT240', 'L1_HTT255', 'L1_HTT270', 'L1_HTT280', 'L1_HTT300', 'L1_HTT320', 'L1_IsoEG18er_IsoTau24er_dEta_Min0p2', 'L1_IsoEG20er_IsoTau25er_dEta_Min0p2', 'L1_IsoEG22er_IsoTau26er_dEta_Min0p2', 'L1_IsoEG22er_Tau20er_dEta_Min0p2', 'L1_IsolatedBunch', 'L1_Jet32_DoubleMu_10_0_dPhi_Jet_Mu0_Max0p4_dPhi_Mu_Mu_Min1p0', 'L1_Jet32_Mu0_EG10_dPhi_Jet_Mu_Max0p4_dPhi_Mu_EG_Min1p0', 'L1_MU20_EG15', 'L1_MinimumBiasHF0_AND', 'L1_MinimumBiasHF0_AND_BptxAND', 'L1_MinimumBiasHF0_OR', 'L1_MinimumBiasHF0_OR_BptxAND', 'L1_MinimumBiasHF1_AND', 'L1_MinimumBiasHF1_AND_BptxAND', 'L1_MinimumBiasHF1_OR', 'L1_MinimumBiasHF1_OR_BptxAND', 'L1_Mu0er_ETM40', 'L1_Mu0er_ETM55', 'L1_Mu10er_ETM30', 'L1_Mu10er_ETM50', 'L1_Mu12_EG10', 'L1_Mu14er_ETM30', 'L1_Mu16er_Tau20er', 'L1_Mu16er_Tau24er', 'L1_Mu18er_IsoTau26er', 'L1_Mu18er_Tau20er', 'L1_Mu18er_Tau24er', 'L1_Mu20_EG10', 'L1_Mu20_EG17', 'L1_Mu20_IsoEG6', 'L1_Mu20er_IsoTau26er', 'L1_Mu22er_IsoTau26er', 'L1_Mu23_EG10', 'L1_Mu23_IsoEG10', 'L1_Mu25er_IsoTau26er', 'L1_Mu3_JetC120', 'L1_Mu3_JetC120_dEta_Max0p4_dPhi_Max0p4', 'L1_Mu3_JetC16', 'L1_Mu3_JetC16_dEta_Max0p4_dPhi_Max0p4', 'L1_Mu3_JetC60', 'L1_Mu3_JetC60_dEta_Max0p4_dPhi_Max0p4', 'L1_Mu5_EG15', 'L1_Mu5_EG20', 'L1_Mu5_EG23', 'L1_Mu5_IsoEG18', 'L1_Mu5_IsoEG20', 'L1_Mu6_DoubleEG10', 'L1_Mu6_DoubleEG17', 'L1_Mu6_HTT200', 'L1_Mu8_HTT150', 'L1_NotBptxOR', 'L1_QuadJetC36_Tau52', 'L1_QuadJetC40', 'L1_QuadJetC50', 'L1_QuadJetC60', 'L1_QuadMu0', 'L1_SingleEG10', 'L1_SingleEG15', 'L1_SingleEG18', 'L1_SingleEG24', 'L1_SingleEG26', 'L1_SingleEG28', 'L1_SingleEG2_BptxAND', 'L1_SingleEG30', 'L1_SingleEG32', 'L1_SingleEG34', 'L1_SingleEG36', 'L1_SingleEG38', 'L1_SingleEG40', 'L1_SingleEG45', 'L1_SingleEG5', 'L1_SingleIsoEG18', 'L1_SingleIsoEG18er', 'L1_SingleIsoEG20', 'L1_SingleIsoEG20er', 'L1_SingleIsoEG22', 'L1_SingleIsoEG22er', 'L1_SingleIsoEG24', 'L1_SingleIsoEG24er', 'L1_SingleIsoEG26', 'L1_SingleIsoEG26er', 'L1_SingleIsoEG28', 'L1_SingleIsoEG28er', 'L1_SingleIsoEG30', 'L1_SingleIsoEG30er', 'L1_SingleIsoEG32', 'L1_SingleIsoEG32er', 'L1_SingleIsoEG34', 'L1_SingleIsoEG34er', 'L1_SingleIsoEG36', 'L1_SingleJet120', 'L1_SingleJet12_BptxAND', 'L1_SingleJet140', 'L1_SingleJet150', 'L1_SingleJet16', 'L1_SingleJet160', 'L1_SingleJet170', 'L1_SingleJet180', 'L1_SingleJet20', 'L1_SingleJet200', 'L1_SingleJet35', 'L1_SingleJet60', 'L1_SingleJet8_BptxAND', 'L1_SingleJet90', 'L1_SingleJetC20_NotBptxOR', 'L1_SingleJetC20_NotBptxOR_3BX', 'L1_SingleJetC32_NotBptxOR', 'L1_SingleJetC32_NotBptxOR_3BX', 'L1_SingleJetC36_NotBptxOR_3BX', 'L1_SingleMu10_LowQ', 'L1_SingleMu12', 'L1_SingleMu14', 'L1_SingleMu14er', 'L1_SingleMu16', 'L1_SingleMu16er', 'L1_SingleMu18', 'L1_SingleMu18er', 'L1_SingleMu20', 'L1_SingleMu20er', 'L1_SingleMu22', 'L1_SingleMu22er', 'L1_SingleMu25', 'L1_SingleMu25er', 'L1_SingleMu3', 'L1_SingleMu30', 'L1_SingleMu30er', 'L1_SingleMu5', 'L1_SingleMu7', 'L1_SingleMuCosmics', 'L1_SingleMuOpen', 'L1_SingleMuOpen_NotBptxOR', 'L1_SingleMuOpen_NotBptxOR_3BX', 'L1_SingleTau100er', 'L1_SingleTau120er', 'L1_SingleTau80er', 'L1_TripleEG_14_10_8', 'L1_TripleEG_18_17_8', 'L1_TripleJet_84_68_48_VBF', 'L1_TripleJet_88_72_56_VBF', 'L1_TripleJet_92_76_64_VBF', 'L1_TripleMu0', 'L1_TripleMu_5_0_0', 'L1_TripleMu_5_5_3', 'L1_ZeroBias', 'L1_ZeroBias_FirstCollidingBunch', 'L1_ZeroBias_copy', 'L1_UnprefireableEvent', 'Flag_HBHENoiseFilter', 'Flag_HBHENoiseIsoFilter', 'Flag_CSCTightHaloFilter', 'Flag_CSCTightHaloTrkMuUnvetoFilter', 'Flag_CSCTightHalo2015Filter', 'Flag_globalTightHalo2016Filter', 'Flag_globalSuperTightHalo2016Filter', 'Flag_HcalStripHaloFilter', 'Flag_hcalLaserEventFilter', 'Flag_EcalDeadCellTriggerPrimitiveFilter', 'Flag_EcalDeadCellBoundaryEnergyFilter', 'Flag_ecalBadCalibFilter', 'Flag_goodVertices', 'Flag_eeBadScFilter', 'Flag_ecalLaserCorrFilter', 'Flag_trkPOGFilters', 'Flag_chargedHadronTrackResolutionFilter', 'Flag_muonBadTrackFilter', 'Flag_BadChargedCandidateFilter', 'Flag_BadPFMuonFilter', 'Flag_BadPFMuonDzFilter', 'Flag_hfNoisyHitsFilter', 'Flag_BadChargedCandidateSummer16Filter', 'Flag_BadPFMuonSummer16Filter', 'Flag_trkPOG_manystripclus53X', 'Flag_trkPOG_toomanystripclus53X', 'Flag_trkPOG_logErrorTooManyClusters', 'Flag_METFilters', 'L1Reco_step', 'HLTriggerFirstPath', 'HLT_AK8PFJet360_TrimMass30', 'HLT_AK8PFJet400_TrimMass30', 'HLT_AK8PFHT750_TrimMass50', 'HLT_AK8PFHT800_TrimMass50', 'HLT_AK8DiPFJet300_200_TrimMass30_BTagCSV_p20', 'HLT_AK8DiPFJet280_200_TrimMass30_BTagCSV_p087', 'HLT_AK8DiPFJet300_200_TrimMass30_BTagCSV_p087', 'HLT_AK8DiPFJet300_200_TrimMass30', 'HLT_AK8PFHT700_TrimR0p1PT0p03Mass50', 'HLT_AK8PFHT650_TrimR0p1PT0p03Mass50', 'HLT_AK8PFHT600_TrimR0p1PT0p03Mass50_BTagCSV_p20', 'HLT_AK8DiPFJet280_200_TrimMass30', 'HLT_AK8DiPFJet250_200_TrimMass30', 'HLT_AK8DiPFJet280_200_TrimMass30_BTagCSV_p20', 'HLT_AK8DiPFJet250_200_TrimMass30_BTagCSV_p20', 'HLT_CaloJet260', 'HLT_CaloJet500_NoJetID', 'HLT_Dimuon13_PsiPrime', 'HLT_Dimuon13_Upsilon', 'HLT_Dimuon20_Jpsi', 'HLT_DoubleEle24_22_eta2p1_WPLoose_Gsf', 'HLT_DoubleEle25_CaloIdL_GsfTrkIdVL', 'HLT_DoubleEle33_CaloIdL', 'HLT_DoubleEle33_CaloIdL_MW', 'HLT_DoubleEle33_CaloIdL_GsfTrkIdVL_MW', 'HLT_DoubleEle33_CaloIdL_GsfTrkIdVL', 'HLT_DoubleMediumCombinedIsoPFTau35_Trk1_eta2p1_Reg', 'HLT_DoubleTightCombinedIsoPFTau35_Trk1_eta2p1_Reg', 'HLT_DoubleMediumCombinedIsoPFTau40_Trk1_eta2p1_Reg', 'HLT_DoubleTightCombinedIsoPFTau40_Trk1_eta2p1_Reg', 'HLT_DoubleMediumCombinedIsoPFTau40_Trk1_eta2p1', 'HLT_DoubleTightCombinedIsoPFTau40_Trk1_eta2p1', 'HLT_DoubleMediumIsoPFTau35_Trk1_eta2p1_Reg', 'HLT_DoubleMediumIsoPFTau40_Trk1_eta2p1_Reg', 'HLT_DoubleMediumIsoPFTau40_Trk1_eta2p1', 'HLT_DoubleEle37_Ele27_CaloIdL_GsfTrkIdVL', 'HLT_DoubleMu33NoFiltersNoVtx', 'HLT_DoubleMu38NoFiltersNoVtx', 'HLT_DoubleMu23NoFiltersNoVtxDisplaced', 'HLT_DoubleMu28NoFiltersNoVtxDisplaced', 'HLT_DoubleMu0', 'HLT_DoubleMu4_3_Bs', 'HLT_DoubleMu4_3_Jpsi_Displaced', 'HLT_DoubleMu4_JpsiTrk_Displaced', 'HLT_DoubleMu4_LowMassNonResonantTrk_Displaced', 'HLT_DoubleMu3_Trk_Tau3mu', 'HLT_DoubleMu4_PsiPrimeTrk_Displaced', 'HLT_Mu7p5_L2Mu2_Jpsi', 'HLT_Mu7p5_L2Mu2_Upsilon', 'HLT_Mu7p5_Track2_Jpsi', 'HLT_Mu7p5_Track3p5_Jpsi', 'HLT_Mu7p5_Track7_Jpsi', 'HLT_Mu7p5_Track2_Upsilon', 'HLT_Mu7p5_Track3p5_Upsilon', 'HLT_Mu7p5_Track7_Upsilon', 'HLT_Dimuon0er16_Jpsi_NoOS_NoVertexing', 'HLT_Dimuon0er16_Jpsi_NoVertexing', 'HLT_Dimuon6_Jpsi_NoVertexing', 'HLT_Photon150', 'HLT_Photon90_CaloIdL_HT300', 'HLT_HT250_CaloMET70', 'HLT_DoublePhoton60', 'HLT_DoublePhoton85', 'HLT_Ele17_Ele8_Gsf', 'HLT_Ele20_eta2p1_WPLoose_Gsf_LooseIsoPFTau28', 'HLT_Ele22_eta2p1_WPLoose_Gsf_LooseIsoPFTau29', 'HLT_Ele22_eta2p1_WPLoose_Gsf', 'HLT_Ele22_eta2p1_WPLoose_Gsf_LooseIsoPFTau20_SingleL1', 'HLT_Ele23_WPLoose_Gsf', 'HLT_Ele23_WPLoose_Gsf_WHbbBoost', 'HLT_Ele24_eta2p1_WPLoose_Gsf', 'HLT_Ele24_eta2p1_WPLoose_Gsf_LooseIsoPFTau20', 'HLT_Ele24_eta2p1_WPLoose_Gsf_LooseIsoPFTau20_SingleL1', 'HLT_Ele24_eta2p1_WPLoose_Gsf_LooseIsoPFTau30', 'HLT_Ele25_WPTight_Gsf', 'HLT_Ele25_eta2p1_WPLoose_Gsf', 'HLT_Ele25_eta2p1_WPTight_Gsf', 'HLT_Ele27_WPLoose_Gsf', 'HLT_Ele27_WPLoose_Gsf_WHbbBoost', 'HLT_Ele27_WPTight_Gsf', 'HLT_Ele27_WPTight_Gsf_L1JetTauSeeded', 'HLT_Ele27_eta2p1_WPLoose_Gsf', 'HLT_Ele27_eta2p1_WPLoose_Gsf_LooseIsoPFTau20_SingleL1', 'HLT_Ele27_eta2p1_WPTight_Gsf', 'HLT_Ele30_WPTight_Gsf', 'HLT_Ele30_eta2p1_WPLoose_Gsf', 'HLT_Ele30_eta2p1_WPTight_Gsf', 'HLT_Ele32_WPTight_Gsf', 'HLT_Ele32_eta2p1_WPLoose_Gsf', 'HLT_Ele32_eta2p1_WPLoose_Gsf_LooseIsoPFTau20_SingleL1', 'HLT_Ele32_eta2p1_WPTight_Gsf', 'HLT_Ele35_WPLoose_Gsf', 'HLT_Ele35_CaloIdVT_GsfTrkIdT_PFJet150_PFJet50', 'HLT_Ele36_eta2p1_WPLoose_Gsf_LooseIsoPFTau20_SingleL1', 'HLT_Ele45_WPLoose_Gsf', 'HLT_Ele45_WPLoose_Gsf_L1JetTauSeeded', 'HLT_Ele45_CaloIdVT_GsfTrkIdT_PFJet200_PFJet50', 'HLT_Ele105_CaloIdVT_GsfTrkIdT', 'HLT_Ele30WP60_SC4_Mass55', 'HLT_Ele30WP60_Ele8_Mass55', 'HLT_HT200', 'HLT_HT275', 'HLT_HT325', 'HLT_HT425', 'HLT_HT575', 'HLT_HT410to430', 'HLT_HT430to450', 'HLT_HT450to470', 'HLT_HT470to500', 'HLT_HT500to550', 'HLT_HT550to650', 'HLT_HT650', 'HLT_Mu16_eta2p1_MET30', 'HLT_IsoMu16_eta2p1_MET30', 'HLT_IsoMu16_eta2p1_MET30_LooseIsoPFTau50_Trk30_eta2p1', 'HLT_IsoMu17_eta2p1', 'HLT_IsoMu17_eta2p1_LooseIsoPFTau20', 'HLT_IsoMu17_eta2p1_LooseIsoPFTau20_SingleL1', 'HLT_DoubleIsoMu17_eta2p1', 'HLT_DoubleIsoMu17_eta2p1_noDzCut', 'HLT_IsoMu18', 'HLT_IsoMu19_eta2p1_LooseIsoPFTau20', 'HLT_IsoMu19_eta2p1_LooseIsoPFTau20_SingleL1', 'HLT_IsoMu19_eta2p1_MediumIsoPFTau32_Trk1_eta2p1_Reg', 'HLT_IsoMu19_eta2p1_LooseCombinedIsoPFTau20', 'HLT_IsoMu19_eta2p1_MediumCombinedIsoPFTau32_Trk1_eta2p1_Reg', 'HLT_IsoMu19_eta2p1_TightCombinedIsoPFTau32_Trk1_eta2p1_Reg', 'HLT_IsoMu21_eta2p1_MediumCombinedIsoPFTau32_Trk1_eta2p1_Reg', 'HLT_IsoMu21_eta2p1_TightCombinedIsoPFTau32_Trk1_eta2p1_Reg', 'HLT_IsoMu20', 'HLT_IsoMu21_eta2p1_LooseIsoPFTau20_SingleL1', 'HLT_IsoMu21_eta2p1_LooseIsoPFTau50_Trk30_eta2p1_SingleL1', 'HLT_IsoMu21_eta2p1_MediumIsoPFTau32_Trk1_eta2p1_Reg', 'HLT_IsoMu22', 'HLT_IsoMu22_eta2p1', 'HLT_IsoMu24', 'HLT_IsoMu27', 'HLT_IsoTkMu18', 'HLT_IsoTkMu20', 'HLT_IsoTkMu22', 'HLT_IsoTkMu22_eta2p1', 'HLT_IsoTkMu24', 'HLT_IsoTkMu27', 'HLT_JetE30_NoBPTX3BX', 'HLT_JetE30_NoBPTX', 'HLT_JetE50_NoBPTX3BX', 'HLT_JetE70_NoBPTX3BX', 'HLT_L1SingleMu18', 'HLT_L2Mu10', 'HLT_L1SingleMuOpen', 'HLT_L1SingleMuOpen_DT', 'HLT_L2DoubleMu23_NoVertex', 'HLT_L2DoubleMu28_NoVertex_2Cha_Angle2p5_Mass10', 'HLT_L2DoubleMu38_NoVertex_2Cha_Angle2p5_Mass10', 'HLT_L2Mu10_NoVertex_NoBPTX3BX', 'HLT_L2Mu10_NoVertex_NoBPTX', 'HLT_L2Mu45_NoVertex_3Sta_NoBPTX3BX', 'HLT_L2Mu40_NoVertex_3Sta_NoBPTX3BX', 'HLT_LooseIsoPFTau50_Trk30_eta2p1', 'HLT_LooseIsoPFTau50_Trk30_eta2p1_MET80', 'HLT_LooseIsoPFTau50_Trk30_eta2p1_MET90', 'HLT_LooseIsoPFTau50_Trk30_eta2p1_MET110', 'HLT_LooseIsoPFTau50_Trk30_eta2p1_MET120', 'HLT_PFTau120_eta2p1', 'HLT_PFTau140_eta2p1', 'HLT_VLooseIsoPFTau120_Trk50_eta2p1', 'HLT_VLooseIsoPFTau140_Trk50_eta2p1', 'HLT_Mu17_Mu8', 'HLT_Mu17_Mu8_DZ', 'HLT_Mu17_Mu8_SameSign', 'HLT_Mu17_Mu8_SameSign_DZ', 'HLT_Mu20_Mu10', 'HLT_Mu20_Mu10_DZ', 'HLT_Mu20_Mu10_SameSign', 'HLT_Mu20_Mu10_SameSign_DZ', 'HLT_Mu17_TkMu8_DZ', 'HLT_Mu17_TrkIsoVVL_Mu8_TrkIsoVVL', 'HLT_Mu17_TrkIsoVVL_Mu8_TrkIsoVVL_DZ', 'HLT_Mu17_TrkIsoVVL_TkMu8_TrkIsoVVL', 'HLT_Mu17_TrkIsoVVL_TkMu8_TrkIsoVVL_DZ', 'HLT_Mu25_TkMu0_dEta18_Onia', 'HLT_Mu27_TkMu8', 'HLT_Mu30_TkMu11', 'HLT_Mu30_eta2p1_PFJet150_PFJet50', 'HLT_Mu40_TkMu11', 'HLT_Mu40_eta2p1_PFJet200_PFJet50', 'HLT_Mu20', 'HLT_TkMu17', 'HLT_TkMu17_TrkIsoVVL_TkMu8_TrkIsoVVL', 'HLT_TkMu17_TrkIsoVVL_TkMu8_TrkIsoVVL_DZ', 'HLT_TkMu20', 'HLT_Mu24_eta2p1', 'HLT_TkMu24_eta2p1', 'HLT_Mu27', 'HLT_TkMu27', 'HLT_Mu45_eta2p1', 'HLT_Mu50', 'HLT_TkMu50', 'HLT_Mu38NoFiltersNoVtx_Photon38_CaloIdL', 'HLT_Mu42NoFiltersNoVtx_Photon42_CaloIdL', 'HLT_Mu28NoFiltersNoVtxDisplaced_Photon28_CaloIdL', 'HLT_Mu33NoFiltersNoVtxDisplaced_Photon33_CaloIdL', 'HLT_Mu23NoFiltersNoVtx_Photon23_CaloIdL', 'HLT_DoubleMu18NoFiltersNoVtx', 'HLT_Mu33NoFiltersNoVtxDisplaced_DisplacedJet50_Tight', 'HLT_Mu33NoFiltersNoVtxDisplaced_DisplacedJet50_Loose', 'HLT_Mu28NoFiltersNoVtx_DisplacedJet40_Loose', 'HLT_Mu38NoFiltersNoVtxDisplaced_DisplacedJet60_Tight', 'HLT_Mu38NoFiltersNoVtxDisplaced_DisplacedJet60_Loose', 'HLT_Mu38NoFiltersNoVtx_DisplacedJet60_Loose', 'HLT_Mu28NoFiltersNoVtx_CentralCaloJet40', 'HLT_PFHT300_PFMET100', 'HLT_PFHT300_PFMET110', 'HLT_PFHT550_4JetPt50', 'HLT_PFHT650_4JetPt50', 'HLT_PFHT750_4JetPt50', 'HLT_PFHT750_4JetPt70', 'HLT_PFHT750_4JetPt80', 'HLT_PFHT800_4JetPt50', 'HLT_PFHT850_4JetPt50', 'HLT_PFJet15_NoCaloMatched', 'HLT_PFJet25_NoCaloMatched', 'HLT_DiPFJet15_NoCaloMatched', 'HLT_DiPFJet25_NoCaloMatched', 'HLT_DiPFJet15_FBEta3_NoCaloMatched', 'HLT_DiPFJet25_FBEta3_NoCaloMatched', 'HLT_DiPFJetAve15_HFJEC', 'HLT_DiPFJetAve25_HFJEC', 'HLT_DiPFJetAve35_HFJEC', 'HLT_AK8PFJet40', 'HLT_AK8PFJet60', 'HLT_AK8PFJet80', 'HLT_AK8PFJet140', 'HLT_AK8PFJet200', 'HLT_AK8PFJet260', 'HLT_AK8PFJet320', 'HLT_AK8PFJet400', 'HLT_AK8PFJet450', 'HLT_AK8PFJet500', 'HLT_PFJet40', 'HLT_PFJet60', 'HLT_PFJet80', 'HLT_PFJet140', 'HLT_PFJet200', 'HLT_PFJet260', 'HLT_PFJet320', 'HLT_PFJet400', 'HLT_PFJet450', 'HLT_PFJet500', 'HLT_DiPFJetAve40', 'HLT_DiPFJetAve60', 'HLT_DiPFJetAve80', 'HLT_DiPFJetAve140', 'HLT_DiPFJetAve200', 'HLT_DiPFJetAve260', 'HLT_DiPFJetAve320', 'HLT_DiPFJetAve400', 'HLT_DiPFJetAve500', 'HLT_DiPFJetAve60_HFJEC', 'HLT_DiPFJetAve80_HFJEC', 'HLT_DiPFJetAve100_HFJEC', 'HLT_DiPFJetAve160_HFJEC', 'HLT_DiPFJetAve220_HFJEC', 'HLT_DiPFJetAve300_HFJEC', 'HLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu140', 'HLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu80', 'HLT_DiCentralPFJet170', 'HLT_SingleCentralPFJet170_CFMax0p1', 'HLT_DiCentralPFJet170_CFMax0p1', 'HLT_DiCentralPFJet220_CFMax0p3', 'HLT_DiCentralPFJet330_CFMax0p5', 'HLT_DiCentralPFJet430', 'HLT_PFHT125', 'HLT_PFHT200', 'HLT_PFHT250', 'HLT_PFHT300', 'HLT_PFHT350', 'HLT_PFHT400', 'HLT_PFHT475', 'HLT_PFHT600', 'HLT_PFHT650', 'HLT_PFHT800', 'HLT_PFHT900', 'HLT_PFHT200_PFAlphaT0p51', 'HLT_PFHT200_DiPFJetAve90_PFAlphaT0p57', 'HLT_PFHT200_DiPFJetAve90_PFAlphaT0p63', 'HLT_PFHT250_DiPFJetAve90_PFAlphaT0p55', 'HLT_PFHT250_DiPFJetAve90_PFAlphaT0p58', 'HLT_PFHT300_DiPFJetAve90_PFAlphaT0p53', 'HLT_PFHT300_DiPFJetAve90_PFAlphaT0p54', 'HLT_PFHT350_DiPFJetAve90_PFAlphaT0p52', 'HLT_PFHT350_DiPFJetAve90_PFAlphaT0p53', 'HLT_PFHT400_DiPFJetAve90_PFAlphaT0p51', 'HLT_PFHT400_DiPFJetAve90_PFAlphaT0p52', 'HLT_MET60_IsoTrk35_Loose', 'HLT_MET75_IsoTrk50', 'HLT_MET90_IsoTrk50', 'HLT_PFMET120_BTagCSV_p067', 'HLT_PFMET120_Mu5', 'HLT_PFMET170_NotCleaned', 'HLT_PFMET170_NoiseCleaned', 'HLT_PFMET170_HBHECleaned', 'HLT_PFMET170_JetIdCleaned', 'HLT_PFMET170_BeamHaloCleaned', 'HLT_PFMET170_HBHE_BeamHaloCleaned', 'HLT_PFMETTypeOne190_HBHE_BeamHaloCleaned', 'HLT_PFMET90_PFMHT90_IDTight', 'HLT_PFMET100_PFMHT100_IDTight', 'HLT_PFMET100_PFMHT100_IDTight_BeamHaloCleaned', 'HLT_PFMET110_PFMHT110_IDTight', 'HLT_PFMET120_PFMHT120_IDTight', 'HLT_CaloMHTNoPU90_PFMET90_PFMHT90_IDTight_BTagCSV_p067', 'HLT_CaloMHTNoPU90_PFMET90_PFMHT90_IDTight', 'HLT_QuadPFJet_BTagCSV_p016_p11_VBF_Mqq200', 'HLT_QuadPFJet_BTagCSV_p016_VBF_Mqq460', 'HLT_QuadPFJet_BTagCSV_p016_p11_VBF_Mqq240', 'HLT_QuadPFJet_BTagCSV_p016_VBF_Mqq500', 'HLT_QuadPFJet_VBF', 'HLT_L1_TripleJet_VBF', 'HLT_QuadJet45_TripleBTagCSV_p087', 'HLT_QuadJet45_DoubleBTagCSV_p087', 'HLT_DoubleJet90_Double30_TripleBTagCSV_p087', 'HLT_DoubleJet90_Double30_DoubleBTagCSV_p087', 'HLT_DoubleJetsC100_DoubleBTagCSV_p026_DoublePFJetsC160', 'HLT_DoubleJetsC100_DoubleBTagCSV_p014_DoublePFJetsC100MaxDeta1p6', 'HLT_DoubleJetsC112_DoubleBTagCSV_p026_DoublePFJetsC172', 'HLT_DoubleJetsC112_DoubleBTagCSV_p014_DoublePFJetsC112MaxDeta1p6', 'HLT_DoubleJetsC100_SingleBTagCSV_p026', 'HLT_DoubleJetsC100_SingleBTagCSV_p014', 'HLT_DoubleJetsC100_SingleBTagCSV_p026_SinglePFJetC350', 'HLT_DoubleJetsC100_SingleBTagCSV_p014_SinglePFJetC350', 'HLT_Photon135_PFMET100', 'HLT_Photon20_CaloIdVL_IsoL', 'HLT_Photon22_R9Id90_HE10_Iso40_EBOnly_PFMET40', 'HLT_Photon22_R9Id90_HE10_Iso40_EBOnly_VBF', 'HLT_Photon250_NoHE', 'HLT_Photon300_NoHE', 'HLT_Photon26_R9Id85_OR_CaloId24b40e_Iso50T80L_Photon16_AND_HE10_R9Id65_Eta2_Mass60', 'HLT_Photon36_R9Id85_OR_CaloId24b40e_Iso50T80L_Photon22_AND_HE10_R9Id65_Eta2_Mass15', 'HLT_Photon36_R9Id90_HE10_Iso40_EBOnly_PFMET40', 'HLT_Photon36_R9Id90_HE10_Iso40_EBOnly_VBF', 'HLT_Photon50_R9Id90_HE10_Iso40_EBOnly_PFMET40', 'HLT_Photon50_R9Id90_HE10_Iso40_EBOnly_VBF', 'HLT_Photon75_R9Id90_HE10_Iso40_EBOnly_PFMET40', 'HLT_Photon75_R9Id90_HE10_Iso40_EBOnly_VBF', 'HLT_Photon90_R9Id90_HE10_Iso40_EBOnly_PFMET40', 'HLT_Photon90_R9Id90_HE10_Iso40_EBOnly_VBF', 'HLT_Photon120_R9Id90_HE10_Iso40_EBOnly_PFMET40', 'HLT_Photon120_R9Id90_HE10_Iso40_EBOnly_VBF', 'HLT_Mu8_TrkIsoVVL', 'HLT_Mu17_TrkIsoVVL', 'HLT_Ele8_CaloIdL_TrackIdL_IsoVL_PFJet30', 'HLT_Ele12_CaloIdL_TrackIdL_IsoVL_PFJet30', 'HLT_Ele17_CaloIdL_TrackIdL_IsoVL_PFJet30', 'HLT_Ele23_CaloIdL_TrackIdL_IsoVL_PFJet30', 'HLT_BTagMu_DiJet20_Mu5', 'HLT_BTagMu_DiJet40_Mu5', 'HLT_BTagMu_DiJet70_Mu5', 'HLT_BTagMu_DiJet110_Mu5', 'HLT_BTagMu_DiJet170_Mu5', 'HLT_BTagMu_Jet300_Mu5', 'HLT_BTagMu_AK8Jet300_Mu5', 'HLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL_DZ', 'HLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL_DZ_L1JetTauSeeded', 'HLT_Ele17_Ele12_CaloIdL_TrackIdL_IsoVL_DZ', 'HLT_Ele16_Ele12_Ele8_CaloIdL_TrackIdL', 'HLT_Mu8_TrkIsoVVL_Ele17_CaloIdL_TrackIdL_IsoVL', 'HLT_Mu8_TrkIsoVVL_Ele23_CaloIdL_TrackIdL_IsoVL', 'HLT_Mu8_TrkIsoVVL_Ele23_CaloIdL_TrackIdL_IsoVL_DZ', 'HLT_Mu12_TrkIsoVVL_Ele23_CaloIdL_TrackIdL_IsoVL', 'HLT_Mu12_TrkIsoVVL_Ele23_CaloIdL_TrackIdL_IsoVL_DZ', 'HLT_Mu17_TrkIsoVVL_Ele12_CaloIdL_TrackIdL_IsoVL', 'HLT_Mu23_TrkIsoVVL_Ele8_CaloIdL_TrackIdL_IsoVL', 'HLT_Mu23_TrkIsoVVL_Ele8_CaloIdL_TrackIdL_IsoVL_DZ', 'HLT_Mu23_TrkIsoVVL_Ele12_CaloIdL_TrackIdL_IsoVL', 'HLT_Mu23_TrkIsoVVL_Ele12_CaloIdL_TrackIdL_IsoVL_DZ', 'HLT_Mu30_Ele30_CaloIdL_GsfTrkIdVL', 'HLT_Mu33_Ele33_CaloIdL_GsfTrkIdVL', 'HLT_Mu37_Ele27_CaloIdL_GsfTrkIdVL', 'HLT_Mu27_Ele37_CaloIdL_GsfTrkIdVL', 'HLT_Mu8_DiEle12_CaloIdL_TrackIdL', 'HLT_Mu12_Photon25_CaloIdL', 'HLT_Mu12_Photon25_CaloIdL_L1ISO', 'HLT_Mu12_Photon25_CaloIdL_L1OR', 'HLT_Mu17_Photon22_CaloIdL_L1ISO', 'HLT_Mu17_Photon30_CaloIdL_L1ISO', 'HLT_Mu17_Photon35_CaloIdL_L1ISO', 'HLT_DiMu9_Ele9_CaloIdL_TrackIdL', 'HLT_TripleMu_5_3_3', 'HLT_TripleMu_12_10_5', 'HLT_Mu3er_PFHT140_PFMET125', 'HLT_Mu6_PFHT200_PFMET80_BTagCSV_p067', 'HLT_Mu6_PFHT200_PFMET100', 'HLT_Mu14er_PFMET100', 'HLT_Ele17_Ele12_CaloIdL_TrackIdL_IsoVL', 'HLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL', 'HLT_Ele12_CaloIdL_TrackIdL_IsoVL', 'HLT_Ele17_CaloIdL_GsfTrkIdVL', 'HLT_Ele17_CaloIdL_TrackIdL_IsoVL', 'HLT_Ele23_CaloIdL_TrackIdL_IsoVL', 'HLT_PFHT650_WideJetMJJ900DEtaJJ1p5', 'HLT_PFHT650_WideJetMJJ950DEtaJJ1p5', 'HLT_Photon22', 'HLT_Photon30', 'HLT_Photon36', 'HLT_Photon50', 'HLT_Photon75', 'HLT_Photon90', 'HLT_Photon120', 'HLT_Photon175', 'HLT_Photon165_HE10', 'HLT_Photon22_R9Id90_HE10_IsoM', 'HLT_Photon30_R9Id90_HE10_IsoM', 'HLT_Photon36_R9Id90_HE10_IsoM', 'HLT_Photon50_R9Id90_HE10_IsoM', 'HLT_Photon75_R9Id90_HE10_IsoM', 'HLT_Photon90_R9Id90_HE10_IsoM', 'HLT_Photon120_R9Id90_HE10_IsoM', 'HLT_Photon165_R9Id90_HE10_IsoM', 'HLT_Diphoton30_18_R9Id_OR_IsoCaloId_AND_HE_R9Id_Mass90', 'HLT_Diphoton30_18_R9Id_OR_IsoCaloId_AND_HE_R9Id_DoublePixelSeedMatch_Mass70', 'HLT_Diphoton30PV_18PV_R9Id_AND_IsoCaloId_AND_HE_R9Id_DoublePixelVeto_Mass55', 'HLT_Diphoton30_18_Solid_R9Id_AND_IsoCaloId_AND_HE_R9Id_Mass55', 'HLT_Diphoton30EB_18EB_R9Id_OR_IsoCaloId_AND_HE_R9Id_DoublePixelVeto_Mass55', 'HLT_Dimuon0_Jpsi_Muon', 'HLT_Dimuon0_Upsilon_Muon', 'HLT_QuadMuon0_Dimuon0_Jpsi', 'HLT_QuadMuon0_Dimuon0_Upsilon', 'HLT_Rsq0p25_Calo', 'HLT_RsqMR240_Rsq0p09_MR200_4jet_Calo', 'HLT_RsqMR240_Rsq0p09_MR200_Calo', 'HLT_Rsq0p25', 'HLT_Rsq0p30', 'HLT_RsqMR240_Rsq0p09_MR200', 'HLT_RsqMR240_Rsq0p09_MR200_4jet', 'HLT_RsqMR270_Rsq0p09_MR200', 'HLT_RsqMR270_Rsq0p09_MR200_4jet', 'HLT_Rsq0p02_MR300_TriPFJet80_60_40_BTagCSV_p063_p20_Mbb60_200', 'HLT_Rsq0p02_MR400_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200', 'HLT_Rsq0p02_MR450_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200', 'HLT_Rsq0p02_MR500_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200', 'HLT_Rsq0p02_MR550_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200', 'HLT_HT200_DisplacedDijet40_DisplacedTrack', 'HLT_HT250_DisplacedDijet40_DisplacedTrack', 'HLT_HT350_DisplacedDijet40_DisplacedTrack', 'HLT_HT350_DisplacedDijet80_DisplacedTrack', 'HLT_HT350_DisplacedDijet80_Tight_DisplacedTrack', 'HLT_HT350_DisplacedDijet40_Inclusive', 'HLT_HT400_DisplacedDijet40_Inclusive', 'HLT_HT500_DisplacedDijet40_Inclusive', 'HLT_HT550_DisplacedDijet40_Inclusive', 'HLT_HT550_DisplacedDijet80_Inclusive', 'HLT_HT650_DisplacedDijet80_Inclusive', 'HLT_HT750_DisplacedDijet80_Inclusive', 'HLT_VBF_DisplacedJet40_DisplacedTrack', 'HLT_VBF_DisplacedJet40_DisplacedTrack_2TrackIP2DSig5', 'HLT_VBF_DisplacedJet40_TightID_DisplacedTrack', 'HLT_VBF_DisplacedJet40_Hadronic', 'HLT_VBF_DisplacedJet40_Hadronic_2PromptTrack', 'HLT_VBF_DisplacedJet40_TightID_Hadronic', 'HLT_VBF_DisplacedJet40_VTightID_Hadronic', 'HLT_VBF_DisplacedJet40_VVTightID_Hadronic', 'HLT_VBF_DisplacedJet40_VTightID_DisplacedTrack', 'HLT_VBF_DisplacedJet40_VVTightID_DisplacedTrack', 'HLT_PFMETNoMu90_PFMHTNoMu90_IDTight', 'HLT_PFMETNoMu100_PFMHTNoMu100_IDTight', 'HLT_PFMETNoMu110_PFMHTNoMu110_IDTight', 'HLT_PFMETNoMu120_PFMHTNoMu120_IDTight', 'HLT_MonoCentralPFJet80_PFMETNoMu90_PFMHTNoMu90_IDTight', 'HLT_MonoCentralPFJet80_PFMETNoMu100_PFMHTNoMu100_IDTight', 'HLT_MonoCentralPFJet80_PFMETNoMu110_PFMHTNoMu110_IDTight', 'HLT_MonoCentralPFJet80_PFMETNoMu120_PFMHTNoMu120_IDTight', 'HLT_Ele27_eta2p1_WPLoose_Gsf_HT200', 'HLT_Photon90_CaloIdL_PFHT500', 'HLT_DoubleMu8_Mass8_PFHT250', 'HLT_Mu8_Ele8_CaloIdM_TrackIdM_Mass8_PFHT250', 'HLT_DoubleEle8_CaloIdM_TrackIdM_Mass8_PFHT250', 'HLT_DoubleMu8_Mass8_PFHT300', 'HLT_Mu8_Ele8_CaloIdM_TrackIdM_Mass8_PFHT300', 'HLT_DoubleEle8_CaloIdM_TrackIdM_Mass8_PFHT300', 'HLT_Mu10_CentralPFJet30_BTagCSV_p13', 'HLT_DoubleMu3_PFMET50', 'HLT_Ele10_CaloIdM_TrackIdM_CentralPFJet30_BTagCSV_p13', 'HLT_Ele15_IsoVVVL_BTagCSV_p067_PFHT400', 'HLT_Ele15_IsoVVVL_PFHT350_PFMET50', 'HLT_Ele15_IsoVVVL_PFHT600', 'HLT_Ele15_IsoVVVL_PFHT350', 'HLT_Ele15_IsoVVVL_PFHT400_PFMET50', 'HLT_Ele15_IsoVVVL_PFHT400', 'HLT_Ele50_IsoVVVL_PFHT400', 'HLT_Mu8_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT300_PFMETNoMu60', 'HLT_Mu10_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT350_PFMETNoMu60', 'HLT_Mu15_IsoVVVL_BTagCSV_p067_PFHT400', 'HLT_Mu15_IsoVVVL_PFHT350_PFMET50', 'HLT_Mu15_IsoVVVL_PFHT600', 'HLT_Mu15_IsoVVVL_PFHT350', 'HLT_Mu15_IsoVVVL_PFHT400_PFMET50', 'HLT_Mu15_IsoVVVL_PFHT400', 'HLT_Mu50_IsoVVVL_PFHT400', 'HLT_Dimuon16_Jpsi', 'HLT_Dimuon10_Jpsi_Barrel', 'HLT_Dimuon8_PsiPrime_Barrel', 'HLT_Dimuon8_Upsilon_Barrel', 'HLT_Dimuon0_Phi_Barrel', 'HLT_Mu16_TkMu0_dEta18_Onia', 'HLT_Mu16_TkMu0_dEta18_Phi', 'HLT_TrkMu15_DoubleTrkMu5NoFiltersNoVtx', 'HLT_TrkMu17_DoubleTrkMu8NoFiltersNoVtx', 'HLT_Mu8', 'HLT_Mu17', 'HLT_Mu3_PFJet40', 'HLT_Ele8_CaloIdM_TrackIdM_PFJet30', 'HLT_Ele12_CaloIdM_TrackIdM_PFJet30', 'HLT_Ele17_CaloIdM_TrackIdM_PFJet30', 'HLT_Ele23_CaloIdM_TrackIdM_PFJet30', 'HLT_Ele50_CaloIdVT_GsfTrkIdT_PFJet140', 'HLT_Ele50_CaloIdVT_GsfTrkIdT_PFJet165', 'HLT_PFHT400_SixJet30_DoubleBTagCSV_p056', 'HLT_PFHT450_SixJet40_BTagCSV_p056', 'HLT_PFHT400_SixJet30', 'HLT_PFHT450_SixJet40', 'HLT_Ele115_CaloIdVT_GsfTrkIdT', 'HLT_Mu55', 'HLT_Photon42_R9Id85_OR_CaloId24b40e_Iso50T80L_Photon25_AND_HE10_R9Id65_Eta2_Mass15', 'HLT_Photon90_CaloIdL_PFHT600', 'HLT_PixelTracks_Multiplicity60ForEndOfFill', 'HLT_PixelTracks_Multiplicity85ForEndOfFill', 'HLT_PixelTracks_Multiplicity110ForEndOfFill', 'HLT_PixelTracks_Multiplicity135ForEndOfFill', 'HLT_PixelTracks_Multiplicity160ForEndOfFill', 'HLT_FullTracks_Multiplicity80', 'HLT_FullTracks_Multiplicity100', 'HLT_FullTracks_Multiplicity130', 'HLT_FullTracks_Multiplicity150', 'HLT_ECALHT800', 'HLT_DiSC30_18_EIso_AND_HE_Mass70', 'HLT_Photon125', 'HLT_MET100', 'HLT_MET150', 'HLT_MET200', 'HLT_Ele27_HighEta_Ele20_Mass55', 'HLT_L1FatEvents', 'HLT_Physics', 'HLT_L1FatEvents_part0', 'HLT_L1FatEvents_part1', 'HLT_L1FatEvents_part2', 'HLT_L1FatEvents_part3', 'HLT_Random', 'HLT_ZeroBias', 'HLT_AK4CaloJet30', 'HLT_AK4CaloJet40', 'HLT_AK4CaloJet50', 'HLT_AK4CaloJet80', 'HLT_AK4CaloJet100', 'HLT_AK4PFJet30', 'HLT_AK4PFJet50', 'HLT_AK4PFJet80', 'HLT_AK4PFJet100', 'HLT_HISinglePhoton10', 'HLT_HISinglePhoton15', 'HLT_HISinglePhoton20', 'HLT_HISinglePhoton40', 'HLT_HISinglePhoton60', 'HLT_EcalCalibration', 'HLT_HcalCalibration', 'HLT_GlobalRunHPDNoise', 'HLT_L1BptxMinus', 'HLT_L1BptxPlus', 'HLT_L1NotBptxOR', 'HLT_L1BeamGasMinus', 'HLT_L1BeamGasPlus', 'HLT_L1BptxXOR', 'HLT_L1MinimumBiasHF_OR', 'HLT_L1MinimumBiasHF_AND', 'HLT_HcalNZS', 'HLT_HcalPhiSym', 'HLT_HcalIsolatedbunch', 'HLT_ZeroBias_FirstCollisionAfterAbortGap', 'HLT_ZeroBias_FirstCollisionAfterAbortGap_copy', 'HLT_ZeroBias_FirstCollisionAfterAbortGap_TCDS', 'HLT_ZeroBias_IsolatedBunches', 'HLT_ZeroBias_FirstCollisionInTrain', 'HLT_ZeroBias_FirstBXAfterTrain', 'HLT_Photon500', 'HLT_Photon600', 'HLT_Mu300', 'HLT_Mu350', 'HLT_MET250', 'HLT_MET300', 'HLT_MET600', 'HLT_MET700', 'HLT_PFMET300', 'HLT_PFMET400', 'HLT_PFMET500', 'HLT_PFMET600', 'HLT_Ele250_CaloIdVT_GsfTrkIdT', 'HLT_Ele300_CaloIdVT_GsfTrkIdT', 'HLT_HT2000', 'HLT_HT2500', 'HLT_IsoTrackHE', 'HLT_IsoTrackHB', 'HLTriggerFinalPath', 'L1simulation_step']\n</code></pre> <p>Using the <code>pretty_print</code> function we can get a formatted version of the keys.</p> <pre><code>#### Pretty print all the keys with the default format\n\n#pretty_print(events.keys())\n\n#### Pretty print keys with 30 characters per column, for keys that contain `FatJet`\n#pretty_print(events.keys(), fmt='30s', require='FatJet')\n\n#### Pretty print keys with 40 characters per column, for keys that contain `Muon` and `Iso` but ignore ones with `HLT`\npretty_print(events.keys(), fmt='40s', require=['Muon', 'Iso'], ignore='HLT')\n</code></pre> <pre><code>Muon_jetRelIso                           Muon_miniPFRelIso_all                    \nMuon_miniPFRelIso_chg                    Muon_pfRelIso03_all                      \nMuon_pfRelIso03_chg                      Muon_pfRelIso04_all                      \nMuon_tkRelIso                            Muon_miniIsoId                           \nMuon_multiIsoId                          Muon_pfIsoId                             \nMuon_puppiIsoId                          Muon_tkIsoId\n</code></pre>"},{"location":"Single_Lepton/Single_lepton_comment/#extracting-data","title":"Extracting data","text":"<p>We will extract subsets of data from the files to do our analysis.</p> <p>You can find a list of the variable names in each dataset on the CERN Open Data Portal page for that dataset, for example, here.</p> <p>We will work with the next sets of variables: * <code>Jet</code> for non-merged jets * <code>Muon</code> for muons * <code>Electron</code> for electrons * <code>MET</code> for the missing energy in the transverse plane</p> <p>In addition we will use some <code>Triggers</code> from the dataset.</p> <pre><code># -------------------------------\n# Jets\n# -------------------------------\njet_btag = events['Jet_btagDeepB'].array()\njet_jetid = events['Jet_jetId'].array()\njet_pt = events['Jet_pt'].array()\njet_eta = events['Jet_eta'].array()\njet_phi = events['Jet_phi'].array()\njet_mass = events['Jet_mass'].array()\n\n# -------------------------------\n# Electrons\n# -------------------------------\nelectron_pt = events['Electron_pt'].array()\nelectron_eta = events['Electron_eta'].array()\nelectron_phi = events['Electron_phi'].array()\nelectron_mass = events['Electron_mass'].array()\nelectron_cutB = events['Electron_cutBased'].array()\n\n# -------------------------------\n# Muons\n# -------------------------------\nmuon_pt = events['Muon_pt'].array()\nmuon_eta = events['Muon_eta'].array()\nmuon_phi = events['Muon_phi'].array()\nmuon_mass = events['Muon_mass'].array()\nmuon_loose = events['Muon_looseId'].array()\nmuon_iso = events['Muon_pfRelIso04_all'].array()\n\n# -------------------------------\n# MET\n# -------------------------------\nmet_pt = events['MET_pt'].array()\nmet_eta = 0 * events['MET_pt'].array()\nmet_phi = events['MET_phi'].array()\n\n</code></pre>"},{"location":"Single_Lepton/Single_lepton_comment/#applying-cuts","title":"Applying cuts","text":"<p>The first step to analyze the data is to select the filters that we are going to apply in it. In order to separate the two possibilities in the single lepton channel, we create a <code>CONFIG</code> dictionary to save the parameters that muons and electrons must pass to count them in the single lepton channel.</p> <p>In addition, we save other parameters shared with the all-hadronic channel.</p>"},{"location":"Single_Lepton/Single_lepton_comment/#single-lepton-event-selection-why-these-cuts","title":"Single-Lepton Event Selection: Why These Cuts?","text":"<p>When analyzing single-lepton events (muon or electron) for dark matter searches, the goal is to maximize sensitivity to potential signals while suppressing Standard Model backgrounds like \\(t\\bar{t}\\), W+jets, and QCD multijet events. Each selection criterion has a clear purpose:</p>"},{"location":"Single_Lepton/Single_lepton_comment/#signal-lepton-selection","title":"Signal Lepton Selection","text":"<ul> <li>Exactly one isolated lepton (muon or electron).  </li> <li>Kinematic requirements ensure the lepton is well-reconstructed and on the trigger plateau:</li> <li>Muons: \\(p_T &gt; 26~\\text{GeV}\\), \\(|\\eta| &lt; 2.4\\), Tight ID, relative isolation &lt; 0.15.  </li> <li>Electrons: \\(p_T &gt; 30~\\text{GeV}\\), \\(|\\eta| &lt; 2.5\\), Tight ID, avoid ECAL crack.  </li> <li>Reason: We want a clean, genuine lepton to identify the event topology reliably.</li> </ul>"},{"location":"Single_Lepton/Single_lepton_comment/#jets-and-b-jets","title":"Jets and b-Jets","text":"<ul> <li>At least 2 jets, with at least 1 b-tagged.</li> <li>Criteria: \\(p_T &gt; 30~\\text{GeV}\\), \\(|\\eta| &lt; 2.4\\), Jet ID \u2265 Tight, DeepCSV medium b-tag.</li> <li>Reasoning: Top quarks produce b-jets; this suppresses W+light-flavor jets and enhances top+MET purity.</li> </ul>"},{"location":"Single_Lepton/Single_lepton_comment/#kinematic-variables","title":"Kinematic Variables","text":"<ul> <li>Missing Transverse Energy (MET) &gt; 160 GeV </li> <li>Dark matter escapes undetected \u2192 large MET.  </li> <li>Transverse Mass (MT) &gt; 160 GeV </li> <li>Reduces W+jets background, which peaks near \\(M_W\\).  </li> <li>Azimuthal separation \u0394\u03c6(j1,2, MET) &gt; 1.2 </li> <li>Reduces QCD multijet events with fake MET aligned to jets.  </li> <li>b\u2013MET transverse mass MT(b, MET) &gt; 180 GeV </li> <li>Further discriminates signal from \\(t\\bar{t}\\) events.</li> </ul> <pre><code># -------------------------------\n# Single-lepton selection configs\n# -------------------------------\n\nCONFIG = {\n    \"muon\": {\n        # Selected lepton (muon)\n        \"MU_PT_MIN\": 26.0,        # pT \u2265 26 GeV (HLT_IsoMu24 turn-on)\n        \"MU_ETA_MAX\": 2.4,\n        \"MU_TIGHT_ID\": 1,         # tightId == True\n        \"MU_ISO_MAX\": 0.15,       # relative isolation &lt; 0.15\n\n        # Veto lepton (electron)\n        \"EL_PT_MIN\": 15.0,\n        \"EL_ETA_MAX\": 2.5,\n        \"EL_CUTBASED_MIN\":1,     # cutBased \u2265 Loose\n        \"EL_VETO_ECAL_CRACK\": False,  # you can set True later\n    },\n\n    \"electron\": {\n        # Selected lepton (electron)\n        \"EL_PT_MIN\": 30.0,        # pT \u2265 30 GeV (HLT_Ele27_WPTight)\n        \"EL_ETA_MAX\": 2.5,\n        \"EL_CUTBASED_MIN\": 4,     # cutBased \u2265 Tight\n        \"EL_VETO_ECAL_CRACK\": True,  # 1.4442&lt;|etaSC|&lt;1.566\n\n        # Veto lepton (muon)\n        \"MU_PT_MIN\": 10.0,\n        \"MU_ETA_MAX\": 2.4,\n        \"MU_LOOSE_ID\": 1,         # looseId == True\n        \"MU_ISO_MAX\": 0.25,\n    }\n}\n\n# -------------------------------\n# Common (channel-independent)\n# -------------------------------\nJET_PT_MIN   = 30.0\nJET_ETA_MAX  = 2.4\nJET_ID_MIN   = 2           # 2 = tight, 6 = tightLepVeto\nBTAG_WP_MED  = 0.632       # DeepCSV medium (UL2016)\nFWD_PT_MIN   = 30.0\nFWD_ABS_ETA_MIN = 2.5\nFWD_ABS_ETA_MAX = 4.7\nDR_LEP_JET   = 0.4\n\nMIN_NJETS    = 2\nMIN_NBTAGS   = 1\n\nMET_MIN      = 40.0\nMT_MIN       = 50.0          # mT(W) &gt; 50 GeV\nMT_B_MET_MIN = 160.0         # mT(b, MET) &gt; 160 GeV  (typical DM analysis cut)\nDPHI_J12_MET_MIN = 0.4       # min \u0394\u03c6(j1/2, MET) &gt; 0.4\n\nMT_MIN       = 50.0\n\n</code></pre> <p>Using jets as an example, we can create a filter the common parameters:</p> <pre><code>cut_jet = (abs(jet_eta) &lt; JET_ETA_MAX) &amp; (jet_pt &gt; JET_PT_MIN) &amp; ( jet_jetid &gt;= JET_ID_MIN)\nprint(cut_jet)\n\n</code></pre> <pre><code>[[True, True, False, True, True, False], ..., [True, True, ..., False, False]]\n</code></pre> <p>We can see that the <code>cut_jet</code> cut is an array of <code>True</code> and <code>False</code> that will filter the events.</p> <p>Let's see how this works</p> <pre><code>print(\"Jet pt without cuts\")\nprint(jet_pt)\nprint(\"-----------\")\nprint(\"Jet pt after cuts\")\nprint(jet_pt[cut_jet])\n</code></pre> <pre><code>Jet pt without cuts\n[[81.5, 79.1, 61.2, 49, 34.3, 16.5], ..., [173, 75.4, 54.5, ..., 28.3, 18]]\n-----------\nJet pt after cuts\n[[81.5, 79.1, 49, 34.3], [125, ...], ..., [173, 75.4, 54.5, 53.9, 37.9, 36.2]]\n</code></pre> <p>Let's select the <code>electron</code> channel and create the cuts</p> <pre><code>channel = \"electron\"\ncfg = CONFIG[channel]\n</code></pre> <pre><code># -------------------------------\n# Lepton selections\n# -------------------------------\ncut_electron = (\n            (electron_pt &gt; cfg[\"EL_PT_MIN\"]) &amp;\n            (abs(electron_eta) &lt; cfg[\"EL_ETA_MAX\"]) &amp;\n            (electron_cutB &gt;= cfg[\"EL_CUTBASED_MIN\"])\n        )\ncut_one_lepton = (ak.num(electron_pt[cut_electron]) == 1)\n\ncut_mu_veto = (\n            (muon_pt &gt; cfg[\"MU_PT_MIN\"]) &amp;\n            (abs(muon_eta) &lt; cfg[\"MU_ETA_MAX\"]) &amp;\n            (muon_loose == cfg[\"MU_LOOSE_ID\"]) &amp;\n            (muon_iso &lt; cfg[\"MU_ISO_MAX\"])\n        )\ncut_no_other = (ak.num(muon_pt[cut_mu_veto]) == 0)\n\nlep_pt_single = electron_pt[:,:1]\nlep_phi_single = electron_phi[:,:1]\n</code></pre> <pre><code># Other filters:\n# --- Math utilities ---\ndef delta_phi(phi1, phi2):\n    dphi = phi1 - phi2\n    dphi = (dphi + np.pi) % (2*np.pi) - np.pi\n    return np.abs(dphi)\n\ndef transverse_mass(pt1, phi1, pt2, phi2):\n    dphi = delta_phi(phi1, phi2)\n    return np.sqrt(2 * pt1 * pt2 * (1 - np.cos(dphi)))\n</code></pre> <p>Both functions above calculate the diference in the phi angle and the transverse mass of the event respectively.</p> <pre><code># -------------------------------\n# Jets / forward jets\n# -------------------------------\ncut_jet = (abs(jet_eta) &lt; JET_ETA_MAX) &amp; (jet_pt &gt; JET_PT_MIN) &amp; (jet_jetid &gt;= JET_ID_MIN)\ncut_nj = ak.num(jet_pt[cut_jet]) &gt;= MIN_NJETS\ncut_bjet = (jet_btag &gt; BTAG_WP_MED)\ncut_fj = (\n        (jet_pt &gt; FWD_PT_MIN) &amp;\n        (abs(jet_eta) &gt; FWD_ABS_ETA_MIN) &amp;\n        (abs(jet_eta) &lt; FWD_ABS_ETA_MAX) &amp;\n        (jet_jetid &gt;= JET_ID_MIN)\n    )\n\n# -------------------------------\n# Special kinematics\n# -------------------------------\nmT = transverse_mass(lep_pt_single, lep_phi_single, met_pt, met_phi)\ncut_mtw = ak.fill_none(ak.firsts(mT &gt; MT_MIN), False)\n\nbest_bjet_idx = ak.argmax(jet_btag, axis=1, keepdims=True)\nbest_bjet_pt = jet_pt[best_bjet_idx]\nbest_bjet_phi = jet_phi[best_bjet_idx]\nmTb = transverse_mass(ak.flatten(best_bjet_pt), ak.flatten(best_bjet_phi), met_pt, met_phi)\n\njet1_phi = jet_phi[:, :1]\njet2_phi = jet_phi[:, 1:2]\ndphi_j1 = delta_phi(jet1_phi, met_phi)\ndphi_j2 = delta_phi(jet2_phi, met_phi)\nmin_dphi = ak.min(ak.concatenate([dphi_j1[:, None], dphi_j2[:, None]], axis=1), axis=1)\n\ncut_mtb = mTb &gt; MT_B_MET_MIN\ncut_dphi = ak.fill_none(ak.firsts(min_dphi &gt; DPHI_J12_MET_MIN), False)\ncut_esp = cut_mtw &amp; cut_mtb &amp; cut_dphi\n\n# -------------------------------\n# Event-level cuts\n# -------------------------------\ncut_met = (met_pt &gt; MET_MIN)\ncut_base = cut_nj &amp; cut_one_lepton &amp; cut_no_other &amp; cut_met &amp; cut_esp\n\ncut_1b = ak.count(jet_pt[cut_bjet], axis=1) == 1\ncut_0fj = ak.num(jet_pt[cut_fj]) == 0\ncut1 = cut_base &amp; cut_1b &amp; cut_0fj\n\ncut_full_event = cut1\n</code></pre>"},{"location":"Single_Lepton/Single_lepton_comment/#building-output-objects","title":"Building output objects","text":"<p>Using the Vector class and the akward library we will create 4-vector objects.</p> <pre><code># -------------------------------\n# Build output objects\n# -------------------------------\nmet = ak.zip(\n        {\"pt\": met_pt[cut_full_event],\n         \"eta\": met_eta[cut_full_event],\n         \"phi\": met_phi[cut_full_event],\n         \"mass\": 0},\n        with_name=\"Momentum4D\",\n    )\n\nif channel == \"muon\":\n    lep = ak.zip(\n            {\"pt\": muon_pt[cut_full_event][cut_muon[cut_full_event]],\n             \"eta\": muon_eta[cut_full_event][cut_muon[cut_full_event]],\n             \"phi\": muon_phi[cut_full_event][cut_muon[cut_full_event]],\n             \"mass\": muon_mass[cut_full_event][cut_muon[cut_full_event]]},\n            with_name=\"Momentum4D\",\n        )\nelse:\n    lep = ak.zip(\n            {\"pt\": electron_pt[cut_full_event][cut_electron[cut_full_event]],\n             \"eta\": electron_eta[cut_full_event][cut_electron[cut_full_event]],\n             \"phi\": electron_phi[cut_full_event][cut_electron[cut_full_event]],\n             \"mass\": electron_mass[cut_full_event][cut_electron[cut_full_event]]},\n            with_name=\"Momentum4D\",\n        )\n\n</code></pre> <p>Let's plot the lepton (electron) momentum.</p> <pre><code>x = lep.pt\n\n# Plot it!\nplt.hist(ak.flatten(x,axis=None), bins=100, range=(0,500))\nplt.xlabel(f'$p_{{T}}$ $GeV$', fontsize=18)\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"Single_Lepton/Single_lepton_comment/#processing-the-files","title":"Processing the files","text":"<p>From what we showed you above, we define a <code>process_file</code> function to process the files.</p> <p>The first thing it does is open the ROOT file from the files we made at the beginning. If theres no problem it prints the number of events in the file.</p> <p>Then it loads the configuration that we selected above and access the required information in the dataset, for example the B-tagging variable <code>Jet_btagDeepB</code>.</p> <p>According to the specified <code>channel</code> it applies the parameters for muons or electrons. Then it applies the cuts for the events, for instance filtering the transverse mass of the event.</p> <p>After that we save the object we are interested in, in this case the MET of the event, using the <code>akward</code> library.</p> <pre><code># ==========================================================\n# Generic processing function with channel support\n# ==========================================================\ndef process_file(filename, dataset='default', channel='muon', IS_DATA=False):\n    print(f\"Opening...{filename}\")\n    try:\n        f = uproot.open(filename)\n    except:\n        print(f\"Could not open {filename}\")\n        return None\n\n    events = f['Events']\n    nevents = events.num_entries\n    print(f\"{nevents = }\")\n\n    # -------------------------------\n    # Load channel-specific config\n    # -------------------------------\n    cfg = CONFIG[channel]\n\n    # -------------------------------\n    # Jets\n    # -------------------------------\n    jet_btag = events['Jet_btagDeepB'].array()\n    jet_jetid = events['Jet_jetId'].array()\n    jet_pt = events['Jet_pt'].array()\n    jet_eta = events['Jet_eta'].array()\n    jet_phi = events['Jet_phi'].array()\n    jet_mass = events['Jet_mass'].array()\n\n    # -------------------------------\n    # Electrons\n    # -------------------------------\n    electron_pt = events['Electron_pt'].array()\n    electron_eta = events['Electron_eta'].array()\n    electron_phi = events['Electron_phi'].array()\n    electron_mass = events['Electron_mass'].array()\n    electron_cutB = events['Electron_cutBased'].array()\n\n    # -------------------------------\n    # Muons\n    # -------------------------------\n    muon_pt = events['Muon_pt'].array()\n    muon_eta = events['Muon_eta'].array()\n    muon_phi = events['Muon_phi'].array()\n    muon_mass = events['Muon_mass'].array()\n    muon_loose = events['Muon_looseId'].array()\n    muon_iso = events['Muon_pfRelIso04_all'].array()\n\n    # -------------------------------\n    # MET\n    # -------------------------------\n    met_pt = events['MET_pt'].array()\n    met_eta = 0 * events['MET_pt'].array()\n    met_phi = events['MET_phi'].array()\n\n    # -------------------------------\n    # Lepton selections\n    # -------------------------------\n    if channel == \"muon\":\n        cut_muon = (\n            (muon_pt &gt; cfg[\"MU_PT_MIN\"]) &amp;\n            (abs(muon_eta) &lt; cfg[\"MU_ETA_MAX\"]) &amp;\n            (events['Muon_tightId'].array() == cfg[\"MU_TIGHT_ID\"]) &amp;\n            (muon_iso &lt; cfg[\"MU_ISO_MAX\"])\n        )\n        cut_one_lepton = (ak.num(muon_pt[cut_muon]) == 1)\n\n        cut_e_veto = (\n            (electron_pt &gt; cfg[\"EL_PT_MIN\"]) &amp;\n            (abs(electron_eta) &lt; cfg[\"EL_ETA_MAX\"]) &amp;\n            (electron_cutB &gt;= cfg[\"EL_CUTBASED_MIN\"])\n        )\n        cut_no_other = (ak.num(electron_pt[cut_e_veto]) == 0)\n\n        lep_pt_single = muon_pt[:,:1]\n        lep_phi_single = muon_phi[:,:1]\n\n    elif channel == \"electron\":\n        cut_electron = (\n            (electron_pt &gt; cfg[\"EL_PT_MIN\"]) &amp;\n            (abs(electron_eta) &lt; cfg[\"EL_ETA_MAX\"]) &amp;\n            (electron_cutB &gt;= cfg[\"EL_CUTBASED_MIN\"])\n        )\n        cut_one_lepton = (ak.num(electron_pt[cut_electron]) == 1)\n\n        cut_mu_veto = (\n            (muon_pt &gt; cfg[\"MU_PT_MIN\"]) &amp;\n            (abs(muon_eta) &lt; cfg[\"MU_ETA_MAX\"]) &amp;\n            (muon_loose == cfg[\"MU_LOOSE_ID\"]) &amp;\n            (muon_iso &lt; cfg[\"MU_ISO_MAX\"])\n        )\n        cut_no_other = (ak.num(muon_pt[cut_mu_veto]) == 0)\n\n        lep_pt_single = electron_pt[:,:1]\n        lep_phi_single = electron_phi[:,:1]\n\n    # -------------------------------\n    # Jets / forward jets\n    # -------------------------------\n    cut_jet = (abs(jet_eta) &lt; JET_ETA_MAX) &amp; (jet_pt &gt; JET_PT_MIN) &amp; (jet_jetid &gt;= JET_ID_MIN)\n    cut_nj = ak.num(jet_pt[cut_jet]) &gt;= MIN_NJETS\n    cut_bjet = (jet_btag &gt; BTAG_WP_MED)\n    cut_fj = (\n        (jet_pt &gt; FWD_PT_MIN) &amp;\n        (abs(jet_eta) &gt; FWD_ABS_ETA_MIN) &amp;\n        (abs(jet_eta) &lt; FWD_ABS_ETA_MAX) &amp;\n        (jet_jetid &gt;= JET_ID_MIN)\n    )\n\n    # -------------------------------\n    # Special kinematics\n    # -------------------------------\n    mT = transverse_mass(lep_pt_single, lep_phi_single, met_pt, met_phi)\n    cut_mtw = ak.fill_none(ak.firsts(mT &gt; MT_MIN), False)\n\n    best_bjet_idx = ak.argmax(jet_btag, axis=1, keepdims=True)\n    best_bjet_pt = jet_pt[best_bjet_idx]\n    best_bjet_phi = jet_phi[best_bjet_idx]\n    mTb = transverse_mass(ak.flatten(best_bjet_pt), ak.flatten(best_bjet_phi), met_pt, met_phi)\n\n    jet1_phi = jet_phi[:, :1]\n    jet2_phi = jet_phi[:, 1:2]\n    dphi_j1 = delta_phi(jet1_phi, met_phi)\n    dphi_j2 = delta_phi(jet2_phi, met_phi)\n    min_dphi = ak.min(ak.concatenate([dphi_j1[:, None], dphi_j2[:, None]], axis=1), axis=1)\n\n    cut_mtb = mTb &gt; MT_B_MET_MIN\n    cut_dphi = ak.fill_none(ak.firsts(min_dphi &gt; DPHI_J12_MET_MIN), False)\n    cut_esp = cut_mtw &amp; cut_mtb &amp; cut_dphi\n\n    # -------------------------------\n    # Event-level cuts\n    # -------------------------------\n    cut_met = (met_pt &gt; MET_MIN)\n    cut_base = cut_nj &amp; cut_one_lepton &amp; cut_no_other &amp; cut_met &amp; cut_esp\n\n    cut_1b = ak.count(jet_pt[cut_bjet], axis=1) == 1\n    cut_0fj = ak.num(jet_pt[cut_fj]) == 0\n    cut1 = cut_base &amp; cut_1b &amp; cut_0fj\n\n    cut_full_event = cut1\n    if IS_DATA:\n        mask_lumi = build_lumi_mask('Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt', events)\n        cut_full_event = cut1 &amp; mask_lumi\n\n    # -------------------------------\n    # Build output objects\n    # -------------------------------\n    met = ak.zip(\n        {\"pt\": met_pt[cut_full_event],\n         \"eta\": met_eta[cut_full_event],\n         \"phi\": met_phi[cut_full_event],\n         \"mass\": 0},\n        with_name=\"Momentum4D\",\n    )\n\n    if channel == \"muon\":\n        lep = ak.zip(\n            {\"pt\": muon_pt[cut_full_event][cut_muon[cut_full_event]],\n             \"eta\": muon_eta[cut_full_event][cut_muon[cut_full_event]],\n             \"phi\": muon_phi[cut_full_event][cut_muon[cut_full_event]],\n             \"mass\": muon_mass[cut_full_event][cut_muon[cut_full_event]]},\n            with_name=\"Momentum4D\",\n        )\n    else:\n        lep = ak.zip(\n            {\"pt\": electron_pt[cut_full_event][cut_electron[cut_full_event]],\n             \"eta\": electron_eta[cut_full_event][cut_electron[cut_full_event]],\n             \"phi\": electron_phi[cut_full_event][cut_electron[cut_full_event]],\n             \"mass\": electron_mass[cut_full_event][cut_electron[cut_full_event]]},\n            with_name=\"Momentum4D\",\n        )\n\n    # -------------------------------\n    # Weights / pileup\n    # -------------------------------\n    N_gen, gw_pos, gw_neg = -999, -999, -999\n    tmpval_events = np.ones(len(ak.flatten(met.pt, axis=None)))\n    tmpval = ak.ones_like(ak.flatten(met.pt, axis=None))\n\n    if not IS_DATA:\n        gen_weights = events['genWeight'].array()[cut_full_event]\n        pileup = events['Pileup_nTrueInt'].array()[cut_full_event]\n\n        if len(gen_weights) != len(tmpval):\n            gen_weights = ak.flatten(gen_weights, axis=None)\n            pileup = ak.flatten(pileup, axis=None)\n\n        gen_weights_per_candidate = tmpval * gen_weights\n        pileup_per_candidate = tmpval * pileup\n\n        gen_weights_org = events['genWeight'].array()\n        gw_pos = ak.count(gen_weights_org[gen_weights_org &gt; 0])\n        gw_neg = ak.count(gen_weights_org[gen_weights_org &lt; 0])\n        N_gen = gw_pos - gw_neg\n    else:\n        pileup_per_candidate = -999 * tmpval\n        gen_weights_per_candidate = -999 * tmpval\n\n    # -------------------------------\n    # Save to DataFrame\n    # -------------------------------\n    mydict = {\n        'met_pt': ak.flatten(met.pt, axis=None),\n        'lep_pt': ak.flatten(lep.pt, axis=None),\n        'pileup': pileup_per_candidate,\n        'weight': gen_weights_per_candidate,\n        'nevents': nevents * tmpval_events,\n        'N_gen': N_gen * tmpval_events,\n        'gw_pos': gw_pos * tmpval_events,\n        'gw_neg': gw_neg * tmpval_events\n    }\n\n    df = pd.DataFrame.from_dict(mydict)\n    outfilename = f\"OUTPUT_{dataset}_{channel}_{filename.split('/')[-1].split('.')[0]}.csv\"\n    print(f\"Saving output to {outfilename}\")\n    df.to_csv(outfilename, index=False)\n    return df\n\n</code></pre> <p>To simplify the selection of the channel, we have created two functions using the <code>process_file</code> function and assigning them a specific channel.</p> <pre><code># ==========================================================\n# Processing function for single-muon channel\n# ==========================================================\ndef process_file_muon(filename, dataset='default', IS_DATA=False):\n    return process_file(filename, dataset=dataset, channel=\"muon\", IS_DATA=IS_DATA)\n\n\n# ==========================================================\n# Processing function for single-electron channel\n# ==========================================================\ndef process_file_electron(filename, dataset='default', IS_DATA=False):\n    return process_file(filename, dataset=dataset, channel=\"electron\", IS_DATA=IS_DATA)\n\n</code></pre> <p>Now we can use these functions to process some data</p> <pre><code>dataset = \"ttsemilep\"\nfilenames = get_files_for_dataset(dataset, random=True, n=1)  # por ejemplo\nfor filename in filenames:\n    df = process_file_muon(filename, dataset=dataset, IS_DATA=False)\n\n</code></pre> <pre><code>Opening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/120000/301EA765-5A14-1B43-ADED-D3BE6147134B.root\n\nnevents = 905000\nSaving output to OUTPUT_ttsemilep_muon_301EA765-5A14-1B43-ADED-D3BE6147134B.csv\n</code></pre> <pre><code>dataset = \"ttsemilep\"\nfilenames = get_files_for_dataset(dataset, random=True, n=1)  \nfor filename in filenames:\n    df = process_file_electron(filename, dataset=dataset, IS_DATA=False)\n\n</code></pre> <pre><code>Opening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/120000/581CBD80-7D60-5E42-9952-B1234440AE4F.root\n\nnevents = 336000\nSaving output to OUTPUT_ttsemilep_electron_581CBD80-7D60-5E42-9952-B1234440AE4F.csv\n</code></pre> <pre><code>#channel mu\u00f3n\ndataset = \"collsm\"   # SingleMuon data\nfiles = get_files_for_dataset(dataset, random=True, n=1)  \n\nfor f in files:\n    df_data_mu = process_file_muon(f, dataset=dataset, IS_DATA=True)\n\n</code></pre> <pre><code>Opening...root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/70000/01479010-3B51-B04A-9C5C-7800085D11B8.root\n\nnevents = 887639\nSaving output to OUTPUT_collsm_muon_01479010-3B51-B04A-9C5C-7800085D11B8.csv\n</code></pre> <pre><code>#channel electr\u00f3n\ndataset = \"collse\"   # SingleElectron data\nfiles = get_files_for_dataset(dataset, random=True, n=1)\n\nfor f in files:\n    df_data_el = process_file_electron(f, dataset=dataset, IS_DATA=True)\n\n</code></pre> <pre><code>Opening...root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleElectron/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/270000/0D3360E0-CCE2-3543-A953-620C362B10C2.root\n\nnevents = 864865\nSaving output to OUTPUT_collse_electron_0D3360E0-CCE2-3543-A953-620C362B10C2.csv\n</code></pre> <p>As there are lots of datasets, we are going to add them to a single file. In order to do that we are going to need a list of <code>datasets</code> and <code>channels</code>.</p> <pre><code>import glob\nimport os\nimport pandas as pd\n\ndatasets = ['ttsemilep', 'collsm','collse']   # MC and data\nchannels = ['muon', 'electron']     # both channels\n\nfor dataset in datasets:\n    for channel in channels:\n        path = './'\n        all_files = glob.glob(os.path.join(path, f\"OUTPUT_{dataset}_{channel}_*.csv\"))\n\n        print(f\"Found {len(all_files)} files for {dataset} ({channel})\")\n\n        if len(all_files) == 0:\n            print(f\"No files found for {dataset} ({channel}), skipping.\")\n            continue\n\n        dfs = [pd.read_csv(f) for f in all_files]\n        df = pd.concat(dfs, ignore_index=True)\n\n        # Recalcular totales\n        N_gen = df['N_gen'].iloc[0]\n        nevents = df['nevents'].iloc[0]\n\n        df['N_gen'] = N_gen\n        df['nevents'] = nevents\n\n        outname = f\"SUMMED_{dataset}_{channel}.csv\"\n        df.to_csv(outname, index=False)\n        print(f\"Saved {outname}\")\n\n</code></pre> <pre><code>Found 1 files for ttsemilep (muon)\nSaved SUMMED_ttsemilep_muon.csv\nFound 1 files for ttsemilep (electron)\nSaved SUMMED_ttsemilep_electron.csv\nFound 1 files for collsm (muon)\nSaved SUMMED_collsm_muon.csv\nFound 0 files for collsm (electron)\nNo files found for collsm (electron), skipping.\nFound 0 files for collse (muon)\nNo files found for collse (muon), skipping.\nFound 1 files for collse (electron)\nSaved SUMMED_collse_electron.csv\n</code></pre> <p>Now we can access the data and plot it as you can see below</p> <pre><code>import matplotlib.pyplot as plt\n\ndf_mu = pd.read_csv(\"SUMMED_ttsemilep_muon.csv\")\ndf_el = pd.read_csv(\"SUMMED_ttsemilep_electron.csv\")\n\nplt.hist(df_mu['lep_pt'], bins=30, range=(0,300), histtype=\"step\", label=\"Muon channel\")\nplt.hist(df_el['lep_pt'], bins=30, range=(0,300), histtype=\"step\", label=\"Electron channel\")\nplt.xlabel(\"Lepton $p_T$ [GeV]\")\nplt.ylabel(\"Events\")\nplt.legend()\nplt.show()\n\n</code></pre> <p></p> <pre><code>import matplotlib.pyplot as plt\n\ndf_mu = pd.read_csv(\"SUMMED_ttsemilep_muon.csv\")\ndf_el = pd.read_csv(\"SUMMED_ttsemilep_electron.csv\")\n\nplt.hist(df_mu['met_pt'], bins=30, range=(0,300), histtype=\"step\", label=\"Muon channel\")\nplt.hist(df_el['met_pt'], bins=30, range=(0,300), histtype=\"step\", label=\"Electron channel\")\nplt.xlabel(\"Lepton $met$ [GeV]\")\nplt.ylabel(\"Events\")\nplt.legend()\nplt.show()\n\n</code></pre> <p></p> <pre><code># Cargar Data y MC\ndf_data_mu = pd.read_csv(\"SUMMED_collsm_muon.csv\")\ndf_mc_mu   = pd.read_csv(\"SUMMED_ttsemilep_muon.csv\")\n\ndf_data_el = pd.read_csv(\"SUMMED_collse_electron.csv\")\ndf_mc_el   = pd.read_csv(\"SUMMED_ttsemilep_electron.csv\")\n</code></pre> <pre><code># Shapes normalizados (misma \u00e1rea)\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.hist(df_mc_mu['lep_pt'], bins=30, range=(0,300), density=True,\n         histtype=\"step\", label=\"MC (shape)\")\nplt.hist(df_data_mu['lep_pt'], bins=30, range=(0,300), density=True,\n         histtype=\"step\", label=\"Data (shape)\")\nplt.xlabel(\"Muon $p_T$ [GeV]\"); plt.ylabel(\"Normalized\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.hist(df_mc_el['lep_pt'], bins=30, range=(0,300), density=True,\n         histtype=\"step\", label=\"MC (shape)\")\nplt.hist(df_data_el['lep_pt'], bins=30, range=(0,300), density=True,\n         histtype=\"step\", label=\"Data (shape)\")\nplt.xlabel(\"Electron $p_T$ [GeV]\"); plt.ylabel(\"Normalized\")\nplt.legend()\nplt.tight_layout(); plt.show()\n\n</code></pre> <p></p> <pre><code># Shapes normalizados (misma \u00e1rea)\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.hist(df_mc_mu['met_pt'], bins=30, range=(0,300), density=True,\n         histtype=\"step\", label=\"MC (shape)\")\nplt.hist(df_data_mu['met_pt'], bins=30, range=(0,300), density=True,\n         histtype=\"step\", label=\"Data (shape)\")\nplt.xlabel(\"Muon $p_T$ [GeV]\"); plt.ylabel(\"Normalized\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.hist(df_mc_el['met_pt'], bins=30, range=(0,300), density=True,\n         histtype=\"step\", label=\"MC (shape)\")\nplt.hist(df_data_el['met_pt'], bins=30, range=(0,300), density=True,\n         histtype=\"step\", label=\"Data (shape)\")\nplt.xlabel(\"Electron $p_T$ [GeV]\"); plt.ylabel(\"Normalized\")\nplt.legend()\nplt.tight_layout(); plt.show()\n\n</code></pre> <p></p> <p>Here we want to get ahead with the normalization, so that we can also compare shapes and see the value of the luminosity, which clearly should go with a dataset. This is just a preliminary step; it will be explained in more detail later because of the normalization.--Ignore</p> <pre><code>import matplotlib.pyplot as plt\n\n# Cargar Data y MC\ndf_data_mu = pd.read_csv(\"SUMMED_collsm_muon.csv\")\ndf_mc_mu   = pd.read_csv(\"SUMMED_ttsemilep_muon.csv\")\n\ndf_data_el = pd.read_csv(\"SUMMED_collse_electron.csv\")\ndf_mc_el   = pd.read_csv(\"SUMMED_ttsemilep_electron.csv\")\n\n# Par\u00e1metros f\u00edsicos\nintegrated_luminosity = 60.0  # fb^-1 (ejemplo, dataset peque\u00f1o)\nxsec_ttsemilep = 831.76     # pb (aprox.)\n\n\n\n\n# Calcular pesos MC\nN_gen_mc_mu = df_mc_mu['N_gen'][0]\nN_gen_mc_el = df_mc_el['N_gen'][0]\n\nweight_mc_mu = integrated_luminosity * xsec_ttsemilep / N_gen_mc_mu\nweight_mc_el = integrated_luminosity * xsec_ttsemilep / N_gen_mc_el\n\n# ========================\n# Plots\n# ========================\n\nplt.figure(figsize=(12,5))\n\n# Muon channel\nplt.subplot(1,2,1)\nplt.hist(df_mc_mu['lep_pt'], bins=30, range=(0,300), \n         weights=[weight_mc_mu]*len(df_mc_mu), alpha=0.5, label=\"MC ttsemilep\")\nplt.hist(df_data_mu['lep_pt'], bins=30, range=(0,300), \n         histtype=\"step\", color=\"k\", label=\"Data (SingleMuon)\")\nplt.xlabel(\"Muon $p_T$ [GeV]\")\nplt.ylabel(\"Events\")\nplt.legend()\n\n# Electron channel\nplt.subplot(1,2,2)\nplt.hist(df_mc_el['lep_pt'], bins=30, range=(0,300), \n         weights=[weight_mc_el]*len(df_mc_el), alpha=0.5, label=\"MC ttsemilep\")\nplt.hist(df_data_el['lep_pt'], bins=30, range=(0,300), \n         histtype=\"step\", color=\"k\", label=\"Data (SingleElectron)\")\nplt.xlabel(\"Electron $p_T$ [GeV]\")\nplt.ylabel(\"Events\")\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(\"lep_pt_data_vs_mc.png\")\nplt.show()\n\n</code></pre> <pre><code>/tmp/ipykernel_116/3175142839.py:50: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n</code></pre> <p></p> <pre><code>import matplotlib.pyplot as plt\n\n# Cargar Data y MC\ndf_data_mu = pd.read_csv(\"SUMMED_collsm_muon.csv\")\ndf_mc_mu   = pd.read_csv(\"SUMMED_ttsemilep_muon.csv\")\n\ndf_data_el = pd.read_csv(\"SUMMED_collse_electron.csv\")\ndf_mc_el   = pd.read_csv(\"SUMMED_ttsemilep_electron.csv\")\n\n# Par\u00e1metros f\u00edsicos\nintegrated_luminosity = 60.0  # pb^-1 (ejemplo, dataset peque\u00f1o)\nxsec_ttsemilep = 831.76     # pb (aprox.)\n\n\n\n\n# Calcular pesos MC\nN_gen_mc_mu = df_mc_mu['N_gen'][0]\nN_gen_mc_el = df_mc_el['N_gen'][0]\n\nweight_mc_mu = integrated_luminosity * xsec_ttsemilep / N_gen_mc_mu\nweight_mc_el = integrated_luminosity * xsec_ttsemilep / N_gen_mc_el\n\n# ========================\n# Plots\n# ========================\n\nplt.figure(figsize=(12,5))\n\n# Muon channel\nplt.subplot(1,2,1)\nplt.hist(df_mc_mu['met_pt'], bins=30, range=(0,300), \n         weights=[weight_mc_mu]*len(df_mc_mu), alpha=0.5, label=\"MC ttsemilep\")\nplt.hist(df_data_mu['met_pt'], bins=30, range=(0,300), \n         histtype=\"step\", color=\"k\", label=\"Data (SingleMuon)\")\nplt.xlabel(\"Muon $met$ [GeV]\")\nplt.ylabel(\"Events\")\nplt.legend()\n\n# Electron channel\nplt.subplot(1,2,2)\nplt.hist(df_mc_el['met_pt'], bins=30, range=(0,300), \n         weights=[weight_mc_el]*len(df_mc_el), alpha=0.5, label=\"MC ttsemilep\")\nplt.hist(df_data_el['met_pt'], bins=30, range=(0,300), \n         histtype=\"step\", color=\"k\", label=\"Data (SingleElectron)\")\nplt.xlabel(\"Electron $met$ [GeV]\")\nplt.ylabel(\"Events\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n</code></pre> <p></p>"},{"location":"Single_Lepton/Single_lepton_comment/#to-process-all-available-data-sets","title":"To process all available data sets","text":""},{"location":"Single_Lepton/Single_lepton_comment/#muon","title":"Muon","text":"<pre><code># ==========================\n# Process all datasets (muon channel)\n# ==========================\n#datasets = ['collsm', 'ttsemilep', 'ttop', 'ttv', 'Wjets']\ndatasets = ['ttop', 'ttv', 'Wjets']\n\n\nfor dataset in datasets:\n    IS_DATA = dataset.startswith(\"coll\")  # True for collision data, False for MC\n    filenames = get_files_for_dataset(dataset, random=True, n=1)  # adjust n as needed\n\n    print(f\"Processing {len(filenames)} files for {dataset} (muon channel)\")\n\n    for filename in filenames:\n        df_mu = process_file_muon(filename, dataset=dataset, IS_DATA=IS_DATA)\n\n</code></pre> <pre><code>Processing 1 files for ttop (muon channel)\nOpening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/ST_t-channel_top_4f_InclusiveDecays_TuneCP5CR2_13TeV-powheg-madspin-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/2500000/C7DFF052-C1CD-6747-AAD5-73AA56AD82CB.root\n\nnevents = 895000\nSaving output to OUTPUT_ttop_muon_C7DFF052-C1CD-6747-AAD5-73AA56AD82CB.csv\nProcessing 1 files for ttv (muon channel)\nOpening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTWJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-madspin-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/270000/FD0364C0-B9CF-9C43-ADE8-A950A91D5A8D.root\n\nnevents = 673411\nSaving output to OUTPUT_ttv_muon_FD0364C0-B9CF-9C43-ADE8-A950A91D5A8D.csv\nProcessing 1 files for Wjets (muon channel)\nOpening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/WJetsToLNu_HT-400To600_TuneCP5_13TeV-madgraphMLM-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/280000/216B70A2-8211-2B4E-8B82-F3CB8F097641.root\n\nnevents = 197244\nSaving output to OUTPUT_Wjets_muon_216B70A2-8211-2B4E-8B82-F3CB8F097641.csv\n</code></pre>"},{"location":"Single_Lepton/Single_lepton_comment/#electron","title":"Electron","text":"<pre><code># ==========================\n# Process all datasets (electron channel)\n# ==========================\n#datasets = ['collse', 'ttsemilep', 'ttop', 'ttv', 'Wjets']\ndatasets = ['ttop', 'ttv', 'Wjets']\n\n\nfor dataset in datasets:\n    IS_DATA = dataset.startswith(\"coll\")\n    filenames = get_files_for_dataset(dataset, random=True, n=1)\n\n    print(f\" Processing {len(filenames)} files for {dataset} (electron channel)\")\n\n    for filename in filenames:\n        df_el = process_file_electron(filename, dataset=dataset, IS_DATA=IS_DATA)\n\n</code></pre> <pre><code> Processing 1 files for ttop (electron channel)\nOpening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/ST_t-channel_top_4f_InclusiveDecays_TuneCP5CR2_13TeV-powheg-madspin-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/2500000/46E58BC2-6B38-1845-BE51-0C65F7D8CDB6.root\n\nnevents = 1241000\nSaving output to OUTPUT_ttop_electron_46E58BC2-6B38-1845-BE51-0C65F7D8CDB6.csv\n Processing 1 files for ttv (electron channel)\nOpening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTWJetsToLNu_TuneCP5_13TeV-amcatnloFXFX-madspin-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/130000/C6039E7C-9B89-F84E-A1BE-309DD4A34153.root\n\nnevents = 272723\nSaving output to OUTPUT_ttv_electron_C6039E7C-9B89-F84E-A1BE-309DD4A34153.csv\n Processing 1 files for Wjets (electron channel)\nOpening...root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/WJetsToLNu_HT-400To600_TuneCP5_13TeV-madgraphMLM-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/120000/30407250-6834-4642-AD3A-0AB0D1F9E639.root\n\nnevents = 41364\nSaving output to OUTPUT_Wjets_electron_30407250-6834-4642-AD3A-0AB0D1F9E639.csv\n</code></pre>"},{"location":"Single_Lepton/Single_lepton_comment/#uniting-the-two","title":"Uniting the two","text":"<pre><code>import glob\nimport os\nimport pandas as pd\n\ndatasets = ['collse', 'collsm', 'ttsemilep', 'ttop', 'ttv', 'Wjets']\nchannels = ['muon', 'electron']   # ensure we split per channel\n\nfor dataset in datasets:\n    for channel in channels:\n        N_gen = 0\n        nevents = 0\n        gw_pos = 0\n        gw_neg = 0\n        IS_COLLISION = dataset.startswith(\"coll\")\n\n        path = './'\n        all_files = glob.glob(os.path.join(path, f\"OUTPUT_{dataset}_{channel}_*.csv\"))\n\n        print(f\"\ud83d\udd0e Processing {len(all_files)} files in {dataset} ({channel})\")\n\n        if len(all_files) == 0:\n            print(f\"\u26a0\ufe0fNo files found for {dataset} ({channel}), skipping.\")\n            continue\n\n        list_of_dataframes = []\n\n        for filename in all_files:\n            df = pd.read_csv(filename, index_col=None, header=0)\n            list_of_dataframes.append(df)\n\n            if len(df) &gt; 0:\n                N_gen += df['N_gen'][0]\n                nevents += df['nevents'][0]\n                gw_pos += df['gw_pos'][0]\n                gw_neg += df['gw_neg'][0]\n\n        # For real collision data we don\u2019t use MC weights\n        if IS_COLLISION:\n            N_gen = -999\n            gw_pos = -999\n            gw_neg = -999\n\n        df = pd.concat(list_of_dataframes, axis=0, ignore_index=True)\n        df['N_gen'] = N_gen\n        df['nevents'] = nevents\n        df['gw_pos'] = gw_pos\n        df['gw_neg'] = gw_neg\n\n        outname = f\"SUMMED_{dataset}_{channel}.csv\"\n        df.to_csv(outname, index=False)\n        print(f\"\u2705 Saved {outname}\")\n\n</code></pre> <pre><code>\ud83d\udd0e Processing 0 files in collse (muon)\n\u26a0\ufe0fNo files found for collse (muon), skipping.\n\ud83d\udd0e Processing 1 files in collse (electron)\n\u2705 Saved SUMMED_collse_electron.csv\n\ud83d\udd0e Processing 1 files in collsm (muon)\n\u2705 Saved SUMMED_collsm_muon.csv\n\ud83d\udd0e Processing 0 files in collsm (electron)\n\u26a0\ufe0fNo files found for collsm (electron), skipping.\n\ud83d\udd0e Processing 1 files in ttsemilep (muon)\n\u2705 Saved SUMMED_ttsemilep_muon.csv\n\ud83d\udd0e Processing 1 files in ttsemilep (electron)\n\u2705 Saved SUMMED_ttsemilep_electron.csv\n\ud83d\udd0e Processing 1 files in ttop (muon)\n\u2705 Saved SUMMED_ttop_muon.csv\n\ud83d\udd0e Processing 1 files in ttop (electron)\n\u2705 Saved SUMMED_ttop_electron.csv\n\ud83d\udd0e Processing 1 files in ttv (muon)\n\u2705 Saved SUMMED_ttv_muon.csv\n\ud83d\udd0e Processing 1 files in ttv (electron)\n\u2705 Saved SUMMED_ttv_electron.csv\n\ud83d\udd0e Processing 1 files in Wjets (muon)\n\u2705 Saved SUMMED_Wjets_muon.csv\n\ud83d\udd0e Processing 1 files in Wjets (electron)\n\u2705 Saved SUMMED_Wjets_electron.csv\n</code></pre>"},{"location":"Single_Lepton/Single_lepton_comment/#why-normalization-is-necessary","title":"Why Normalization is Necessary","text":"<p>When comparing data and Monte Carlo (MC) simulations, the raw event counts are not directly comparable:</p> <ul> <li>Data:  </li> <li>Events are collected with a detector during a given time period.  </li> <li>The \"size\" of the dataset is controlled by the integrated luminosity (L).  </li> <li>Example: UL2016 SingleMuon dataset corresponds to ~35.9 fb\u207b\u00b9.  </li> <li> <p>There is no cross section attached \u2014 it is just what was recorded.</p> </li> <li> <p>MC (simulations):  </p> </li> <li>Each simulated dataset corresponds to a particular physics process (e.g. t t\u0304, W+jets).  </li> <li>Generators simulate a finite number of events (N\u208dgen\u208e) with a known theoretical cross section (\u03c3).  </li> <li>By construction, MC samples may represent more or fewer events than what would be seen in real data.  </li> <li>Therefore, they must be scaled.</li> </ul>"},{"location":"Single_Lepton/Single_lepton_comment/#the-normalization-formula","title":"The Normalization Formula","text":"<p>To make MC comparable to data, we apply a per-event weight:</p> \\[ w = \\frac{\\sigma \\cdot L}{N_\\text{gen}} \\] <p>Where: - \u03c3 (pb) = process cross section (from theory/measurements). - L (fb\u207b\u00b9) = integrated luminosity of the data sample.   - Convert to pb\u207b\u00b9 by multiplying by 1000. - N\u208dgen\u208e = total number of generated MC events (before cuts).  </p> <p>This weight ensures that when we sum the MC events after applying cuts, the histograms reflect the expected yield in the same luminosity as the data.</p>"},{"location":"Single_Lepton/Single_lepton_comment/#why-it-matters","title":"Why it Matters","text":"<ul> <li>Without normalization, MC histograms are arbitrary and cannot be compared to data.  </li> <li>After normalization, MC yields represent the expected number of events under the Standard Model.  </li> <li>This allows us to:  </li> <li>Compare data vs MC in control regions.  </li> <li>Estimate backgrounds in signal regions.  </li> <li>Search for discrepancies that may indicate new physics (e.g. Dark Matter).  </li> </ul>"},{"location":"Single_Lepton/Single_lepton_comment/#define-cross-sections","title":"Define Cross-Sections","text":"<p>For each simulated process (MC), we need the theoretical cross section (\u03c3) in pb. This will later be combined with the luminosity (L) and the number of generated events (N_gen) to normalize MC histograms.</p> <ul> <li>Data samples (SingleMuon, MET, \u2026) have no cross section.  </li> <li>MC samples (ttbar, W+jets, dibosons, \u2026) each get a \u03c3 value.  </li> </ul> <p>Below is a table of cross-sections (UL2016, approximate values).</p> <pre><code>\nimport pandas as pd\n\nintegrated_luminosity = 60.0  # * 1000.0  # convert fb^-1 to pb^-1 and nd we have no idea of the value of this\n\nxsecs = {\n    \"Wjets\": 45.21,\n    \"ttsemilep\": 831.76,\n    \"ttv\": 0.2151,\n    \"ttop\": 113.4,\n}\n\ndatasets = ['collse', 'collsm', 'ttsemilep', 'ttop', 'ttv', 'Wjets']\nchannels = ['muon', 'electron']\n\n# Diccionario para guardar los pesos\nweights = {}\n\nprint(\"\\n=== Weights per dataset/channel ===\")\nfor dataset in datasets:\n    for channel in channels:\n        try:\n            df = pd.read_csv(f\"SUMMED_{dataset}_{channel}.csv\")\n        except FileNotFoundError:\n            continue\n\n        N_gen = df['N_gen'][0]\n\n        if dataset.startswith(\"coll\"):\n            weight = 1.0\n        elif dataset in xsecs:\n            xsec = xsecs[dataset]\n            weight = integrated_luminosity * xsec / N_gen \n        else:\n            print(f\"No xsec for {dataset}, skipping...\")\n            continue\n\n        key = f\"{dataset}_{channel}\"\n        weights[key] = weight\n        print(f\"{key:20s}  weight = {weight:.4f}\")\n</code></pre> <pre><code>=== Weights per dataset/channel ===\ncollse_electron       weight = 1.0000\ncollsm_muon           weight = 1.0000\nttsemilep_muon        weight = 0.0556\nttsemilep_electron    weight = 0.1498\nttop_muon             weight = 0.0081\nttop_electron         weight = 0.0059\nttv_muon              weight = 0.0000\nttv_electron          weight = 0.0001\nWjets_muon            weight = 0.0138\nWjets_electron        weight = 0.0656\n</code></pre> <pre><code># ================== CMS STYLE CONFIG ==================\nimport matplotlib.pyplot as plt\nimport mplhep as hep\n\n# Use CMS style from mplhep\nplt.style.use(hep.style.CMS)\n\n# ---- global style tweaks ----\nplt.rcParams.update({\n    \"axes.titlesize\": 10,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 10,\n    \"ytick.labelsize\": 10,\n    \"legend.fontsize\": 10,\n    \"figure.constrained_layout.use\": True,  # avoid tight_layout warning\n})\n\n\n\n\n</code></pre> <pre><code>from hist import Hist\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport mplhep as hep\nimport numpy as np\n\n# Build histogram for muon channel\nh_mu = (\n    Hist.new.Reg(20, 0, 500, name=\"met_pt\", label=r\"$p_T^{miss}$ [GeV]\")\n        .StrCat([], name=\"dataset\", label=\"dataset\", growth=True)\n        .Weight()\n)\n\ndatasets_muon = [\"collsm\", \"ttsemilep\", \"ttop\", \"ttv\", \"Wjets\"]\ncollision_muon = [\"collsm\"]\nbackground_muon = [\"ttsemilep\", \"ttop\", \"ttv\", \"Wjets\"]\n\nfor dataset in datasets_muon:\n    try:\n        df = pd.read_csv(f\"SUMMED_{dataset}_muon.csv\")\n    except FileNotFoundError:\n        print(f\"Missing file SUMMED_{dataset}_muon.csv, skipping\")\n        continue\n\n    vals = df[\"met_pt\"].values\n    key = f\"{dataset}_muon\"\n\n\n    weight = weights[key]\n    h_mu.fill(met_pt=vals, dataset=dataset, weight=weight)\n\n# ---------- Plot ----------\nfig, (ax, rax) = plt.subplots(\n    2, 1, gridspec_kw=dict(height_ratios=[3, 1], hspace=0.05),\n    sharex=True, figsize=(7, 6)\n)\n\nhep.cms.label(\"Open Data\", ax=ax, data=True, lumi=0.169, year=2016, fontsize=12)\n\n# Stack MC\nh_mu[:, background_muon].stack(\"dataset\")[:].project(\"met_pt\").plot(\n    ax=ax, stack=True, histtype=\"fill\"\n)\n\n# Overlay Data\nh_mu[:, collision_muon].project(\"met_pt\").plot(\n    ax=ax, histtype=\"errorbar\", color=\"k\", markersize=6, label=\"Data\"\n)\n\nax.set_ylabel(\"Events\")\nax.set_title(\"Single-Muon channel\")\nax.legend()\n\n# ---------- Ratio plot ----------\nHdata = h_mu[:, collision_muon].project(\"met_pt\")\nHmc   = h_mu[:, background_muon].project(\"met_pt\")\n\ncenters = 0.5 * (Hdata.axes[0].edges[:-1] + Hdata.axes[0].edges[1:])\ndata_vals = Hdata.values()\nmc_vals   = Hmc.values()\n\nratio = np.divide(data_vals, mc_vals, out=np.zeros_like(data_vals, dtype=float), where=mc_vals &gt; 0)\nerr   = np.divide(np.sqrt(data_vals), mc_vals, out=np.zeros_like(data_vals, dtype=float), where=mc_vals &gt; 0)\n\nrax.errorbar(centers, ratio, yerr=err, fmt=\"o\", color=\"k\")\nrax.axhline(1.0, ls=\"--\", color=\"k\")\nrax.set_ylabel(\"Data/MC\")\nrax.set_xlabel(r\"$p_T^{miss}$ [GeV]\")\nrax.set_ylim(0.5, 1.5)\nax.set_xlim(0, 300)\nrax.set_xlim(0, 300)\n\nplt.show()\n</code></pre> <pre><code>/usr/local/venv/lib/python3.10/site-packages/hist/basehist.py:480: UserWarning: List indexing selection is experimental. Removed bins are not placed in overflow.\n  return super().__getitem__(self._index_transform(index))\n/usr/local/venv/lib/python3.10/site-packages/hist/basehist.py:480: UserWarning: List indexing selection is experimental. Removed bins are not placed in overflow.\n  return super().__getitem__(self._index_transform(index))\n</code></pre> <p></p> <pre><code># Build histogram for electron channel\nh_el = (\n    Hist.new.Reg(20, 0, 500, name=\"met_pt\", label=r\"$p_T^{miss}$ [GeV]\")\n        .StrCat([], name=\"dataset\", label=\"dataset\", growth=True)\n        .Weight()\n)\n\ndatasets_electron = [\"collse\", \"ttsemilep\", \"ttop\", \"ttv\", \"Wjets\"]\ncollision_electron = [\"collse\"]\nbackground_electron = [\"ttsemilep\", \"ttop\", \"ttv\", \"Wjets\"]\n\nfor dataset in datasets_electron:\n    try:\n        df = pd.read_csv(f\"SUMMED_{dataset}_electron.csv\")\n    except FileNotFoundError:\n        print(f\"Missing file SUMMED_{dataset}_electron.csv, skipping\")\n        continue\n\n    vals = df[\"met_pt\"].values\n    key = f\"{dataset}_electron\"\n\n    weight = weights[key]\n    h_el.fill(met_pt=vals, dataset=dataset, weight=weight)\n\n# ---------- Plot ----------\nfig, (ax, rax) = plt.subplots(\n    2, 1, gridspec_kw=dict(height_ratios=[3, 1], hspace=0.05),\n    sharex=True, figsize=(7, 6)\n)\n\nhep.cms.label(\n    \"Open Data\",\n    ax=ax,\n    data=True,\n    lumi=0.169,\n    year=2016,\n    fontsize=12\n)\n\n# Stack MC\nh_el[:, background_electron].stack(\"dataset\")[:].project(\"met_pt\").plot(\n    ax=ax, stack=True, histtype=\"fill\"\n)\n\n# Overlay Data\nh_el[:, collision_electron].project(\"met_pt\").plot(\n    ax=ax, histtype=\"errorbar\", color=\"k\", markersize=6, label=\"Data\"\n)\n\nax.set_ylabel(\"Events\")\nax.set_title(\"Single-Electron channel\")\nax.legend()\n\n# ---------- Ratio plot ----------\nHdata = h_el[:, collision_electron].project(\"met_pt\")\nHmc   = h_el[:, background_electron].project(\"met_pt\")\n\ncenters = 0.5 * (Hdata.axes[0].edges[:-1] + Hdata.axes[0].edges[1:])\ndata_vals = Hdata.values()\nmc_vals   = Hmc.values()\n\nratio = np.divide(data_vals, mc_vals, out=np.zeros_like(data_vals, dtype=float), where=mc_vals &gt; 0)\nerr   = np.divide(np.sqrt(data_vals), mc_vals, out=np.zeros_like(data_vals, dtype=float), where=mc_vals &gt; 0)\n\nrax.errorbar(centers, ratio, yerr=err, fmt=\"o\", color=\"k\")\nrax.axhline(1.0, ls=\"--\", color=\"k\")\nrax.set_ylabel(\"Data/MC\")\nrax.set_xlabel(r\"$p_T^{miss}$ [GeV]\")\nrax.set_ylim(0.5, 1.5)\nax.set_xlim(0, 300)\nrax.set_xlim(0, 300)\n\nplt.show()\n</code></pre> <pre><code>/usr/local/venv/lib/python3.10/site-packages/hist/basehist.py:480: UserWarning: List indexing selection is experimental. Removed bins are not placed in overflow.\n  return super().__getitem__(self._index_transform(index))\n/usr/local/venv/lib/python3.10/site-packages/hist/basehist.py:480: UserWarning: List indexing selection is experimental. Removed bins are not placed in overflow.\n  return super().__getitem__(self._index_transform(index))\n</code></pre> <p></p> <pre><code>\n</code></pre>"}]}