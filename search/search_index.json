{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"EPN CMS collaboration","text":""},{"location":"#advances-of-the-project","title":"Advances of the Project","text":""},{"location":"#reproducible-analysis","title":"Reproducible Analysis:","text":"<p>Reproducible Analysis of CMS Open Data: Search for Dark Matter in Association with Top Quarks (Based on the CMS publication: \u201cSearch for dark matter produced in association with a single top quark or a top quark pair in proton\u2013proton collisions at (\\(\\sqrt s = 13 \\TeV\\)\u201d).</p>"},{"location":"All_Hadronic/all%20hadronic%20si/","title":"All hadronic chanel","text":""},{"location":"All_Hadronic/all%20hadronic%20si/#physics-motivation-and-channel-strategy-all-hadronic-channel","title":"Physics Motivation and Channel Strategy: All-Hadronic Channel","text":"<p>The Large Hadron Collider (LHC) collides protons at center-of-mass energies high enough to probe physics beyond the Standard Model. Although the protons are composite objects, the relevant hard scatterings occur between their constituents \u2014 quarks and gluons. In the context of simplified dark matter models, these partonic interactions can produce top quarks together with a new mediator particle (commonly denoted \u03c6 for scalar or a for pseudoscalar). The mediator then decays invisibly into a pair of dark matter candidates (\u03c7\u03c7\u0304). At the detector level, this results in events with multiple top quarks plus significant missing transverse momentum (p_T^miss), the latter coming from the invisible \u03c7 particles.</p> <p>The production mechanisms of interest include:</p> <p>\u2022 Gluon fusion:   $$ gg \\to t \\bar{t}\\,\\phi \\to t \\bar{t} + \\chi \\bar{\\chi} $$</p> <p>\u2022 Single top associated production:   $$ gb \\to t \\phi \\to t + \\chi \\bar{\\chi} $$</p> <p>\u2022 t\u2013channel production:   $$ qq' \\to tb \\phi \\to tb + \\chi \\bar{\\chi} $$</p> <p>In all cases, the top quarks decay via \\(t \\to W b\\). Each W boson subsequently decays either leptonically (\\(W \\to \\ell \\nu\\)) or hadronically (\\(W \\to q \\bar{q}'\\)). Thus, the final states contain a mixture of b-tagged jets, light-flavor jets, charged leptons (electrons or muons), and genuine \\(p_T^{\\text{miss}}\\).</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#channel-strategy","title":"Channel Strategy","text":"<p>Because of the different W decay modes, analyses are divided into channels, each defined by the number of isolated charged leptons:</p> <p>\u2022 Single-lepton (SL): one isolated electron or muon, several jets (including \u22651 b-tag), and nonzero \\(p_T^{\\text{miss}}\\). This channel is statistically powerful and relatively clean, striking a balance between signal sensitivity and manageable backgrounds.</p> <p>\u2022 All-hadronic (AH): no isolated leptons, many jets including b-tagged jets, and \\(p_T^{\\text{miss}}\\). While it has the largest raw yield, it suffers from overwhelming QCD multijet background, which can fake \\(p_T^{\\text{miss}}\\).</p> <p>\u2022 Dilepton: two isolated leptons, large \\(p_T^{\\text{miss}}\\), and multiple jets. It provides a very clean signal region but is limited by low branching fraction, hence low statistics.</p> <p>In this notebook, we concentrate on the all-hadronic channel with no isolated leptons.</p> <p>There are both theoretical and practical reasons for this choice:</p> <ul> <li> <p>From the physics side: The AH channel has the highest branching ratio (~46% for \\(t\\bar{t}\\)) since both W bosons decay hadronically. This provides maximum statistical power despite the challenging QCD background.</p> </li> <li> <p>From the experimental side: Dedicated MET-based triggers and stringent angular cuts can effectively suppress QCD contamination. The presence of multiple b-jets further enhances signal-to-background discrimination.</p> </li> </ul> <p>This focus allows us to demonstrate the full workflow \u2014 from event selection to histograms \u2014 in a setting where the interplay between signal characteristics and background processes can be clearly explained. Splitting into channels is therefore not a stylistic decision but a physics necessity: each final state probes the same underlying processes under different background conditions and detector signatures.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#software-setup-and-package-imports","title":"Software Setup and Package Imports","text":"<p>Before we start analyzing data, we need to set up the software environment. This section imports the Python packages that allow us to read CMS NanoAOD files, manipulate event data, and produce plots in a reproducible and physics-oriented way.</p> <p>\u2022 Core utilities:   - <code>os</code>, <code>time</code>, <code>json</code>, <code>logging</code>, <code>asyncio</code> \u2192 for file handling, timing, and bookkeeping.   - These are standard Python libraries that help organize the workflow and log progress.</p> <p>\u2022 Numerical analysis:   - <code>numpy</code> (<code>np</code>) \u2192 fundamental for vectorized calculations on arrays.   - <code>pandas</code> (<code>pd</code>) \u2192 convenient for storing metadata (cross sections, cutflows, etc.) in table form.</p> <p>\u2022 Visualization:   - <code>matplotlib</code> (<code>mpl</code>, <code>plt</code>) \u2192 general-purpose plotting.   - <code>hist</code> \u2192 modern histogramming library designed for HEP, integrates smoothly with <code>mplhep</code>.</p> <p>\u2022 HEP-specific data access:   - <code>uproot</code> \u2192 reads CMS <code>.root</code> files into Python without needing C++/ROOT. Essential for open data workflows.   - <code>awkward</code> (<code>ak</code>) \u2192 handles \"jagged arrays\" (variable-length collections per event), e.g. different numbers of jets per event.   - <code>vector</code> \u2192 enables 4-vector operations (pT, eta, phi, invariant masses) in a NumPy/Awkward-friendly way. We register it with Awkward to use directly on event data.</p> <p>\u2022 Coffea ecosystem:   - <code>coffea.processor</code> \u2192 framework to run HEP analyses at scale.   - <code>NanoAODSchema</code> and <code>NanoEventsFactory</code> \u2192 provide a high-level interface to CMS NanoAOD, automatically building event objects like muons, jets, MET, etc.   - <code>transforms</code> and <code>methods.base/vector</code> \u2192 ensure physics-style behavior (Lorentz vectors, masks, object methods) are available on Awkward arrays.</p> <p>Finally, we configure Coffea to ignore unusual cross-references in CMS open data (they don't affect our analysis) and print the versions of the key packages. This ensures reproducibility: anyone re-running the notebook can confirm they are using the same software environment.</p> <pre><code>import asyncio, logging, os, time, json\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport hist\nimport uproot\nimport awkward as ak\nimport vector as v\nv.register_awkward()\n\n# Coffea\nfrom coffea import processor\nfrom coffea.nanoevents import NanoAODSchema, NanoEventsFactory\nfrom coffea.nanoevents import transforms\nfrom coffea.nanoevents.methods import base, vector as cvector\n\n# Ensure Coffea behaviors are available on Awkward objects\nak.behavior.update(base.behavior)\nak.behavior.update(cvector.behavior)\n\n# Be gentle with open-data odd cross-refs\nNanoAODSchema.warn_missing_crossrefs = False\n\nimport coffea\nprint(\"versions:\",\n      \"coffea\", coffea.__version__,\n      \"| awkward\", ak.__version__,\n      \"| uproot\", uproot.__version__)\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#dataset-definitions-and-luminosity-masks","title":"Dataset Definitions and Luminosity Masks","text":"<p>The CMS Open Data portal provides access to collision and simulation datasets in the NanoAOD format. Each dataset (e.g. MET, TTToHadronic, W+jets) is split across many <code>.root</code> files, sometimes hundreds per process. To manage this efficiently, we build a Python dictionary <code>nanoaod_filenames</code> that maps process names to their corresponding file index lists. Each entry points to a <code>.txt</code> file containing the remote paths (via XRootD) of the NanoAOD files.</p> <p>Examples:</p> <p>\u2022 Data (collision events): <code>MET</code>. These correspond to recorded events with MET-based trigger paths.</p> <p>\u2022 Signal MC: e.g. hypothetical \\(t\\bar{t} + DM\\) samples.</p> <p>\u2022 Background MC: top pair production (hadronic, semileptonic, dileptonic), single top (t-channel, tW), and electroweak processes like W+jets, Z\u2192\u03bd\u03bd, or dibosons (WW, ZZ).</p> <p>This separation is not just organizational:</p> <ul> <li> <p>From a physics perspective, each dataset represents a different process contributing to the observed events. Signal vs. background categories are crucial for defining the search strategy.</p> </li> <li> <p>From a programming perspective, keeping datasets in a dictionary allows us to iterate over them in loops, automate file loading, and apply the same selections consistently.</p> </li> </ul>"},{"location":"All_Hadronic/all%20hadronic%20si/#luminosity-masks","title":"Luminosity Masks","text":"<p>Real CMS data are recorded in luminosity sections (blocks of events). Not all sections are usable: some are flagged as problematic by the detector monitoring. To ensure reproducibility, CMS provides certified luminosity JSON files, which specify the \"good\" sections.</p> <p>The function <code>build_lumi_mask()</code> implements this filter:</p> <p>\u2022 Reads the certified JSON file. \u2022 Compares the <code>run</code> and <code>luminosityBlock</code> of each event to the approved ranges. \u2022 Returns a boolean mask selecting only events in certified sections.</p> <p>This step is critical in real data analysis:</p> <ul> <li>Physics motivation: prevents contamination from detector malfunctions.</li> </ul>"},{"location":"All_Hadronic/all%20hadronic%20si/#helper-functions","title":"Helper Functions","text":"<p>\u2022 <code>get_files_for_dataset(dataset, random=False, n=0)</code> \u2192 loads a subset of filenames for a given dataset. Useful when testing code with fewer files to save time.</p> <p>\u2022 <code>pretty_print(fields, ...)</code> \u2192 formats lists of branches or variables, making the NanoAOD structure easier to inspect.</p> <p>Together, these utilities allow us to handle dozens of datasets and millions of events in a manageable, modular way.</p> <pre><code>import dpoa_workshop\nfrom dpoa_workshop import nanoaod_filenames\nfrom dpoa_workshop import get_files_for_dataset\nfrom dpoa_workshop import pretty_print\nfrom dpoa_workshop import build_lumi_mask\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#building-the-ntuple-file-index","title":"Building the Ntuple File Index","text":"<p>CMS Open Data provides file index text files (<code>file_index.txt</code>) for each dataset. These contain the actual XRootD paths to the NanoAOD <code>.root</code> files, along with metadata such as the number of events per file.</p> <p>To streamline the workflow:</p> <p>\u2022 We define a function <code>download_and_parse_fileindex(url)</code> that fetches each <code>file_index.txt</code> via HTTP and extracts only the ROOT file paths.</p> <p>\u2022 We loop over all entries in <code>nanoaod_filenames</code> (the dictionary we built earlier) and collect the full list of ROOT files per dataset.</p> <p>\u2022 The result is stored in a new dictionary <code>ntuples_simple</code>, which maps dataset \u2192 list of ROOT file paths.</p> <p>\u2022 Finally, we save this as a JSON file (<code>ntuples_simple.json</code>) for later reuse.</p> <pre><code>import os, json, requests\nfrom dpoa_workshop import nanoaod_filenames\n\ndef download_and_parse_fileindex(url):\n    \"\"\"Download a file_index.txt and return list of ROOT paths.\"\"\"\n    r = requests.get(url)\n    lines = [ln.strip() for ln in r.text.splitlines() if ln.strip()]\n    # Each line is: root://... nevts=N\n    paths = [ln.split()[0] for ln in lines]\n    return paths\n\nntuples_simple = {}\n\n# Loop through nanoaod_filenames and save only paths\nfor dataset, urls in nanoaod_filenames.items():\n    all_paths = []\n    for url in urls:\n        try:\n            all_paths.extend(download_and_parse_fileindex(url))\n        except Exception as e:\n            print(f\"[warn] {dataset} {url} -&gt; {e}\")\n\n    ntuples_simple[dataset] = all_paths\n\n# Save the JSON\nwith open(\"ntuples_simple.json\", \"w\") as f:\n    json.dump(ntuples_simple, f, indent=2)\n\nprint(\"ntuples_simple.json created with datasets:\", list(ntuples_simple.keys()))\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#analysis-configuration","title":"Analysis Configuration","text":"<p>We first define which datasets to analyze. This includes data (<code>MET</code>) and several MC backgrounds (ttbar, single top, W+jets, dibosons, Z\u2192\u03bd\u03bd, etc.).</p> <p>We also set controls for testing:</p> <p>\u2022 <code>N_FILES_MAX_PER_SAMPLE</code>: how many ROOT files per dataset. \u2022 <code>MAX_EVENTS_PER_FILE</code>: how many events to read per file.</p> <p>This allows us to run quickly on subsets of data before scaling to the full analysis.</p> <pre><code># ================================\n# Initial configuration\n# ================================\nDATASETS_TO_RUN = [\n    \"MET\",\n    \"ttbar-hadronic\",\n    \"ttbar-semileptonic\",\n    \"t-channel-top\",\n    \"ttW\",\n    \"WJets-HT400to600\",\n    \"WJets-2J-FxFx\",\n    \"DYJets-Zpt200\",\n    \"Zvv\",\n    \"ZZ\",\n    \"WW\",\n]\n\n# Limit ROOTs and events\nN_FILES_MAX_PER_SAMPLE = 1  # use only 1 ROOT file per dataset\nMAX_EVENTS_PER_FILE = 50000  # None = use all events\n\n# Luminosity (fb^-1)\nLUMI_FB = 1.0\n\n# JSON with paths to the ROOTs\nNTUPLES_JSON = \"ntuples_simple.json\"\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#constructing-the-fileset","title":"Constructing the Fileset","text":"<p>We construct a fileset, which is a dictionary that maps each dataset name to:</p> <p>\u2022 A list of ROOT file paths. \u2022 Metadata such as process name, cross section, and variation.</p> <p>This abstraction allows us to loop over datasets uniformly later, regardless of whether they are data or MC.</p> <pre><code>import json\n\ndef construct_fileset_simple(\n    n_files_max_per_sample=1,\n    ntuples_json=\"ntuples_simple.json\"\n):\n    with open(ntuples_json) as f:\n        info = json.load(f)\n\n    fileset = {}\n    for key, files in info.items():\n        if key not in DATASETS_TO_RUN:\n            continue\n\n        # limit number of files\n        if n_files_max_per_sample == -1:\n            use_files = files\n        else:\n            use_files = files[:n_files_max_per_sample]\n\n        # normalize file list\n        file_list = [f[\"path\"] if isinstance(f, dict) else f for f in use_files]\n\n        fileset[key] = {\n            \"files\": file_list,\n            \"metadata\": {\n                \"process\": key,\n                \"dataset\": key,\n                \"variation\": \"nominal\",\n            }\n        }\n\n    return fileset\n\n# --- construct fileset ---\nfileset = construct_fileset_simple(\n    n_files_max_per_sample=N_FILES_MAX_PER_SAMPLE,\n    ntuples_json=NTUPLES_JSON\n)\n\nprint(f\"[OK] Fileset constructed with {len(fileset)} datasets\")\nfor k, pack in fileset.items():\n    print(f\"  {k}: {len(pack['files'])} file(s)\")\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#baseline-event-selection-all-hadronic-channel","title":"Baseline Event Selection: All-Hadronic Channel","text":"<p>We now define the baseline cuts for the all-hadronic channel with no isolated leptons. These are motivated by CMS dark matter searches and are designed to suppress dominant Standard Model backgrounds (mainly QCD multijet, \\(t\\bar{t}\\), and W/Z+jets) while enhancing sensitivity to dark matter signals with real \\(p_T^{\\text{miss}}\\).</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#1-trigger-requirements","title":"1. Trigger Requirements","text":"<p>Events are selected if they fire any of the following HLT paths:</p> <p>\u2022 <code>HLT_PFMETNoMu120</code> \u2022 <code>HLT_PFMETNoMu90_PFMHTNoMu90_IDTight</code> \u2022 <code>HLT_PFMETNoMu110_PFMHTNoMu110_IDTight</code> \u2022 <code>HLT_PFMETNoMu120_PFMHTNoMu120_IDTight</code> \u2022 <code>HLT_PFMETNoMu90_JetIdCleaned_PFMHTNoMu90_IDTight</code> \u2022 <code>HLT_PFMETNoMu120_JetIdCleaned_PFMHTNoMu120_IDTight</code> \u2022 <code>HLT_PFMET120_PFMHT120</code> \u2022 <code>HLT_PFMET110_PFMHT110_IDTight</code> \u2022 <code>HLT_PFMET120_PFMHT120_IDTight</code> \u2022 <code>HLT_PFMET170</code> \u2022 <code>HLT_PFMET170_NoiseCleaned</code> \u2022 <code>HLT_PFMET170_HBHECleaned</code> \u2022 <code>HLT_PFMET170_HBHE_BeamHaloCleaned</code></p> <p>Motivation: These MET-based triggers are efficient for hadronic final states with genuine missing energy.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#2-event-cleaning-flags","title":"2. Event Cleaning Flags","text":"<p>The following filters must be applied to both data and simulation:</p> <p>\u2022 <code>HBHENoiseFilter</code> \u2022 <code>HBHENoiseIsoFilter</code> \u2022 <code>ECALDeadCellFilter</code> \u2022 <code>GlobalTightHalo2016Filter</code> \u2022 <code>BadPFMuonFilter</code> \u2022 <code>BadChargedHadronFilter</code></p> <p>The following filters are applied to data only:</p> <p>\u2022 <code>EEBadScFilter</code> \u2022 <code>BadMuons</code> \u2022 <code>DuplicateMuons</code></p> <p>Motivation: Remove events with detector noise and instrumental fake MET.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#3-lepton-veto","title":"3. Lepton Veto","text":"<p>\u2022 No \"Veto\" Leptons: Events must contain no \"Veto\" leptons.   - Electrons: No electrons with \\(p_T &gt; 10\\) GeV, \\(|\\eta| &lt; 2.5\\), and loose ID.   - Muons: No muons with \\(p_T &gt; 10\\) GeV, \\(|\\eta| &lt; 2.4\\), and loose ID.</p> <p>Motivation: Suppress semileptonic \\(t\\bar{t}\\), W+jets, and dilepton backgrounds.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#4-jet-selection","title":"4. Jet Selection","text":"<p>\u2022 Jet \\(p_T\\): &gt; 30 GeV \u2022 Jet Eta:   - Central jets: \\(|\\eta| &lt; 2.4\\)   - Forward jets: \\(2.4 &lt; |\\eta| &lt; 5\\) (optional, depending on analysis) \u2022 Jet ID: Loose jet ID requirements \u2022 Overlap Removal: Jet objects are not considered if they are within \\(\\Delta R &lt; 0.4\\) of a \"Tight\" electron or muon.</p> <p>Motivation: Ensure well-reconstructed jets while avoiding double-counting with leptons.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#5-minimum-number-of-jets","title":"5. Minimum Number of Jets","text":"<p>\u2022 Baseline: \u2265 3 jets</p> <p>Motivation: Top decays produce at least 6 quarks in the \\(t\\bar{t}\\) fully hadronic channel (2 b-quarks + 4 light quarks from W decays). Requiring \u22653 jets ensures we capture the event topology.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#6-minimum-number-of-b-tagged-jets","title":"6. Minimum Number of b-tagged Jets","text":"<p>\u2022 Baseline: \u2265 1 b-tagged jet (CSVM working point) with \\(p_T &gt; 30\\) GeV \u2022 Categorization:   - \\(n_b = 1\\) (for single top + DM events)   - \\(n_b \u2265 2\\) (for \\(t\\bar{t}\\) + DM events)</p> <p>Motivation: Top-quark decays always produce b-jets, so this suppresses W+light-flavor jets and QCD.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#7-met-requirements","title":"7. MET Requirements","text":"<p>\u2022 Baseline: \\(p_T^{\\text{miss}} \u2265 250\\) GeV</p> <p>Motivation: Dark matter escapes undetected \u2192 large genuine MET. This high threshold strongly suppresses QCD multijet background.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#8-angular-separation-cuts","title":"8. Angular Separation: \u0394\u03c6 Cuts","text":"<p>\u2022 Baseline: \\(\\min\\Delta\\phi(j_{1,2}, p_T^{\\text{miss}}) &gt; 0.4\\) \u2022 Optimized Selection: \\(\\min\\Delta\\phi(j_{1,2}, p_T^{\\text{miss}}) &gt; 1.0\\)</p> <p>Motivation: Reduces QCD multijet events with mismeasured MET aligned with jets. True MET from dark matter is typically more isotropic.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#9-additional-kinematic-variables","title":"9. Additional Kinematic Variables","text":"<p>\u2022 b\u2013MET Transverse Mass (\\(M_{bT}\\)):   - \\(M_{bT} &gt; 180\\) GeV   - Motivation: Further discriminates signal from \\(t\\bar{t}\\) backgrounds by exploiting the kinematics of b-quarks and MET.</p> <p>\u2022 Jet 1 \\(p_T\\) / \\(H_T\\):   - \u2264 0.5 (specifically for \\(n_b \u2265 2\\) category)   - Motivation: Reduces events where a single jet dominates the hadronic activity, which is characteristic of QCD.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#summary","title":"Summary","text":"<p>The baseline selection region is defined by:</p> <p>\u2022 MET-based triggers (no leptons required) \u2022 Event cleaning filters \u2022 No veto leptons (electrons or muons with \\(p_T &gt; 10\\) GeV) \u2022 \u2265 3 jets with \u2265 1 b-tag \u2022 Large MET (\\(p_T^{\\text{miss}} \u2265 250\\) GeV) \u2022 Angular separation to suppress QCD (\\(\\min\\Delta\\phi(j_{1,2}, p_T^{\\text{miss}}) &gt; 0.4\\) or \\(&gt; 1.0\\)) \u2022 Additional kinematic cuts (\\(M_{bT} &gt; 180\\) GeV, jet \\(p_T\\) / \\(H_T\\) \u2264 0.5)</p> <p>Together, these cuts target signal-like topologies while removing the bulk of Standard Model backgrounds, especially the overwhelming QCD multijet contamination.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#analysis-implementation","title":"Analysis Implementation","text":"<p>This class (<code>DMAnalysisAllHadronic</code>) encodes the physics selection of the analysis for the all-hadronic channel:</p> <p>\u2022 Object-level cuts:   - Veto loose electrons: \\(p_T &gt; 10\\) GeV, \\(|\\eta| &lt; 2.5\\), loose ID.   - Veto loose muons: \\(p_T &gt; 10\\) GeV, \\(|\\eta| &lt; 2.4\\), loose ID.   - Jets: \\(p_T &gt; 30\\) GeV, \\(|\\eta| &lt; 2.4\\), good jetID.   - b-jets: tagged with DeepCSV/DeepFlav WP.</p> <p>\u2022 Event-level cuts:   - Pass MET-based triggers (HLT).   - No veto leptons (0 electrons, 0 muons).   - \u22653 jets, \u22651 b-jet.   - \\(p_T^{\\text{miss}} \u2265 250\\) GeV.   - \\(\\min\\Delta\\phi(j_{1,2}, p_T^{\\text{miss}}) &gt; 0.4\\).</p> <p>\u2022 Outputs:   - Histograms for physics variables.   - Cutflow (number of events passing each step).</p> <pre><code>import time, pickle\nfrom pathlib import Path\nimport numpy as np\nimport awkward as ak\nimport hist\nfrom coffea.nanoevents import NanoAODSchema, NanoEventsFactory\n\n# === Function to read events from a ROOT file with optional limit ===\ndef events_from_file(path, metadata=None, schemaclass=NanoAODSchema, max_events=None):\n    \"\"\"\n    Load events from a ROOT file.\n    If max_events is not None, only read up to that number of events (useful for quick tests).\n    \"\"\"\n    factory = NanoEventsFactory.from_root(\n        {path: \"Events\"},\n        schemaclass=schemaclass,\n        metadata=(metadata or {}),\n        entry_stop=max_events,\n    )\n    return factory.events()\n\n# === Main class for Dark Matter analysis (All-Hadronic channel) ===\nclass DMAnalysisAllHadronic:\n    def __init__(self, DATASET, lumi_fb):\n        self.DATASET = DATASET\n        self.lumi_fb = float(lumi_fb)\n\n        # Histogram axes\n        self.process_cat = hist.axis.StrCategory([], name=\"process\", label=\"Process\", growth=True)\n        self.variation_cat = hist.axis.StrCategory([], name=\"variation\", label=\"Variation\", growth=True)\n\n        # Physics variables\n        num_axis = hist.axis.Regular(15, 0, 15, name=\"var\")      # njets, nbjets\n        met_axis = hist.axis.Regular(30, 0, 600, name=\"var\")     # MET\n        dphi_axis = hist.axis.Regular(32, 0, 3.2, name=\"var\")    # min deltaphi\n        ht_axis = hist.axis.Regular(40, 0, 2000, name=\"var\")     # HT\n        mbt_axis = hist.axis.Regular(30, 0, 600, name=\"var\")     # M_bT\n\n        # Histograms\n        self.h = {\n            'njets':    hist.Hist(num_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'nbjets':   hist.Hist(num_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'met':      hist.Hist(met_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'min_dphi': hist.Hist(dphi_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'ht':       hist.Hist(ht_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n            'mbt':      hist.Hist(mbt_axis, self.process_cat, self.variation_cat, storage=hist.storage.Weight()),\n        }\n\n        # Cutflow\n        self.cut_flow = {\n            \"All\": 0,\n            \"HLT_MET\": 0,\n            \"Filters\": 0,\n            \"LeptonVeto\": 0,\n            \"&gt;=3jets\": 0,\n            \"&gt;=1btag\": 0,\n            \"MET&gt;=250\": 0,\n            \"minDPhi&gt;0.4\": 0,\n        }\n\n    # ---------- helpers ----------\n    def _pass_met_hlt(self, events):\n        \"\"\"Apply MET-based HLT paths.\"\"\"\n        hlt = getattr(events, \"HLT\", None)\n        if hlt is None:\n            return ak.ones_like(events.event, dtype=bool)\n\n        hlt_paths = [\n            \"PFMETNoMu120\",\n            \"PFMETNoMu90_PFMHTNoMu90_IDTight\",\n            \"PFMETNoMu110_PFMHTNoMu110_IDTight\",\n            \"PFMETNoMu120_PFMHTNoMu120_IDTight\",\n            \"PFMET120_PFMHT120\",\n            \"PFMET110_PFMHT110_IDTight\",\n            \"PFMET120_PFMHT120_IDTight\",\n            \"PFMET170\",\n        ]\n\n        masks = []\n        for name in hlt_paths:\n            if hasattr(hlt, name):\n                masks.append(ak.values_astype(getattr(hlt, name), bool))\n\n        if not masks:\n            return ak.ones_like(events.event, dtype=bool)\n\n        # Logical OR of all triggers\n        combined = masks[0]\n        for m in masks[1:]:\n            combined = combined | m\n        return combined\n\n    def _pass_event_filters(self, events):\n        \"\"\"Apply event cleaning flags.\"\"\"\n        flag = getattr(events, \"Flag\", None)\n        if flag is None:\n            return ak.ones_like(events.event, dtype=bool)\n\n        required_flags = [\n            \"HBHENoiseFilter\",\n            \"HBHENoiseIsoFilter\",\n            \"EcalDeadCellTriggerPrimitiveFilter\",\n            \"globalSuperTightHalo2016Filter\",\n            \"BadPFMuonFilter\",\n        ]\n\n        mask = ak.ones_like(events.event, dtype=bool)\n        for fname in required_flags:\n            if hasattr(flag, fname):\n                mask = mask &amp; ak.values_astype(getattr(flag, fname), bool)\n\n        return mask\n\n    def _select_veto_electrons(self, events):\n        \"\"\"Veto loose electrons: pt &gt; 10, |eta| &lt; 2.5.\"\"\"\n        el = events.Electron\n        if hasattr(el, \"cutBased\"):\n            return el[(el.pt &gt; 10) &amp; (abs(el.eta) &lt; 2.5) &amp; (el.cutBased &gt;= 1)]\n        return el[(el.pt &gt; 10) &amp; (abs(el.eta) &lt; 2.5)]\n\n    def _select_veto_muons(self, events):\n        \"\"\"Veto loose muons: pt &gt; 10, |eta| &lt; 2.4.\"\"\"\n        mu = events.Muon\n        if hasattr(mu, \"looseId\"):\n            return mu[(mu.pt &gt; 10) &amp; (abs(mu.eta) &lt; 2.4) &amp; (mu.looseId == True)]\n        return mu[(mu.pt &gt; 10) &amp; (abs(mu.eta) &lt; 2.4)]\n\n    def _select_good_jets(self, events):\n        \"\"\"Jets: pt &gt; 30, |eta| &lt; 2.4, jetId &gt;= 2.\"\"\"\n        jets = events.Jet\n        jetid = getattr(jets, \"jetId\", ak.zeros_like(jets.pt, dtype=np.int32) + 2)\n        return jets[(jets.pt &gt; 30) &amp; (abs(jets.eta) &lt; 2.4) &amp; (jetid &gt;= 2)]\n\n    def _count_bjets(self, jets):\n        \"\"\"Count b-tagged jets (DeepFlav or DeepCSV medium WP).\"\"\"\n        if hasattr(jets, \"btagDeepFlavB\"):\n            mask = jets.btagDeepFlavB &gt; 0.3093  # medium WP\n        elif hasattr(jets, \"btagDeepB\"):\n            mask = jets.btagDeepB &gt; 0.6321      # medium WP\n        else:\n            mask = ak.zeros_like(jets.pt, dtype=bool)\n        return ak.sum(mask, axis=1), mask\n\n    def _compute_min_dphi(self, jets, met):\n        \"\"\"Compute min delta phi between leading 2 jets and MET.\"\"\"\n        if ak.any(ak.num(jets) &gt;= 2):\n            jet1 = jets[:, 0]\n            jet2 = jets[:, 1]\n            dphi1 = abs(jet1.phi - met.phi)\n            dphi2 = abs(jet2.phi - met.phi)\n            dphi1 = ak.where(dphi1 &gt; np.pi, 2*np.pi - dphi1, dphi1)\n            dphi2 = ak.where(dphi2 &gt; np.pi, 2*np.pi - dphi2, dphi2)\n            min_dphi = ak.where(dphi1 &lt; dphi2, dphi1, dphi2)\n        else:\n            min_dphi = ak.zeros_like(met.pt) + 999.0\n        return min_dphi\n\n    def _compute_ht(self, jets):\n        \"\"\"Compute HT = scalar sum of jet pT.\"\"\"\n        return ak.sum(jets.pt, axis=1)\n\n    def _compute_mbt(self, bjets, met):\n        \"\"\"Compute transverse mass between leading b-jet and MET.\"\"\"\n        if ak.any(ak.num(bjets) &gt;= 1):\n            bjet_lead = bjets[:, 0]\n            mbt = np.sqrt(2.0 * bjet_lead.pt * met.pt * \n                         (1.0 - np.cos(bjet_lead.phi - met.phi)))\n        else:\n            mbt = ak.zeros_like(met.pt)\n        return mbt\n\n    def _weights(self, events):\n        \"\"\"Weights disabled: return 1 for each event.\"\"\"\n        return np.ones(len(events), dtype=\"float64\")\n\n    def _labels_array(self, label, n):\n        return np.array([label] * int(n), dtype=object)\n\n    def _fill_event_hist(self, H, var_np, process, variation, w_np):\n        H.fill(\n            var=var_np,\n            process=self._labels_array(process, len(var_np)),\n            variation=self._labels_array(variation, len(var_np)),\n            weight=w_np\n        )\n\n    # ---------- main pipeline ----------\n    def process(self, events):\n        process = events.metadata.get(\"process\", \"unknown\")\n        variation = events.metadata.get(\"variation\", \"nominal\")\n\n        w_evt = self._weights(events)\n        self.cut_flow[\"All\"] += len(events)\n\n        # HLT\n        hltmask = self._pass_met_hlt(events)\n        events = events[hltmask]\n        w_evt = w_evt[ak.to_numpy(hltmask)]\n```python\n        self.cut_flow[\"HLT_MET\"] += len(events)\n\n        # Event filters\n        filtermask = self._pass_event_filters(events)\n        events = events[filtermask]\n        w_evt = w_evt[ak.to_numpy(filtermask)]\n        self.cut_flow[\"Filters\"] += len(events)\n\n        # Lepton veto (no veto electrons, no veto muons)\n        el_veto = self._select_veto_electrons(events)\n        mu_veto = self._select_veto_muons(events)\n        mask_lep = (ak.num(el_veto) == 0) &amp; (ak.num(mu_veto) == 0)\n        events = events[mask_lep]\n        w_evt = w_evt[ak.to_numpy(mask_lep)]\n        self.cut_flow[\"LeptonVeto\"] += len(events)\n\n        # Jets\n        jets = self._select_good_jets(events)\n        nj = ak.num(jets)\n\n        # &gt;= 3 jets\n        jets_ok = (nj &gt;= 3)\n        events = events[jets_ok]\n        jets = jets[jets_ok]\n        nj = nj[jets_ok]\n        w_evt = w_evt[ak.to_numpy(jets_ok)]\n        self.cut_flow[\"&gt;=3jets\"] += len(events)\n\n        if len(events) == 0:\n            return {\"nevents\": {process: 0}, \"hists\": self.h}\n\n        # b-jets\n        nb, btag_mask = self._count_bjets(jets)\n        bjets = jets[btag_mask]\n\n        # &gt;= 1 b-tag\n        btag_ok = (nb &gt;= 1)\n        events = events[btag_ok]\n        jets = jets[btag_ok]\n        bjets = bjets[btag_ok]\n        nj = nj[btag_ok]\n        nb = nb[btag_ok]\n        w_evt = w_evt[ak.to_numpy(btag_ok)]\n        self.cut_flow[\"&gt;=1btag\"] += len(events)\n\n        if len(events) == 0:\n            return {\"nevents\": {process: 0}, \"hists\": self.h}\n\n        # MET\n        met = getattr(events, \"MET\", None)\n        met_pt = met.pt if (met is not None and hasattr(met, \"pt\")) else ak.zeros_like(events.event, dtype=float)\n\n        # MET &gt;= 250 GeV\n        met_ok = (met_pt &gt;= 250)\n        events = events[met_ok]\n        jets = jets[met_ok]\n        bjets = bjets[met_ok]\n        nj = nj[met_ok]\n        nb = nb[met_ok]\n        met_pt = met_pt[met_ok]\n        met = met[met_ok]\n        w_evt = w_evt[ak.to_numpy(met_ok)]\n        self.cut_flow[\"MET&gt;=250\"] += len(events)\n\n        if len(events) == 0:\n            return {\"nevents\": {process: 0}, \"hists\": self.h}\n\n        # min delta phi\n        min_dphi = self._compute_min_dphi(jets, met)\n\n        # min delta phi &gt; 0.4\n        dphi_ok = (min_dphi &gt; 0.4)\n        events = events[dphi_ok]\n        jets = jets[dphi_ok]\n        bjets = bjets[dphi_ok]\n        nj = nj[dphi_ok]\n        nb = nb[dphi_ok]\n        met_pt = met_pt[dphi_ok]\n        met = met[dphi_ok]\n        min_dphi = min_dphi[dphi_ok]\n        w_evt = w_evt[ak.to_numpy(dphi_ok)]\n        self.cut_flow[\"minDPhi&gt;0.4\"] += len(events)\n\n        if len(events) == 0:\n            return {\"nevents\": {process: 0}, \"hists\": self.h}\n\n        # Additional kinematic variables\n        ht = self._compute_ht(jets)\n        mbt = self._compute_mbt(bjets, met)\n\n        # Fill histograms\n        w_evt_np = ak.to_numpy(w_evt)\n        self._fill_event_hist(self.h['njets'], ak.to_numpy(nj), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['nbjets'], ak.to_numpy(nb), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['met'], ak.to_numpy(met_pt), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['min_dphi'], ak.to_numpy(min_dphi), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['ht'], ak.to_numpy(ht), process, variation, w_evt_np)\n        self._fill_event_hist(self.h['mbt'], ak.to_numpy(mbt), process, variation, w_evt_np)\n\n        # Return both histos and filtered events\n        return {\n            \"nevents\": {process: int(len(w_evt_np))},\n            \"hists\": self.h,\n            \"selected_events\": events,\n            \"jets\": jets,\n            \"bjets\": bjets,\n            \"njets\": nj,\n            \"nbjets\": nb,\n            \"met\": met_pt,\n            \"min_dphi\": min_dphi,\n            \"ht\": ht,\n            \"mbt\": mbt,\n        }\n\n    def postprocess(self, accumulator):\n        return accumulator\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#running-the-analysis","title":"Running the Analysis","text":"<p>Now we run the main loop:</p> <ol> <li>Iterate over each dataset in the fileset.</li> <li>Read events from ROOT files with <code>events_from_file</code>.</li> <li>Apply the physics cuts with <code>analysis.process</code>.</li> <li>Save results:    \u2022 Per-dataset CSVs with selected event variables.    \u2022 Histograms (pickled) for plotting later.    \u2022 Cutflow table to verify event selection efficiency.    \u2022 Event counts for normalization.</li> </ol> <p>This is the driver stage where the analysis is executed.</p> <pre><code>from pathlib import Path\nimport pandas as pd\nimport awkward as ak\nimport pickle, time\nimport numpy as np\n\nOUT_DIR = Path(\"./outputs_ah\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nOUT_HISTOS = OUT_DIR / \"dm_histograms_ah.pkl\"\nOUT_CUTFLOW = OUT_DIR / \"dm_cutflow_ah.csv\"\nOUT_NEVENTS = OUT_DIR / \"dm_nevents_ah.csv\"\n\nanalysis = DMAnalysisAllHadronic(DATASET=\"MET\", lumi_fb=LUMI_FB)\n\nentries_total = 0\nnevents_rows = []\n\nt0 = time.monotonic()\n\nfor key, pack in fileset.items():\n    flist = pack[\"files\"]\n    meta = pack[\"metadata\"]\n\n    if not flist:\n        continue\n\n    events_all = []\n\n    for path in flist:\n        md = dict(meta)\n\n        try:\n            ev = events_from_file(path, metadata=md, max_events=MAX_EVENTS_PER_FILE)\n        except Exception as e:\n            print(f\"[warn] skip {path} -&gt; {e}\")\n            continue\n\n        entries_total += len(ev)\n\n        # --- run analysis ---\n        out = analysis.process(ev)\n        nsel = list(out[\"nevents\"].values())[0]\n\n        if nsel == 0:\n            continue\n\n        # --- use filtered events for CSV ---\n        ev_sel = out[\"selected_events\"]\n        jets = out[\"jets\"]\n        bjets = out[\"bjets\"]\n        nj = out[\"njets\"]\n        nb = out[\"nbjets\"]\n        met_pt = out[\"met\"]\n        min_dphi = out[\"min_dphi\"]\n        ht = out[\"ht\"]\n        mbt = out[\"mbt\"]\n\n        df_ev = pd.DataFrame({\n            \"process\": [md[\"process\"]]*len(ev_sel),\n            \"dataset\": [md[\"dataset\"]]*len(ev_sel),\n            \"njets\": ak.to_numpy(nj),\n            \"nbjets\": ak.to_numpy(nb),\n            \"met\": ak.to_numpy(met_pt),\n            \"min_dphi\": ak.to_numpy(min_dphi),\n            \"ht\": ak.to_numpy(ht),\n            \"mbt\": ak.to_numpy(mbt),\n        })\n\n        events_all.append(df_ev)\n\n        nevents_rows.append({\n            \"key\": key,\n            \"process\": md[\"process\"],\n            \"dataset\": md[\"dataset\"],\n            \"file\": path,\n            \"selected_events\": int(len(df_ev)),\n            \"entries_in_file\": int(len(ev)),\n            \"xsec\": md.get(\"xsec\", None),\n        })\n\n    # Export CSV for this dataset\n    if events_all:\n        df_all = pd.concat(events_all, ignore_index=True)\n        outfile = OUT_DIR / f\"{key}_processed.csv\"\n        df_all.to_csv(outfile, index=False)\n        print(f\"[OK] Wrote {len(df_all)} events -&gt; {outfile}\")\n\n# Save global results\nelapsed = time.monotonic() - t0\n\nwith open(OUT_HISTOS, \"wb\") as f:\n    pickle.dump(analysis.h, f, protocol=pickle.HIGHEST_PROTOCOL)\n\npd.DataFrame(list(analysis.cut_flow.items()), columns=[\"cut\",\"events\"]).to_csv(OUT_CUTFLOW, index=False)\npd.DataFrame(nevents_rows).to_csv(OUT_NEVENTS, index=False)\n\nprint(f\"\\n[OK] Total entries processed: {entries_total}\")\nprint(f\"[OK] Global cutflow:\\n{analysis.cut_flow}\")\nprint(f\"[timing] Elapsed: {elapsed:.2f} s\")\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#visualization-raw-event-counts","title":"Visualization: Raw Event Counts","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport pandas as pd\nimport mplhep as hep\n\n# Apply CMS style\nplt.style.use(hep.style.CMS)\n\n# Load histograms and nevents\ndf_nevents = pd.read_csv(OUT_NEVENTS)\n\nwith open(OUT_HISTOS, \"rb\") as f:\n    hdict = pickle.load(f)\n\ndef plot_all_variables_raw(hdict, lumi_fb):\n    \"\"\"\n    Plot all variables without any normalization:\n    - MET as data (points with error bars).\n    - All other datasets as MC (stacked histograms).\n    \"\"\"\n    variables = list(hdict.keys())\n    print(\"Available variables:\", variables)\n\n    for var in variables:\n        h = hdict[var]\n        processes = list(h.axes[\"process\"])\n        edges = h.axes[\"var\"].edges\n        centers = 0.5*(edges[:-1] + edges[1:])\n        width = np.diff(edges)\n\n        # Explicitly define data vs MC\n        data_procs = [\"MET\"]\n        mc_procs = [p for p in processes if p not in data_procs]\n\n        plt.figure(figsize=(7,5))\n        bottom = np.zeros(len(edges)-1)\n\n        # --- MC stacked ---\n        for proc in mc_procs:\n            vals = h[{\"process\": proc}].values().ravel()\n            if np.any(vals):\n                plt.bar(edges[:-1], vals, width=width, bottom=bottom,\n                       align=\"edge\", alpha=0.7, label=proc)\n                bottom += vals\n\n        # --- Data as points ---\n        for proc in data_procs:\n            if proc in processes:\n                vals = h[{\"process\": proc}].values().ravel()\n                if np.any(vals):\n                    yerr = np.sqrt(vals)\n                    plt.errorbar(centers, vals, yerr=yerr,\n                               fmt=\"o\", color=\"black\", label=proc)\n\n        plt.xlabel(var)\n        plt.ylabel(\"Raw events\")\n        plt.legend(fontsize=8, frameon=False)\n        plt.grid(True, linestyle=\"--\", alpha=0.4)\n        plt.tight_layout()\n        plt.show()\n\n# === Run ===\nplot_all_variables_raw(hdict, LUMI_FB)\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#cross-sections-and-normalization","title":"Cross-Sections and Normalization","text":""},{"location":"All_Hadronic/all%20hadronic%20si/#why-normalization-is-necessary","title":"Why Normalization is Necessary","text":"<p>When comparing data and Monte Carlo (MC) simulations, the raw event counts are not directly comparable:</p> <p>\u2022 Data:   - Events are collected with a detector during a given time period.   - The \"size\" of the dataset is controlled by the integrated luminosity (L).   - Example: UL2016 MET dataset corresponds to ~35.9 fb\u207b\u00b9.   - There is no cross section attached \u2014 it is just what was recorded.</p> <p>\u2022 MC (simulations):   - Each simulated dataset corresponds to a particular physics process (e.g. \\(t\\bar{t}\\), W+jets).   - Generators simulate a finite number of events (N_gen) with a known theoretical cross section (\u03c3).   - By construction, MC samples may represent more or fewer events than would be seen in real data.   - Therefore, they must be scaled.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#the-normalization-formula","title":"The Normalization Formula","text":"<p>To make MC comparable to data, we apply a per-event weight:</p> \\[ w = \\frac{\\sigma \\cdot L}{N_\\text{gen}} \\] <p>Where:</p> <p>\u2022 \u03c3 (pb) = process cross section (from theory/measurements). \u2022 L (fb\u207b\u00b9) = integrated luminosity of the data sample.   - Convert to pb\u207b\u00b9 by multiplying by 1000. \u2022 N_gen = total number of generated MC events (before cuts).</p> <p>This weight ensures that when we sum the MC events after applying cuts, the histograms reflect the expected yield in the same luminosity as the data.</p>"},{"location":"All_Hadronic/all%20hadronic%20si/#step-1-define-cross-sections","title":"Step 1 \u2014 Define Cross-Sections","text":"<p>For each simulated process (MC), we need the theoretical cross section (\u03c3) in pb. This will later be combined with the luminosity (L) and the number of generated events (N_gen) to normalize MC histograms.</p> <pre><code># ================================\n# Cross-sections (pb) \u2014 official values\n# ================================\nXSEC_PB = {\n    # Data-like (no cross section)\n    (\"MET\", None): None,\n    (\"SingleMuon\", None): None,\n    (\"SingleElectron\", None): None,\n\n    # ttbar\n    (\"ttbar-semileptonic\", None): 831.76,\n    (\"ttbar-hadronic\", None): 831.76,\n\n    # single top\n    (\"t-channel-top\", None): 136.0,\n    (\"t-channel-antitop\", None): 81.0,\n    (\"tW-top\", None): 71.7,\n\n    # ttV\n    (\"ttW\", None): 0.204,\n    (\"ttZ\", None): 0.252,\n\n    # W+jets\n    (\"WJets-HT400to600\", None): 48.9,\n    (\"WJets-2J-FxFx\", None): 615.7,\n\n    # DY+jets\n    (\"DYJets-inclusive\", None): 6025.0,\n    (\"DYJets-Zpt200\", None): 1.27,\n\n    # Diboson + Z\u2192\u03bd\u03bd\n    (\"Zvv\", None): 77.3,\n    (\"WW\", None): 118.7,\n    (\"ZZ\", None): 16.6,\n}\n\ndef get_xsec(proc: str, subgroup: str | None = None):\n    \"\"\"\n    Return cross-section (pb) for (proc, subgroup).\n    Data-like samples (MET, SingleMuon, SingleElectron) return None.\n    \"\"\"\n    key = (proc, subgroup)\n    if key in XSEC_PB:\n        return XSEC_PB[key]\n    return XSEC_PB.get((proc, None), None)\n\nprint(\"=== Cross-sections (pb) ===\")\nfor (proc, subgroup), xsec in XSEC_PB.items():\n    name = proc if subgroup is None else f\"{proc} ({subgroup})\"\n    print(f\"{name:20s} : {xsec}\")\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#step-2-count-generated-events","title":"Step 2 \u2014 Count Generated Events","text":"<p>We need to know how many events were generated (N_gen) for each dataset. This is critical to compute normalization factors.</p> <pre><code>import uproot\n\ndef count_events_one_file(root_path):\n    try:\n        with uproot.open(root_path) as f:\n            return f[\"Events\"].num_entries\n    except Exception as e:\n        print(f\"[warn] could not open {root_path} -&gt; {e}\")\n        return 0\n\n# Dictionary to store counts\nevents_count = {}\n\nfor key, pack in fileset.items():\n    fpaths = pack[\"files\"][:N_FILES_MAX_PER_SAMPLE]\n    if not fpaths:\n        continue\n\n    total_events = 0\n    file_events = []\n\n    for path in fpaths:\n        nevts = count_events_one_file(path)\n        total_events += nevts\n        file_events.append((path, nevts))\n        print(f\"{key}: {path}, events={nevts}\")\n\n    events_count[key] = {\n        \"files\": fpaths,\n        \"file_events\": file_events,\n        \"total_events\": total_events,\n    }\n\n    print(f\"--&gt; {key}: total_events={total_events}\\n\")\n\n# --- Final summary ---\nprint(\"\\nSummary (per dataset):\")\ngrand_total = 0\nfor k, v in events_count.items():\n    print(f\"  {k:25s} \u2192 total_events={v['total_events']} (from {len(v['files'])} file(s))\")\n    grand_total += v[\"total_events\"]\n\nprint(f\"\\n[OK] Grand total across datasets = {grand_total}\")\n</code></pre>"},{"location":"All_Hadronic/all%20hadronic%20si/#normalized-plots","title":"Normalized Plots","text":"<pre><code># ================================\n# Normalization + CMS-style plots\n# ================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\n# Set your analysis luminosity (fb^-1)\nLUMI_FB = 0.01\n\n# Load histograms\nwith open(OUT_HISTOS, \"rb\") as f:\n    hdict = pickle.load(f)\n\n# Explicit N_gen dictionary (truth-level generated events)\nN_EVENTS_GEN = {\n    \"MET\": 1654969,\n    \"SingleMuon\": 14113,\n    \"SingleElectron\": 2338304,\n    \"ttbar-semileptonic\": 1233000,\n    \"ttbar-hadronic\": 1344000,\n    \"t-channel-top\": 168000,\n    \"ttW\": 9451,\n    \"WJets-HT400to600\": 41364,\n    \"WJets-2J-FxFx\": 1766801,\n    \"DYJets-Zpt200\": 9748,\n    \"WW\": 2016000,\n    \"ZZ\": 4000,\n    \"Zvv\": 932,\n}\n\ndef get_neq_gen(proc: str) -&gt; int|None:\n    \"\"\"N_gen = from fixed dictionary.\"\"\"\n    return N_EVENTS_GEN.get(proc, None)\n\ndef norm_factor(proc: str, lumi_fb: float) -&gt; float:\n    if proc in (\"SingleMuon\", \"SingleElectron\", \"MET\"):\n        return 1.0\n\n    xsec = get_xsec(proc)\n    ngen = get_neq_gen(proc)\n\n    if (xsec is None) or (ngen is None) or (ngen &lt;= 0):\n        return 1.0\n\n    return (xsec * lumi_fb * 1e3) / float(ngen)\n\ndef build_norm_report(hdict, lumi_fb):\n    some_var = next(iter(hdict.keys()))\n    processes = list(hdict[some_var].axes[\"process\"])\n\n    rows = []\n    for proc in processes:\n        xsec = get_xsec(proc)\n        ngen = get_neq_gen(proc)\n        factor = norm_factor(proc, lumi_fb)\n\n        rows.append({\n            \"process\": proc,\n            \"xsec_pb\": xsec,\n            \"lumi_fb\": lumi_fb if xsec is not None else None,\n            \"N_gen_used\": ngen,\n            \"scale_factor\": factor,\n            \"is_data\": proc in (\"SingleMuon\", \"SingleElectron\", \"MET\"),\n        })\n\n    return pd.DataFrame(rows)\n\ndef plot_all_variables_normalized(hdict, lumi_fb):\n    rep = build_norm_report(hdict, lumi_fb)\n    print(\"\\n=== Normalization report (All-Hadronic Channel) ===\")\n    print(rep.to_string(index=False))\n\n    variables = list(hdict.keys())\n\n    for var in variables:\n        h = hdict[var]\n        processes = list(h.axes[\"process\"])\n        edges = h.axes[\"var\"].edges\n        centers = 0.5*(edges[:-1] + edges[1:])\n        width = np.diff(edges)\n\n        data_procs = [p for p in processes if p in (\"MET\", \"SingleMuon\", \"SingleElectron\")]\n        mc_procs = [p for p in processes if p not in data_procs]\n\n        plt.figure(figsize=(7,5))\n        bottom = np.zeros(len(edges)-1)\n\n        for proc in mc_procs:\n            vals = h[{\"process\": proc}].values().ravel()\n            vals *= norm_factor(proc, lumi_fb)\n            if np.any(vals):\n                plt.bar(edges[:-1], vals, width=width, bottom=bottom,\n                       align=\"edge\", alpha=0.7, label=proc)\n                bottom += vals\n\n        for proc in data_procs:\n            vals = h[{\"process\": proc}].values().ravel()\n            if np.any(vals):\n                yerr = np.sqrt(vals)\n                plt.errorbar(centers, vals, yerr=yerr, fmt=\"o\", color=\"black\", label=proc)\n\n        plt.xlabel(var)\n        plt.ylabel(\"Events (MC scaled, Data raw)\")\n        plt.legend(fontsize=8, frameon=False)\n        plt.grid(True, linestyle=\"--\", alpha=0.4)\n        plt.tight_layout()\n        plt.show()\n\n# === Run ===\nplot_all_variables_normalized(hdict, LUMI_FB)\n</code></pre> <p>This completes the all-hadronic channel analysis workflow with proper event selection, MET-based triggers, lepton veto, angular cuts to suppress QCD, and normalized MC-to-data comparisons.  </p>"},{"location":"Single_Lepton/Analisis_muon_final_version/","title":"Advances of the Project","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#title-objective","title":"Title / Objective:","text":"<p>Reproducible Analysis of CMS Open Data: Search for Dark Matter in Association with Top Quarks (Based on the CMS publication: \u201cSearch for dark matter produced in association with a single top quark or a top quark pair in proton\u2013proton collisions at (\\(\\sqrt s = 13 \\TeV\\)\u201d).</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#physics-motivation-and-channel-strategy","title":"Physics Motivation and Channel Strategy","text":"<p>The Large Hadron Collider (LHC) collides protons at center-of-mass energies high enough to probe physics beyond the Standard Model. Although the protons are composite objects, the relevant hard scatterings occur between their constituents \u2014 quarks and gluons. In the context of simplified dark matter models, these partonic interactions can produce top quarks together with a new mediator particle (commonly denoted \u03c6 for scalar or a for pseudoscalar). The mediator then decays invisibly into a pair of dark matter candidates (\\(\\chi \\bar{\\chi}\\)). At the detector level, this results in events with multiple top quarks plus significant missing transverse momentum (\\(p_T^{\\text{miss}}\\)), the latter coming from both neutrinos and the invisible \u03c7 particles.</p> <p>The production mechanisms of interest include:  </p> <ul> <li> <p>Gluon fusion:   $$ gg \\to t \\bar{t}\\,\\phi \\to t \\bar{t} + \\chi \\bar{\\chi} $$  </p> </li> <li> <p>Single top associated production:   $$ gb \\to t \\phi \\to t + \\chi \\bar{\\chi} $$  </p> </li> <li> <p>t\u2013channel production:   $$ qq' \\to tb \\phi \\to tb + \\chi \\bar{\\chi} $$  </p> </li> </ul> <p>In all cases, the top quarks decay via \\(t \\to W b\\). Each W boson subsequently decays either leptonically (\\(W \\to \\ell \\nu\\)) or hadronically (\\(W \\to q \\bar{q}'\\)). Thus, the final states contain a mixture of b-tagged jets, light-flavor jets, charged leptons (electrons or muons), and genuine \\(p_T^{\\text{miss}}\\). The specific experimental signature depends strongly on the decay mode of the W bosons.</p> <p>Because of this, analyses are divided into channels, each defined by the number of isolated charged leptons:</p> <ul> <li>Single-lepton (SL): one isolated electron or muon, several jets (including \u22651 b-tag), and nonzero \\(p_T^{\\text{miss}}\\). This channel is statistically powerful and relatively clean, striking a balance between signal sensitivity and manageable backgrounds.  </li> <li>All-hadronic (AH): no isolated leptons, many jets including b-tagged jets, and \\(p_T^{\\text{miss}}\\). While it has the largest raw yield, it suffers from overwhelming QCD multijet background, which can fake \\(p_T^{\\text{miss}}\\).  </li> <li>Dilepton: two isolated leptons, large \\(p_T^{\\text{miss}}\\), and multiple jets. It provides a very clean signal region but is limited by low branching fraction, hence low statistics.</li> </ul> <p>In this notebook, we concentrate on the single-lepton channel with exactly one muon. There are both theoretical and practical reasons for this choice. From the physics side, the SL channel has the right compromise: it suppresses pure QCD while retaining enough events to make meaningful comparisons. From the experimental side, single-muon triggers are robust, well understood in CMS, and ensure efficient data collection. This focus allows us to demonstrate the full workflow \u2014 from event selection to histograms \u2014 in a setting where the interplay between signal characteristics and background processes can be clearly explained. Splitting into channels is therefore not a stylistic decision but a physics necessity: each final state probes the same underlying processes under different background conditions and detector signatures.</p> <p>After defining the objective of the project (Reproducible Analysis of CMS Open Data), we discussed in which data format to work \u2014 NanoAOD or MiniAOD. We decided to use NanoAOD, because it is lighter and optimized for analysis tasks.</p> <p>In the most precise version, we work within cernbox/swan, but it can work in any Jupyter environment; the important packages are: uproot and awkward</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#single-lepton","title":"Single Lepton","text":"<p>We import all the libraries that we are going to use.</p> <pre><code>%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport hist\n\n\nimport requests\nimport os\nimport time\nimport json\n\n\nimport awkward as ak\nimport uproot\nimport vector\nvector.register_awkward()\n\n\n</code></pre> <pre><code>Matplotlib is building the font cache; this may take a moment.\n</code></pre> <p>We will use the dpoa_workshop_utilities module to help you access the datasets. The functions it contains are:</p> <p>The <code>nanoaod_filenames</code> is a dictionary with the urls to the file indexes of the root files for every dataset that we will use in the analysis.</p> <p>The <code>pretty_print(fields, fmt='40s', require=None, ignore=None)</code> function allows you to print subsets of keys based on strings that you require or ignore. It will also format that output based on how many characters you want in a column (you are limited to 80 characters per line).</p> <p>The <code>build_lumi_mask(lumifile, tree, verbose=False)</code> function helps you mask (select) the data that's collected from collisions.</p> <pre><code>#-------------------------------\n\nimport dpoa_workshop\nfrom dpoa_workshop import (\n    nanoaod_filenames,\n    get_files_for_dataset,\n    pretty_print,\n    build_lumi_mask\n)\n#-------------------------------------------\n</code></pre> <p>In the drafts related to the papers, the datasets used from 2016 are listed along with their run periods and corresponding luminosities. However, one must be careful with these values, because not all periods are available and the data format differs from the one originally used, as previously noted.\u201d</p> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(filename=\"dataset_2016.png\"))\n\n</code></pre> <p></p> <pre><code>\ndisplay(Image(filename=\"MC_data.png\"))\n\n</code></pre> <p></p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#building-the-ntuple-file-index","title":"Building the Ntuple File Index","text":"<p>CMS Open Data provides file index text files (<code>file_index.txt</code>) for each dataset. These contain the actual XRootD paths to the NanoAOD <code>.root</code> files, along with metadata such as the number of events per file. Example line:</p> <p>root://eospublic.cern.ch//eos/opendata/cms/mc/.../nano_1.root nevts=58293</p> <p>The objective is to collect file paths from multiple URLs, organize them by dataset, and store them in a JSON file, while handling possible download errors so the program keeps running.</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#to-streamline-the-workflow","title":"To streamline the workflow:","text":"<ul> <li>We define a function <code>download_files(url)</code> that fetches each <code>file_index.txt</code> via HTTP and extracts only the ROOT file paths.  </li> <li>We loop over all entries in <code>nanoaod_filenames</code> (the dictionary we built earlier) and collect the full list of ROOT files per dataset.  </li> <li>The result is stored in a new dictionary <code>ntuples</code>, which maps dataset -- list of ROOT file paths.  </li> <li>Finally, we save this as a JSON file (<code>ntuples.json</code>) for later reuse</li> </ul> <pre><code>def download_files(url):\n    r = requests.get(url)\n    lines = [ln.strip() for ln in r.text.splitlines() if ln.strip()]\n\n    paths = [ln.split()[0] for ln in lines]\n    return paths\n\nntuples = {}\n\nfor dataset, urls in nanoaod_filenames.items():\n    all_paths = []\n    for url in urls:\n        try:\n            all_paths.extend(download_files(url))\n        except Exception as e:\n            print(f\"[warn] {dataset} {url}--{e}\")\n    ntuples[dataset] = all_paths\n\nwith open(\"ntuples.json\", \"w\") as f:\n    json.dump(ntuples, f, indent=2)\n\nprint(\"ntuples.json :\", list(ntuples.keys()))\n\n</code></pre> <pre><code>ntuples.json : ['met', 'SingleMuon', 'SingleElectron', 'ttbar-semileptonic', 'ttbar-hadronic', 't-channel-top', 'ttW', 'WJets-HT400to600', 'DYJets-Zpt200', 'WW', 'ZZ', 'Zvv']\n</code></pre> <p>And we will download important files like the luminosity file.</p> <pre><code>!wget https://opendata.cern.ch/record/14220/files/Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\n\n</code></pre> <pre><code>--2025-11-05 20:43:37--  https://opendata.cern.ch/record/14220/files/Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\nResolving opendata.cern.ch (opendata.cern.ch)... 137.138.6.31, 2001:1458:201:8b::100:1c8\nConnecting to opendata.cern.ch (opendata.cern.ch)|137.138.6.31|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11686 (11K) [text/plain]\nSaving to: \u2018Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\u2019\n\nCert_271036-284044_ 100%[===================&gt;]  11.41K  --.-KB/s    in 0.001s\n\n2025-11-05 20:43:38 (10.2 MB/s) - \u2018Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\u2019 saved [11686/11686]\n</code></pre> <p>Complete dictionary, in case it's possible to add all the events but it hasn't been found or verified yet...</p> <pre><code>    # --- Cross sections in pb de todo los dataset del paper ---\n\nXSEC2_PB = {\n    # --- TOP QUARK ---\n    \"ttbar-semileptonic\": 364.35,  \n    \"t-channel-top\": 136.02,       # ST t-channel top\n    \"t-channel-antitop\": 80.95,    # ST t-channel antitop\n    \"tW-top\": 35.85,               # ST tW top \n\n    # --- Rare Top ---\n    \"ttW\": 0.2043,  #  to TTWJetsToLNu\n    \"ttZ\": 0.2529,  # to TTZToLLNuNu\n\n    # --- WJETS (HT Binned) ---\n\n    \"WJets-HT70to100\":    1372.0,\n    \"WJets-HT100to200\":   1345.0, # (Revisa este valor, parece alto para ser binned, quiz\u00e1s es LO)\n    \"WJets-HT200to400\":   359.7,  \n    \"WJets-HT400to600\":   48.91,  \n    \"WJets-HT600to800\":   12.05,\n    \"WJets-HT800to1200\":  5.501,  \n    \"WJets-HT1200to2500\": 1.329,  \n    \"WJets-HT2500toInf\":  0.0322,\n\n    # --- DRELL-YAN (DY) ---\n    \"DYJets-HT100to200\": 147.40,\n    \"DYJets-HT200to400\": 40.99,\n    \"DYJets-HT400to600\": 5.678,\n\n    # --- DIBOSON ---\n    \"WW\": 118.7, #   sumados dan aprox esto\n    \"ZZ\": 16.6,  # \n    \"WZ\": 47.13  \n}\n</code></pre> <p>until while these datasets are being used</p> <pre><code>    XSEC_PB = {\n    # --- Top Quark ---\n    \"ttbar-semileptonic\": 364.35,   \n    \"ttbar-hadronic\":     377.96,  \n    \"t-channel-top\":      136.02,   \n    \"ttW\":                0.2043,   \n\n    # --- WJets ---\n    \"WJets-HT400to600\":   48.91, \n\n    # --- Electroweak / Bosons ---\n    \"DYJets-Zpt200\":      1.27,     \n    \"WW\":                 118.7,    \n    \"ZZ\":                 16.6,     \n    \"Zvv\":                77.3,     \n}\n\n</code></pre> <p>This function builds the fileset used by the analysis. It reads the JSON inventory of ntuples, identifies which samples are data or MC, applies an optional file limit for fast debugging, and attaches minimal metadata such as the cross section and number of files. The output is a clean, ready-to-use dictionary that tells the processor exactly which datasets to run over and how they should be treated.</p> <pre><code>def construct_fileset(ntuples_json=\"ntuples.json\", limit=None, verbose=True):\n    \"\"\"\n    Parses the input JSON inventory and assigns metadata (xsec, is_data).\n\n    Args:\n        ntuples_json (str): Path to the JSON file containing the file lists.\n        limit (int or None): Max number of files to load per process. \n                             Useful for quick debugging (e.g., limit=1).\n                             If None, loads all files (production mode).\n        verbose (bool): If True, prints a summary table of loaded samples.\n\n    Returns:\n        dict: A dictionary structured for the processor (Coffea/UpRoot).\n    \"\"\"\n\n    # Load the file manifest\n    with open(ntuples_json) as f:\n        info = json.load(f)\n\n    fileset = {}\n\n    if verbose:\n        print(f\"\\n{'Process Name':30} {'Type':&gt;6} {'N Files':&gt;10} {'XSEC [pb]':&gt;12}\")\n        print(\"-\" * 65)\n\n    # Iterate over each process found in the JSON\n    for process_name, file_list in info.items():\n\n        # --- A. APPLY LIMIT (DEBUG MODE) ---\n        # Slicing handles None gracefully, but explicit check is clearer for readers\n        if limit is not None:\n            files_to_use = file_list[:limit]\n        else:\n            files_to_use = file_list\n\n        # --- B. IDENTIFY TYPE (DATA vs MC) ---\n        # Logic: If process exists in the Cross-Section table, treat as Simulation (MC).\n        # Otherwise, treat as Real Data.\n        if process_name in XSEC_PB:\n            is_data = False\n            xsec_value = XSEC_PB[process_name]\n            proc_type = \"MC\"\n        else:\n            is_data = True\n            xsec_value = None  # Real data has no theoretical xsec here\n            proc_type = \"DATA\"\n\n        # --- C. BUILD DICTIONARY ---\n        # Minimal metadata structure to keep it lightweight\n        fileset[process_name] = {\n            \"files\": files_to_use,\n            \"metadata\": {\n                \"is_data\": is_data,\n                \"xsec\": xsec_value,\n                \"n_files_loaded\": len(files_to_use)\n            }\n        }\n\n        # --- D. LOGGING ---\n        if verbose:\n            xsec_str = f\"{xsec_value:.2f}\" if xsec_value else \"-\"\n            print(f\"{process_name:30} {proc_type:&gt;6} {len(files_to_use):&gt;10} {xsec_str:&gt;12}\")\n\n    if verbose: \n        print(\"-\" * 65)\n        print(f\" Fileset construction complete. Loaded {len(fileset)} processes.\")\n\n    return fileset\n</code></pre> <p>This line initializes the full list of datasets for the analysis. It loads all ntuples defined in ntuples.json (since <code>limit=None</code>) and prints a summary of the samples. The resulting <code>fileset</code> becomes the central input that tells the processor which data and MC files to process.</p> <pre><code>fileset = construct_fileset(\n    ntuples_json=\"ntuples.json\",\n    limit=None,      \n    verbose=True\n)\n</code></pre> <pre><code>Process Name                     Type    N Files    XSEC [pb]\n-----------------------------------------------------------------\nmet                              DATA         32            -\nSingleMuon                       DATA         82            -\nSingleElectron                   DATA         80            -\nttbar-semileptonic                 MC        138       364.35\nttbar-hadronic                     MC        146       377.96\nt-channel-top                      MC         25       136.02\nttW                                MC         12         0.20\nWJets-HT400to600                   MC         11        48.91\nDYJets-Zpt200                      MC         10         1.27\nWW                                 MC         41       118.70\nZZ                                 MC         17        16.60\nZvv                                MC         14        77.30\n-----------------------------------------------------------------\n Fileset construction complete. Loaded 12 processes.\n</code></pre> <p>Before running a massive analysis loop (which might take hours), a physicist always opens one single file first. We call this Exploratory Data Analysis (EDA). You need to verify:</p> <p>1.Are the files actually there?</p> <p>2.What are the variable names? (Is it Muon_pt or Muon_pT? Case matters!)</p> <p>3.What Triggers (HLT) are available?</p> <p>Here is the code organized as a clear \"Exploratory Phase\" for students.</p> <p>Why do we do this? We are about to treat the data as a \"black box\" in the processing loop. But we need to verify the inputs first. We use uproot, which is a Python library that allows us to read CERN ROOT files directly, without needing C++.</p> <pre><code>dataset = \"SingleMuon\"\n\nfor i, fpath in enumerate(fileset[dataset][\"files\"][:10]):\n    print(f\"{i+1:2d}. {fpath}\")\n\n</code></pre> <pre><code> 1. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/120000/61FC1E38-F75C-6B44-AD19-A9894155874E.root\n 2. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/1210000/576759DA-4A35-534B-B926-2A9E4A5A7268.root\n 3. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/0107961B-4308-F845-8F96-E14622BBA484.root\n 4. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/0DEE1709-0416-F24B-ACB2-C68997CB6465.root\n 5. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/1C08614E-0C0E-6044-966A-CAF630CAEF8F.root\n 6. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/1D87B4FB-E31C-9F43-AC21-C32469DE9FC6.root\n 7. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/1EB443F2-1230-8042-B8AE-FD50329CA59B.root\n 8. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/2045F967-9F0A-7C46-9946-787B27D56E88.root\n 9. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/236A04EE-C105-D947-8A2E-F8CC6731644F.root\n10. root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/370BE877-DA24-DB41-A875-07A86EAB6852.root\n</code></pre> <pre><code>#import uproot\n\nf = uproot.open(fileset[\"SingleMuon\"][\"files\"][0])\nevents = f[\"Events\"]\nfor name in events.keys():\n    if \"Muon\" in name or \"Electron\" in name or \"Jet\" in name or \"MET\" in name:\n        print(name)\n\n</code></pre> <pre><code>CaloMET_phi\nCaloMET_pt\nCaloMET_sumEt\nChsMET_phi\nChsMET_pt\nChsMET_sumEt\nnCorrT1METJet\nCorrT1METJet_area\nCorrT1METJet_eta\nCorrT1METJet_muonSubtrFactor\nCorrT1METJet_phi\nCorrT1METJet_rawPt\nDeepMETResolutionTune_phi\nDeepMETResolutionTune_pt\nDeepMETResponseTune_phi\nDeepMETResponseTune_pt\nnElectron\nElectron_dEscaleDown\nElectron_dEscaleUp\nElectron_dEsigmaDown\nElectron_dEsigmaUp\nElectron_deltaEtaSC\nElectron_dr03EcalRecHitSumEt\nElectron_dr03HcalDepth1TowerSumEt\nElectron_dr03TkSumPt\nElectron_dr03TkSumPtHEEP\nElectron_dxy\nElectron_dxyErr\nElectron_dz\nElectron_dzErr\nElectron_eCorr\nElectron_eInvMinusPInv\nElectron_energyErr\nElectron_eta\nElectron_hoe\nElectron_ip3d\nElectron_jetPtRelv2\nElectron_jetRelIso\nElectron_mass\nElectron_miniPFRelIso_all\nElectron_miniPFRelIso_chg\nElectron_mvaFall17V2Iso\nElectron_mvaFall17V2noIso\nElectron_pfRelIso03_all\nElectron_pfRelIso03_chg\nElectron_phi\nElectron_pt\nElectron_r9\nElectron_scEtOverPt\nElectron_sieie\nElectron_sip3d\nElectron_mvaTTH\nElectron_charge\nElectron_cutBased\nElectron_jetIdx\nElectron_pdgId\nElectron_photonIdx\nElectron_tightCharge\nElectron_vidNestedWPBitmap\nElectron_vidNestedWPBitmapHEEP\nElectron_convVeto\nElectron_cutBased_HEEP\nElectron_isPFcand\nElectron_jetNDauCharged\nElectron_lostHits\nElectron_mvaFall17V2Iso_WP80\nElectron_mvaFall17V2Iso_WP90\nElectron_mvaFall17V2Iso_WPL\nElectron_mvaFall17V2noIso_WP80\nElectron_mvaFall17V2noIso_WP90\nElectron_mvaFall17V2noIso_WPL\nElectron_seedGain\nnFatJet\nFatJet_area\nFatJet_btagCSVV2\nFatJet_btagDDBvLV2\nFatJet_btagDDCvBV2\nFatJet_btagDDCvLV2\nFatJet_btagDeepB\nFatJet_btagHbb\nFatJet_deepTagMD_H4qvsQCD\nFatJet_deepTagMD_HbbvsQCD\nFatJet_deepTagMD_TvsQCD\nFatJet_deepTagMD_WvsQCD\nFatJet_deepTagMD_ZHbbvsQCD\nFatJet_deepTagMD_ZHccvsQCD\nFatJet_deepTagMD_ZbbvsQCD\nFatJet_deepTagMD_ZvsQCD\nFatJet_deepTagMD_bbvsLight\nFatJet_deepTagMD_ccvsLight\nFatJet_deepTag_H\nFatJet_deepTag_QCD\nFatJet_deepTag_QCDothers\nFatJet_deepTag_TvsQCD\nFatJet_deepTag_WvsQCD\nFatJet_deepTag_ZvsQCD\nFatJet_eta\nFatJet_mass\nFatJet_msoftdrop\nFatJet_n2b1\nFatJet_n3b1\nFatJet_particleNetMD_QCD\nFatJet_particleNetMD_Xbb\nFatJet_particleNetMD_Xcc\nFatJet_particleNetMD_Xqq\nFatJet_particleNet_H4qvsQCD\nFatJet_particleNet_HbbvsQCD\nFatJet_particleNet_HccvsQCD\nFatJet_particleNet_QCD\nFatJet_particleNet_TvsQCD\nFatJet_particleNet_WvsQCD\nFatJet_particleNet_ZvsQCD\nFatJet_particleNet_mass\nFatJet_phi\nFatJet_pt\nFatJet_rawFactor\nFatJet_tau1\nFatJet_tau2\nFatJet_tau3\nFatJet_tau4\nFatJet_lsf3\nFatJet_jetId\nFatJet_subJetIdx1\nFatJet_subJetIdx2\nFatJet_electronIdx3SJ\nFatJet_muonIdx3SJ\nFatJet_nConstituents\nnJet\nJet_area\nJet_btagCSVV2\nJet_btagDeepB\nJet_btagDeepCvB\nJet_btagDeepCvL\nJet_btagDeepFlavB\nJet_btagDeepFlavCvB\nJet_btagDeepFlavCvL\nJet_btagDeepFlavQG\nJet_chEmEF\nJet_chFPV0EF\nJet_chHEF\nJet_eta\nJet_hfsigmaEtaEta\nJet_hfsigmaPhiPhi\nJet_mass\nJet_muEF\nJet_muonSubtrFactor\nJet_neEmEF\nJet_neHEF\nJet_phi\nJet_pt\nJet_puIdDisc\nJet_qgl\nJet_rawFactor\nJet_bRegCorr\nJet_bRegRes\nJet_cRegCorr\nJet_cRegRes\nJet_electronIdx1\nJet_electronIdx2\nJet_hfadjacentEtaStripsSize\nJet_hfcentralEtaStripSize\nJet_jetId\nJet_muonIdx1\nJet_muonIdx2\nJet_nElectrons\nJet_nMuons\nJet_puId\nJet_nConstituents\nL1PreFiringWeight_Muon_Nom\nL1PreFiringWeight_Muon_StatDn\nL1PreFiringWeight_Muon_StatUp\nL1PreFiringWeight_Muon_SystDn\nL1PreFiringWeight_Muon_SystUp\nnLowPtElectron\nLowPtElectron_ID\nLowPtElectron_convVtxRadius\nLowPtElectron_deltaEtaSC\nLowPtElectron_dxy\nLowPtElectron_dxyErr\nLowPtElectron_dz\nLowPtElectron_dzErr\nLowPtElectron_eInvMinusPInv\nLowPtElectron_embeddedID\nLowPtElectron_energyErr\nLowPtElectron_eta\nLowPtElectron_hoe\nLowPtElectron_mass\nLowPtElectron_miniPFRelIso_all\nLowPtElectron_miniPFRelIso_chg\nLowPtElectron_phi\nLowPtElectron_pt\nLowPtElectron_ptbiased\nLowPtElectron_r9\nLowPtElectron_scEtOverPt\nLowPtElectron_sieie\nLowPtElectron_unbiased\nLowPtElectron_charge\nLowPtElectron_convWP\nLowPtElectron_pdgId\nLowPtElectron_convVeto\nLowPtElectron_lostHits\nMET_MetUnclustEnUpDeltaX\nMET_MetUnclustEnUpDeltaY\nMET_covXX\nMET_covXY\nMET_covYY\nMET_phi\nMET_pt\nMET_significance\nMET_sumEt\nMET_sumPtUnclustered\nnMuon\nMuon_dxy\nMuon_dxyErr\nMuon_dxybs\nMuon_dz\nMuon_dzErr\nMuon_eta\nMuon_ip3d\nMuon_jetPtRelv2\nMuon_jetRelIso\nMuon_mass\nMuon_miniPFRelIso_all\nMuon_miniPFRelIso_chg\nMuon_pfRelIso03_all\nMuon_pfRelIso03_chg\nMuon_pfRelIso04_all\nMuon_phi\nMuon_pt\nMuon_ptErr\nMuon_segmentComp\nMuon_sip3d\nMuon_softMva\nMuon_tkRelIso\nMuon_tunepRelPt\nMuon_mvaLowPt\nMuon_mvaTTH\nMuon_charge\nMuon_jetIdx\nMuon_nStations\nMuon_nTrackerLayers\nMuon_pdgId\nMuon_tightCharge\nMuon_fsrPhotonIdx\nMuon_highPtId\nMuon_highPurity\nMuon_inTimeMuon\nMuon_isGlobal\nMuon_isPFcand\nMuon_isStandalone\nMuon_isTracker\nMuon_jetNDauCharged\nMuon_looseId\nMuon_mediumId\nMuon_mediumPromptId\nMuon_miniIsoId\nMuon_multiIsoId\nMuon_mvaId\nMuon_mvaLowPtId\nMuon_pfIsoId\nMuon_puppiIsoId\nMuon_softId\nMuon_softMvaId\nMuon_tightId\nMuon_tkIsoId\nMuon_triggerIdLoose\nPuppiMET_phi\nPuppiMET_phiJERDown\nPuppiMET_phiJERUp\nPuppiMET_phiJESDown\nPuppiMET_phiJESUp\nPuppiMET_phiUnclusteredDown\nPuppiMET_phiUnclusteredUp\nPuppiMET_pt\nPuppiMET_ptJERDown\nPuppiMET_ptJERUp\nPuppiMET_ptJESDown\nPuppiMET_ptJESUp\nPuppiMET_ptUnclusteredDown\nPuppiMET_ptUnclusteredUp\nPuppiMET_sumEt\nRawMET_phi\nRawMET_pt\nRawMET_sumEt\nRawPuppiMET_phi\nRawPuppiMET_pt\nRawPuppiMET_sumEt\nnSoftActivityJet\nSoftActivityJet_eta\nSoftActivityJet_phi\nSoftActivityJet_pt\nSoftActivityJetHT\nSoftActivityJetHT10\nSoftActivityJetHT2\nSoftActivityJetHT5\nSoftActivityJetNjets10\nSoftActivityJetNjets2\nSoftActivityJetNjets5\nnSubJet\nSubJet_btagCSVV2\nSubJet_btagDeepB\nSubJet_eta\nSubJet_mass\nSubJet_n2b1\nSubJet_n3b1\nSubJet_phi\nSubJet_pt\nSubJet_rawFactor\nSubJet_tau1\nSubJet_tau2\nSubJet_tau3\nSubJet_tau4\nTkMET_phi\nTkMET_pt\nTkMET_sumEt\nElectron_cleanmask\nJet_cleanmask\nMuon_cleanmask\nL1_CastorHaloMuon\nL1_CastorHaloMuon_BptxAND\nL1_CastorHighJet_BptxAND\nL1_CastorMediumJet_BptxAND\nL1_DoubleJet12_ForwardBackward\nL1_DoubleJet16_ForwardBackward\nL1_DoubleJet30_Mj30j30_400_Mu10\nL1_DoubleJet30_Mj30j30_400_Mu6\nL1_DoubleJet8_ForwardBackward\nL1_DoubleJetC100\nL1_DoubleJetC112\nL1_DoubleJetC120\nL1_DoubleJetC40\nL1_DoubleJetC50\nL1_DoubleJetC60\nL1_DoubleJetC60_ETM60\nL1_DoubleJetC80\nL1_DoubleJet_100_30_Mj30j30_620\nL1_DoubleJet_90_30_Mj30j30_620\nL1_ETM75_Jet60_dPhi_Min0p4\nL1_Jet32_DoubleMu_10_0_dPhi_Jet_Mu0_Max0p4_dPhi_Mu_Mu_Min1p0\nL1_Jet32_Mu0_EG10_dPhi_Jet_Mu_Max0p4_dPhi_Mu_EG_Min1p0\nL1_Mu3_JetC120\nL1_Mu3_JetC120_dEta_Max0p4_dPhi_Max0p4\nL1_Mu3_JetC16\nL1_Mu3_JetC16_dEta_Max0p4_dPhi_Max0p4\nL1_Mu3_JetC60\nL1_Mu3_JetC60_dEta_Max0p4_dPhi_Max0p4\nL1_QuadJetC36_Tau52\nL1_QuadJetC40\nL1_QuadJetC50\nL1_QuadJetC60\nL1_SingleJet12\nL1_SingleJet120\nL1_SingleJet12_BptxAND\nL1_SingleJet140\nL1_SingleJet150\nL1_SingleJet16\nL1_SingleJet160\nL1_SingleJet170\nL1_SingleJet180\nL1_SingleJet20\nL1_SingleJet200\nL1_SingleJet35\nL1_SingleJet4\nL1_SingleJet60\nL1_SingleJet8\nL1_SingleJet8_BptxAND\nL1_SingleJet90\nL1_SingleJetC20_NotBptxOR\nL1_SingleJetC20_NotBptxOR_3BX\nL1_SingleJetC40_NotBptxOR_3BX\nL1_SingleJetC40_NotBptxOR_5BX\nL1_TripleJet_84_68_48_VBF\nL1_TripleJet_88_72_56_VBF\nL1_TripleJet_92_76_64_VBF\nFlag_BadPFMuonFilter\nFlag_BadPFMuonDzFilter\nFlag_BadPFMuonSummer16Filter\nFlag_METFilters\nFlag_BadPFMuonFilter_pRECO\nFlag_BadPFMuonSummer16Filter_pRECO\nFlag_METFilters_pRECO\nHLT_AK8PFJet360_TrimMass30\nHLT_AK8PFJet400_TrimMass30\nHLT_AK8DiPFJet300_200_TrimMass30_BTagCSV_p20\nHLT_AK8DiPFJet280_200_TrimMass30_BTagCSV_p087\nHLT_AK8DiPFJet300_200_TrimMass30_BTagCSV_p087\nHLT_AK8DiPFJet300_200_TrimMass30\nHLT_AK8DiPFJet280_200_TrimMass30\nHLT_AK8DiPFJet250_200_TrimMass30\nHLT_AK8DiPFJet280_200_TrimMass30_BTagCSV_p20\nHLT_AK8DiPFJet250_200_TrimMass30_BTagCSV_p20\nHLT_CaloJet500_NoJetID\nHLT_Ele27_WPTight_Gsf_L1JetTauSeeded\nHLT_Ele45_CaloIdVT_GsfTrkIdT_PFJet200_PFJet50\nHLT_IsoMu16_eta2p1_MET30\nHLT_IsoMu16_eta2p1_MET30_LooseIsoPFTau50_Trk30_eta2p1\nHLT_JetE30_NoBPTX3BX\nHLT_JetE30_NoBPTX\nHLT_JetE50_NoBPTX3BX\nHLT_JetE70_NoBPTX3BX\nHLT_LooseIsoPFTau50_Trk30_eta2p1_MET90\nHLT_LooseIsoPFTau50_Trk30_eta2p1_MET110\nHLT_LooseIsoPFTau50_Trk30_eta2p1_MET120\nHLT_Mu30_eta2p1_PFJet150_PFJet50\nHLT_Mu40_eta2p1_PFJet200_PFJet50\nHLT_Mu28NoFiltersNoVtx_DisplacedJet40_Loose\nHLT_Mu38NoFiltersNoVtxDisplaced_DisplacedJet60_Tight\nHLT_Mu38NoFiltersNoVtxDisplaced_DisplacedJet60_Loose\nHLT_Mu38NoFiltersNoVtx_DisplacedJet60_Loose\nHLT_Mu28NoFiltersNoVtx_CentralCaloJet40\nHLT_PFHT300_PFMET110\nHLT_PFHT550_4JetPt50\nHLT_PFHT650_4JetPt50\nHLT_PFHT750_4JetPt70\nHLT_PFHT750_4JetPt80\nHLT_PFHT800_4JetPt50\nHLT_PFHT850_4JetPt50\nHLT_PFJet15_NoCaloMatched\nHLT_PFJet25_NoCaloMatched\nHLT_DiPFJet15_NoCaloMatched\nHLT_DiPFJet25_NoCaloMatched\nHLT_DiPFJet15_FBEta3_NoCaloMatched\nHLT_DiPFJet25_FBEta3_NoCaloMatched\nHLT_DiPFJetAve15_HFJEC\nHLT_DiPFJetAve25_HFJEC\nHLT_DiPFJetAve35_HFJEC\nHLT_AK8PFJet40\nHLT_AK8PFJet60\nHLT_AK8PFJet80\nHLT_AK8PFJet140\nHLT_AK8PFJet200\nHLT_AK8PFJet260\nHLT_AK8PFJet320\nHLT_AK8PFJet400\nHLT_AK8PFJet450\nHLT_AK8PFJet500\nHLT_PFJet40\nHLT_PFJet60\nHLT_PFJet80\nHLT_PFJet140\nHLT_PFJet200\nHLT_PFJet260\nHLT_PFJet320\nHLT_PFJet400\nHLT_PFJet450\nHLT_PFJet500\nHLT_DiPFJetAve40\nHLT_DiPFJetAve60\nHLT_DiPFJetAve80\nHLT_DiPFJetAve140\nHLT_DiPFJetAve200\nHLT_DiPFJetAve260\nHLT_DiPFJetAve320\nHLT_DiPFJetAve400\nHLT_DiPFJetAve500\nHLT_DiPFJetAve60_HFJEC\nHLT_DiPFJetAve80_HFJEC\nHLT_DiPFJetAve100_HFJEC\nHLT_DiPFJetAve160_HFJEC\nHLT_DiPFJetAve220_HFJEC\nHLT_DiPFJetAve300_HFJEC\nHLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu140\nHLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu80\nHLT_DiCentralPFJet170\nHLT_SingleCentralPFJet170_CFMax0p1\nHLT_DiCentralPFJet170_CFMax0p1\nHLT_DiCentralPFJet330_CFMax0p5\nHLT_DiCentralPFJet430\nHLT_DiJetVBF_PassThrough\nHLT_DiJetVBFMu_PassThrough\nHLT_PFHT200_DiPFJetAve90_PFAlphaT0p63\nHLT_PFHT250_DiPFJetAve90_PFAlphaT0p58\nHLT_PFHT300_DiPFJetAve90_PFAlphaT0p54\nHLT_PFHT350_DiPFJetAve90_PFAlphaT0p53\nHLT_PFHT400_DiPFJetAve90_PFAlphaT0p52\nHLT_MET60_IsoTrk35_Loose\nHLT_MET75_IsoTrk50\nHLT_MET90_IsoTrk50\nHLT_PFMET170_NotCleaned\nHLT_PFMET170_HBHECleaned\nHLT_PFMET170_BeamHaloCleaned\nHLT_PFMET170_HBHE_BeamHaloCleaned\nHLT_PFMETTypeOne190_HBHE_BeamHaloCleaned\nHLT_PFMET110_PFMHT110_IDTight\nHLT_PFMET120_PFMHT120_IDTight\nHLT_CaloMHTNoPU90_PFMET90_PFMHT90_IDTight_BTagCSV_p067\nHLT_CaloMHTNoPU90_PFMET90_PFMHT90_IDTight\nHLT_QuadPFJet_BTagCSV_p016_p11_VBF_Mqq200\nHLT_QuadPFJet_BTagCSV_p016_VBF_Mqq460\nHLT_QuadPFJet_BTagCSV_p016_p11_VBF_Mqq240\nHLT_QuadPFJet_BTagCSV_p016_VBF_Mqq500\nHLT_QuadPFJet_VBF\nHLT_L1_TripleJet_VBF\nHLT_QuadJet45_TripleBTagCSV_p087\nHLT_QuadJet45_DoubleBTagCSV_p087\nHLT_DoubleJet90_Double30_TripleBTagCSV_p087\nHLT_DoubleJet90_Double30_DoubleBTagCSV_p087\nHLT_DoubleJetsC100_DoubleBTagCSV_p026_DoublePFJetsC160\nHLT_DoubleJetsC100_DoubleBTagCSV_p014_DoublePFJetsC100MaxDeta1p6\nHLT_DoubleJetsC112_DoubleBTagCSV_p026_DoublePFJetsC172\nHLT_DoubleJetsC112_DoubleBTagCSV_p014_DoublePFJetsC112MaxDeta1p6\nHLT_DoubleJetsC100_SingleBTagCSV_p026\nHLT_DoubleJetsC100_SingleBTagCSV_p014\nHLT_DoubleJetsC100_SingleBTagCSV_p026_SinglePFJetC350\nHLT_DoubleJetsC100_SingleBTagCSV_p014_SinglePFJetC350\nHLT_Photon135_PFMET100\nHLT_Photon22_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon36_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon50_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon75_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon90_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon120_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Ele8_CaloIdL_TrackIdL_IsoVL_PFJet30\nHLT_Ele12_CaloIdL_TrackIdL_IsoVL_PFJet30\nHLT_Ele17_CaloIdL_TrackIdL_IsoVL_PFJet30\nHLT_Ele23_CaloIdL_TrackIdL_IsoVL_PFJet30\nHLT_BTagMu_DiJet20_Mu5\nHLT_BTagMu_DiJet40_Mu5\nHLT_BTagMu_DiJet70_Mu5\nHLT_BTagMu_DiJet110_Mu5\nHLT_BTagMu_DiJet170_Mu5\nHLT_BTagMu_Jet300_Mu5\nHLT_BTagMu_AK8Jet300_Mu5\nHLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL_DZ_L1JetTauSeeded\nHLT_Mu6_PFHT200_PFMET100\nHLT_PFHT650_WideJetMJJ900DEtaJJ1p5\nHLT_PFHT650_WideJetMJJ950DEtaJJ1p5\nHLT_Dimuon0_Jpsi_Muon\nHLT_Dimuon0_Upsilon_Muon\nHLT_QuadMuon0_Dimuon0_Jpsi\nHLT_QuadMuon0_Dimuon0_Upsilon\nHLT_Rsq0p02_MR400_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200\nHLT_Rsq0p02_MR450_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200\nHLT_Rsq0p02_MR500_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200\nHLT_Rsq0p02_MR550_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200\nHLT_VBF_DisplacedJet40_DisplacedTrack\nHLT_VBF_DisplacedJet40_DisplacedTrack_2TrackIP2DSig5\nHLT_VBF_DisplacedJet40_TightID_DisplacedTrack\nHLT_VBF_DisplacedJet40_TightID_Hadronic\nHLT_VBF_DisplacedJet40_VTightID_Hadronic\nHLT_VBF_DisplacedJet40_VVTightID_Hadronic\nHLT_VBF_DisplacedJet40_VTightID_DisplacedTrack\nHLT_VBF_DisplacedJet40_VVTightID_DisplacedTrack\nHLT_PFMETNoMu110_PFMHTNoMu110_IDTight\nHLT_PFMETNoMu120_PFMHTNoMu120_IDTight\nHLT_MonoCentralPFJet80_PFMETNoMu110_PFMHTNoMu110_IDTight\nHLT_MonoCentralPFJet80_PFMETNoMu120_PFMHTNoMu120_IDTight\nHLT_Mu10_CentralPFJet30_BTagCSV_p13\nHLT_DoubleMu3_PFMET50\nHLT_Ele10_CaloIdM_TrackIdM_CentralPFJet30_BTagCSV_p13\nHLT_Ele15_IsoVVVL_PFHT400_PFMET50\nHLT_Mu8_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT300_PFMETNoMu60\nHLT_Mu10_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT350_PFMETNoMu60\nHLT_Mu15_IsoVVVL_PFHT400_PFMET50\nHLT_Mu3_PFJet40\nHLT_Ele8_CaloIdM_TrackIdM_PFJet30\nHLT_Ele12_CaloIdM_TrackIdM_PFJet30\nHLT_Ele17_CaloIdM_TrackIdM_PFJet30\nHLT_Ele23_CaloIdM_TrackIdM_PFJet30\nHLT_Ele50_CaloIdVT_GsfTrkIdT_PFJet165\nHLT_PFHT400_SixJet30_DoubleBTagCSV_p056\nHLT_PFHT450_SixJet40_BTagCSV_p056\nHLT_PFHT400_SixJet30\nHLT_PFHT450_SixJet40\nHLT_MET200\nHLT_AK4CaloJet30\nHLT_AK4CaloJet40\nHLT_AK4CaloJet50\nHLT_AK4CaloJet80\nHLT_AK4CaloJet100\nHLT_AK4PFJet30\nHLT_AK4PFJet50\nHLT_AK4PFJet80\nHLT_AK4PFJet100\nHLT_MET250\nHLT_MET300\nHLT_MET600\nHLT_MET700\nHLT_PFMET300\nHLT_PFMET400\nHLT_PFMET500\nHLT_PFMET600\n</code></pre> <pre><code>for b in events.keys():\n    if b.startswith(\"HLT_\"):\n        print(b)\n\n</code></pre> <pre><code>HLT_AK8PFJet360_TrimMass30\nHLT_AK8PFJet400_TrimMass30\nHLT_AK8PFHT750_TrimMass50\nHLT_AK8PFHT800_TrimMass50\nHLT_AK8DiPFJet300_200_TrimMass30_BTagCSV_p20\nHLT_AK8DiPFJet280_200_TrimMass30_BTagCSV_p087\nHLT_AK8DiPFJet300_200_TrimMass30_BTagCSV_p087\nHLT_AK8DiPFJet300_200_TrimMass30\nHLT_AK8PFHT700_TrimR0p1PT0p03Mass50\nHLT_AK8PFHT650_TrimR0p1PT0p03Mass50\nHLT_AK8PFHT600_TrimR0p1PT0p03Mass50_BTagCSV_p20\nHLT_AK8DiPFJet280_200_TrimMass30\nHLT_AK8DiPFJet250_200_TrimMass30\nHLT_AK8DiPFJet280_200_TrimMass30_BTagCSV_p20\nHLT_AK8DiPFJet250_200_TrimMass30_BTagCSV_p20\nHLT_CaloJet500_NoJetID\nHLT_Dimuon13_PsiPrime\nHLT_Dimuon13_Upsilon\nHLT_Dimuon20_Jpsi\nHLT_DoubleEle24_22_eta2p1_WPLoose_Gsf\nHLT_DoubleEle33_CaloIdL\nHLT_DoubleEle33_CaloIdL_MW\nHLT_DoubleEle33_CaloIdL_GsfTrkIdVL_MW\nHLT_DoubleMediumCombinedIsoPFTau35_Trk1_eta2p1_Reg\nHLT_DoubleTightCombinedIsoPFTau35_Trk1_eta2p1_Reg\nHLT_DoubleMediumCombinedIsoPFTau40_Trk1_eta2p1_Reg\nHLT_DoubleTightCombinedIsoPFTau40_Trk1_eta2p1_Reg\nHLT_DoubleMediumCombinedIsoPFTau40_Trk1_eta2p1\nHLT_DoubleTightCombinedIsoPFTau40_Trk1_eta2p1\nHLT_DoubleEle37_Ele27_CaloIdL_GsfTrkIdVL\nHLT_DoubleMu33NoFiltersNoVtx\nHLT_DoubleMu38NoFiltersNoVtx\nHLT_DoubleMu23NoFiltersNoVtxDisplaced\nHLT_DoubleMu28NoFiltersNoVtxDisplaced\nHLT_DoubleMu0\nHLT_DoubleMu4_3_Bs\nHLT_DoubleMu4_3_Jpsi_Displaced\nHLT_DoubleMu4_JpsiTrk_Displaced\nHLT_DoubleMu4_LowMassNonResonantTrk_Displaced\nHLT_DoubleMu3_Trk_Tau3mu\nHLT_DoubleMu4_PsiPrimeTrk_Displaced\nHLT_Mu7p5_L2Mu2_Jpsi\nHLT_Mu7p5_L2Mu2_Upsilon\nHLT_Mu7p5_Track2_Jpsi\nHLT_Mu7p5_Track3p5_Jpsi\nHLT_Mu7p5_Track7_Jpsi\nHLT_Mu7p5_Track2_Upsilon\nHLT_Mu7p5_Track3p5_Upsilon\nHLT_Mu7p5_Track7_Upsilon\nHLT_Dimuon0er16_Jpsi_NoOS_NoVertexing\nHLT_Dimuon6_Jpsi_NoVertexing\nHLT_DoublePhoton60\nHLT_DoublePhoton85\nHLT_Ele20_eta2p1_WPLoose_Gsf_LooseIsoPFTau28\nHLT_Ele22_eta2p1_WPLoose_Gsf_LooseIsoPFTau29\nHLT_Ele22_eta2p1_WPLoose_Gsf\nHLT_Ele24_eta2p1_WPLoose_Gsf_LooseIsoPFTau30\nHLT_Ele25_WPTight_Gsf\nHLT_Ele25_eta2p1_WPTight_Gsf\nHLT_Ele27_WPLoose_Gsf_WHbbBoost\nHLT_Ele27_WPTight_Gsf\nHLT_Ele27_WPTight_Gsf_L1JetTauSeeded\nHLT_Ele27_eta2p1_WPLoose_Gsf\nHLT_Ele27_eta2p1_WPTight_Gsf\nHLT_Ele30_WPTight_Gsf\nHLT_Ele30_eta2p1_WPTight_Gsf\nHLT_Ele32_WPTight_Gsf\nHLT_Ele32_eta2p1_WPTight_Gsf\nHLT_Ele36_eta2p1_WPLoose_Gsf_LooseIsoPFTau20_SingleL1\nHLT_Ele45_CaloIdVT_GsfTrkIdT_PFJet200_PFJet50\nHLT_Ele105_CaloIdVT_GsfTrkIdT\nHLT_HT200\nHLT_HT275\nHLT_HT325\nHLT_HT425\nHLT_HT575\nHLT_HT430to450\nHLT_HT450to470\nHLT_HT470to500\nHLT_HT500to550\nHLT_HT550to650\nHLT_HT650\nHLT_IsoMu16_eta2p1_MET30\nHLT_IsoMu16_eta2p1_MET30_LooseIsoPFTau50_Trk30_eta2p1\nHLT_DoubleIsoMu17_eta2p1_noDzCut\nHLT_IsoMu19_eta2p1_LooseIsoPFTau20\nHLT_IsoMu19_eta2p1_LooseIsoPFTau20_SingleL1\nHLT_IsoMu19_eta2p1_MediumIsoPFTau32_Trk1_eta2p1_Reg\nHLT_IsoMu19_eta2p1_LooseCombinedIsoPFTau20\nHLT_IsoMu19_eta2p1_MediumCombinedIsoPFTau32_Trk1_eta2p1_Reg\nHLT_IsoMu19_eta2p1_TightCombinedIsoPFTau32_Trk1_eta2p1_Reg\nHLT_IsoMu21_eta2p1_MediumCombinedIsoPFTau32_Trk1_eta2p1_Reg\nHLT_IsoMu21_eta2p1_TightCombinedIsoPFTau32_Trk1_eta2p1_Reg\nHLT_IsoMu20\nHLT_IsoMu21_eta2p1_LooseIsoPFTau20_SingleL1\nHLT_IsoMu21_eta2p1_LooseIsoPFTau50_Trk30_eta2p1_SingleL1\nHLT_IsoMu21_eta2p1_MediumIsoPFTau32_Trk1_eta2p1_Reg\nHLT_IsoMu22\nHLT_IsoMu22_eta2p1\nHLT_IsoMu24\nHLT_IsoMu24_eta2p1\nHLT_IsoMu27\nHLT_IsoTkMu20\nHLT_IsoTkMu22\nHLT_IsoTkMu22_eta2p1\nHLT_IsoTkMu24\nHLT_IsoTkMu24_eta2p1\nHLT_IsoTkMu27\nHLT_JetE30_NoBPTX3BX\nHLT_JetE30_NoBPTX\nHLT_JetE50_NoBPTX3BX\nHLT_JetE70_NoBPTX3BX\nHLT_L1SingleMu18\nHLT_L2Mu10\nHLT_L2DoubleMu23_NoVertex\nHLT_L2DoubleMu28_NoVertex_2Cha_Angle2p5_Mass10\nHLT_L2DoubleMu38_NoVertex_2Cha_Angle2p5_Mass10\nHLT_L2Mu10_NoVertex_NoBPTX3BX\nHLT_L2Mu10_NoVertex_NoBPTX\nHLT_L2Mu45_NoVertex_3Sta_NoBPTX3BX\nHLT_L2Mu40_NoVertex_3Sta_NoBPTX3BX\nHLT_LooseIsoPFTau50_Trk30_eta2p1\nHLT_LooseIsoPFTau50_Trk30_eta2p1_MET90\nHLT_LooseIsoPFTau50_Trk30_eta2p1_MET110\nHLT_LooseIsoPFTau50_Trk30_eta2p1_MET120\nHLT_PFTau120_eta2p1\nHLT_PFTau140_eta2p1\nHLT_VLooseIsoPFTau120_Trk50_eta2p1\nHLT_VLooseIsoPFTau140_Trk50_eta2p1\nHLT_Mu17_Mu8\nHLT_Mu17_Mu8_DZ\nHLT_Mu17_Mu8_SameSign\nHLT_Mu17_Mu8_SameSign_DZ\nHLT_Mu20_Mu10\nHLT_Mu20_Mu10_DZ\nHLT_Mu20_Mu10_SameSign\nHLT_Mu20_Mu10_SameSign_DZ\nHLT_Mu17_TkMu8_DZ\nHLT_Mu17_TrkIsoVVL_Mu8_TrkIsoVVL\nHLT_Mu17_TrkIsoVVL_Mu8_TrkIsoVVL_DZ\nHLT_Mu17_TrkIsoVVL_TkMu8_TrkIsoVVL\nHLT_Mu17_TrkIsoVVL_TkMu8_TrkIsoVVL_DZ\nHLT_Mu25_TkMu0_dEta18_Onia\nHLT_Mu27_TkMu8\nHLT_Mu30_TkMu11\nHLT_Mu30_eta2p1_PFJet150_PFJet50\nHLT_Mu40_TkMu11\nHLT_Mu40_eta2p1_PFJet200_PFJet50\nHLT_Mu20\nHLT_TkMu17\nHLT_TkMu17_TrkIsoVVL_TkMu8_TrkIsoVVL\nHLT_TkMu17_TrkIsoVVL_TkMu8_TrkIsoVVL_DZ\nHLT_TkMu20\nHLT_Mu24_eta2p1\nHLT_TkMu24_eta2p1\nHLT_Mu27\nHLT_TkMu27\nHLT_Mu45_eta2p1\nHLT_Mu50\nHLT_TkMu50\nHLT_Mu38NoFiltersNoVtx_Photon38_CaloIdL\nHLT_Mu42NoFiltersNoVtx_Photon42_CaloIdL\nHLT_Mu28NoFiltersNoVtxDisplaced_Photon28_CaloIdL\nHLT_Mu33NoFiltersNoVtxDisplaced_Photon33_CaloIdL\nHLT_Mu23NoFiltersNoVtx_Photon23_CaloIdL\nHLT_DoubleMu18NoFiltersNoVtx\nHLT_Mu28NoFiltersNoVtx_DisplacedJet40_Loose\nHLT_Mu38NoFiltersNoVtxDisplaced_DisplacedJet60_Tight\nHLT_Mu38NoFiltersNoVtxDisplaced_DisplacedJet60_Loose\nHLT_Mu38NoFiltersNoVtx_DisplacedJet60_Loose\nHLT_Mu28NoFiltersNoVtx_CentralCaloJet40\nHLT_PFHT300_PFMET110\nHLT_PFHT550_4JetPt50\nHLT_PFHT650_4JetPt50\nHLT_PFHT750_4JetPt70\nHLT_PFHT750_4JetPt80\nHLT_PFHT800_4JetPt50\nHLT_PFHT850_4JetPt50\nHLT_PFJet15_NoCaloMatched\nHLT_PFJet25_NoCaloMatched\nHLT_DiPFJet15_NoCaloMatched\nHLT_DiPFJet25_NoCaloMatched\nHLT_DiPFJet15_FBEta3_NoCaloMatched\nHLT_DiPFJet25_FBEta3_NoCaloMatched\nHLT_DiPFJetAve15_HFJEC\nHLT_DiPFJetAve25_HFJEC\nHLT_DiPFJetAve35_HFJEC\nHLT_AK8PFJet40\nHLT_AK8PFJet60\nHLT_AK8PFJet80\nHLT_AK8PFJet140\nHLT_AK8PFJet200\nHLT_AK8PFJet260\nHLT_AK8PFJet320\nHLT_AK8PFJet400\nHLT_AK8PFJet450\nHLT_AK8PFJet500\nHLT_PFJet40\nHLT_PFJet60\nHLT_PFJet80\nHLT_PFJet140\nHLT_PFJet200\nHLT_PFJet260\nHLT_PFJet320\nHLT_PFJet400\nHLT_PFJet450\nHLT_PFJet500\nHLT_DiPFJetAve40\nHLT_DiPFJetAve60\nHLT_DiPFJetAve80\nHLT_DiPFJetAve140\nHLT_DiPFJetAve200\nHLT_DiPFJetAve260\nHLT_DiPFJetAve320\nHLT_DiPFJetAve400\nHLT_DiPFJetAve500\nHLT_DiPFJetAve60_HFJEC\nHLT_DiPFJetAve80_HFJEC\nHLT_DiPFJetAve100_HFJEC\nHLT_DiPFJetAve160_HFJEC\nHLT_DiPFJetAve220_HFJEC\nHLT_DiPFJetAve300_HFJEC\nHLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu140\nHLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu80\nHLT_DiCentralPFJet170\nHLT_SingleCentralPFJet170_CFMax0p1\nHLT_DiCentralPFJet170_CFMax0p1\nHLT_DiCentralPFJet330_CFMax0p5\nHLT_DiCentralPFJet430\nHLT_DiJetVBF_PassThrough\nHLT_DiJetVBFMu_PassThrough\nHLT_PFHT125\nHLT_PFHT200\nHLT_PFHT250\nHLT_PFHT300\nHLT_PFHT350\nHLT_PFHT400\nHLT_PFHT475\nHLT_PFHT600\nHLT_PFHT650\nHLT_PFHT900\nHLT_PFHT200_PFAlphaT0p51\nHLT_PFHT200_DiPFJetAve90_PFAlphaT0p63\nHLT_PFHT250_DiPFJetAve90_PFAlphaT0p58\nHLT_PFHT300_DiPFJetAve90_PFAlphaT0p54\nHLT_PFHT350_DiPFJetAve90_PFAlphaT0p53\nHLT_PFHT400_DiPFJetAve90_PFAlphaT0p52\nHLT_MET60_IsoTrk35_Loose\nHLT_MET75_IsoTrk50\nHLT_MET90_IsoTrk50\nHLT_PFMET170_NotCleaned\nHLT_PFMET170_HBHECleaned\nHLT_PFMET170_BeamHaloCleaned\nHLT_PFMET170_HBHE_BeamHaloCleaned\nHLT_PFMETTypeOne190_HBHE_BeamHaloCleaned\nHLT_PFMET110_PFMHT110_IDTight\nHLT_PFMET120_PFMHT120_IDTight\nHLT_CaloMHTNoPU90_PFMET90_PFMHT90_IDTight_BTagCSV_p067\nHLT_CaloMHTNoPU90_PFMET90_PFMHT90_IDTight\nHLT_QuadPFJet_BTagCSV_p016_p11_VBF_Mqq200\nHLT_QuadPFJet_BTagCSV_p016_VBF_Mqq460\nHLT_QuadPFJet_BTagCSV_p016_p11_VBF_Mqq240\nHLT_QuadPFJet_BTagCSV_p016_VBF_Mqq500\nHLT_QuadPFJet_VBF\nHLT_L1_TripleJet_VBF\nHLT_QuadJet45_TripleBTagCSV_p087\nHLT_QuadJet45_DoubleBTagCSV_p087\nHLT_DoubleJet90_Double30_TripleBTagCSV_p087\nHLT_DoubleJet90_Double30_DoubleBTagCSV_p087\nHLT_DoubleJetsC100_DoubleBTagCSV_p026_DoublePFJetsC160\nHLT_DoubleJetsC100_DoubleBTagCSV_p014_DoublePFJetsC100MaxDeta1p6\nHLT_DoubleJetsC112_DoubleBTagCSV_p026_DoublePFJetsC172\nHLT_DoubleJetsC112_DoubleBTagCSV_p014_DoublePFJetsC112MaxDeta1p6\nHLT_DoubleJetsC100_SingleBTagCSV_p026\nHLT_DoubleJetsC100_SingleBTagCSV_p014\nHLT_DoubleJetsC100_SingleBTagCSV_p026_SinglePFJetC350\nHLT_DoubleJetsC100_SingleBTagCSV_p014_SinglePFJetC350\nHLT_Photon135_PFMET100\nHLT_Photon22_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon22_R9Id90_HE10_Iso40_EBOnly_VBF\nHLT_Photon250_NoHE\nHLT_Photon300_NoHE\nHLT_Photon26_R9Id85_OR_CaloId24b40e_Iso50T80L_Photon16_AND_HE10_R9Id65_Eta2_Mass60\nHLT_Photon36_R9Id85_OR_CaloId24b40e_Iso50T80L_Photon22_AND_HE10_R9Id65_Eta2_Mass15\nHLT_Photon36_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon36_R9Id90_HE10_Iso40_EBOnly_VBF\nHLT_Photon50_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon50_R9Id90_HE10_Iso40_EBOnly_VBF\nHLT_Photon75_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon75_R9Id90_HE10_Iso40_EBOnly_VBF\nHLT_Photon90_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon90_R9Id90_HE10_Iso40_EBOnly_VBF\nHLT_Photon120_R9Id90_HE10_Iso40_EBOnly_PFMET40\nHLT_Photon120_R9Id90_HE10_Iso40_EBOnly_VBF\nHLT_Mu8_TrkIsoVVL\nHLT_Mu17_TrkIsoVVL\nHLT_Ele8_CaloIdL_TrackIdL_IsoVL_PFJet30\nHLT_Ele12_CaloIdL_TrackIdL_IsoVL_PFJet30\nHLT_Ele17_CaloIdL_TrackIdL_IsoVL_PFJet30\nHLT_Ele23_CaloIdL_TrackIdL_IsoVL_PFJet30\nHLT_BTagMu_DiJet20_Mu5\nHLT_BTagMu_DiJet40_Mu5\nHLT_BTagMu_DiJet70_Mu5\nHLT_BTagMu_DiJet110_Mu5\nHLT_BTagMu_DiJet170_Mu5\nHLT_BTagMu_Jet300_Mu5\nHLT_BTagMu_AK8Jet300_Mu5\nHLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL_DZ\nHLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL_DZ_L1JetTauSeeded\nHLT_Ele17_Ele12_CaloIdL_TrackIdL_IsoVL_DZ\nHLT_Ele16_Ele12_Ele8_CaloIdL_TrackIdL\nHLT_Mu8_TrkIsoVVL_Ele17_CaloIdL_TrackIdL_IsoVL\nHLT_Mu8_TrkIsoVVL_Ele23_CaloIdL_TrackIdL_IsoVL_DZ\nHLT_Mu12_TrkIsoVVL_Ele23_CaloIdL_TrackIdL_IsoVL\nHLT_Mu12_TrkIsoVVL_Ele23_CaloIdL_TrackIdL_IsoVL_DZ\nHLT_Mu17_TrkIsoVVL_Ele12_CaloIdL_TrackIdL_IsoVL\nHLT_Mu23_TrkIsoVVL_Ele8_CaloIdL_TrackIdL_IsoVL_DZ\nHLT_Mu23_TrkIsoVVL_Ele12_CaloIdL_TrackIdL_IsoVL\nHLT_Mu23_TrkIsoVVL_Ele12_CaloIdL_TrackIdL_IsoVL_DZ\nHLT_Mu33_Ele33_CaloIdL_GsfTrkIdVL\nHLT_Mu37_Ele27_CaloIdL_GsfTrkIdVL\nHLT_Mu27_Ele37_CaloIdL_GsfTrkIdVL\nHLT_Mu8_DiEle12_CaloIdL_TrackIdL\nHLT_Mu12_Photon25_CaloIdL\nHLT_Mu12_Photon25_CaloIdL_L1ISO\nHLT_Mu12_Photon25_CaloIdL_L1OR\nHLT_Mu17_Photon30_CaloIdL_L1ISO\nHLT_Mu17_Photon35_CaloIdL_L1ISO\nHLT_DiMu9_Ele9_CaloIdL_TrackIdL\nHLT_TripleMu_5_3_3_DZ_Mass3p8\nHLT_TripleMu_12_10_5\nHLT_Mu6_PFHT200_PFMET100\nHLT_Ele17_Ele12_CaloIdL_TrackIdL_IsoVL\nHLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL\nHLT_Ele12_CaloIdL_TrackIdL_IsoVL\nHLT_Ele17_CaloIdL_GsfTrkIdVL\nHLT_Ele17_CaloIdL_TrackIdL_IsoVL\nHLT_Ele23_CaloIdL_TrackIdL_IsoVL\nHLT_PFHT650_WideJetMJJ900DEtaJJ1p5\nHLT_PFHT650_WideJetMJJ950DEtaJJ1p5\nHLT_Photon22\nHLT_Photon30\nHLT_Photon36\nHLT_Photon50\nHLT_Photon75\nHLT_Photon90\nHLT_Photon120\nHLT_Photon175\nHLT_Photon165_HE10\nHLT_Photon22_R9Id90_HE10_IsoM\nHLT_Photon30_R9Id90_HE10_IsoM\nHLT_Photon36_R9Id90_HE10_IsoM\nHLT_Photon50_R9Id90_HE10_IsoM\nHLT_Photon75_R9Id90_HE10_IsoM\nHLT_Photon90_R9Id90_HE10_IsoM\nHLT_Photon120_R9Id90_HE10_IsoM\nHLT_Photon165_R9Id90_HE10_IsoM\nHLT_Diphoton30_18_R9Id_OR_IsoCaloId_AND_HE_R9Id_Mass90\nHLT_Diphoton30_18_R9Id_OR_IsoCaloId_AND_HE_R9Id_DoublePixelSeedMatch_Mass70\nHLT_Diphoton30PV_18PV_R9Id_AND_IsoCaloId_AND_HE_R9Id_DoublePixelVeto_Mass55\nHLT_Diphoton30_18_Solid_R9Id_AND_IsoCaloId_AND_HE_R9Id_Mass55\nHLT_Diphoton30EB_18EB_R9Id_OR_IsoCaloId_AND_HE_R9Id_DoublePixelVeto_Mass55\nHLT_Dimuon0_Jpsi_Muon\nHLT_Dimuon0_Upsilon_Muon\nHLT_QuadMuon0_Dimuon0_Jpsi\nHLT_QuadMuon0_Dimuon0_Upsilon\nHLT_Rsq0p25\nHLT_Rsq0p30\nHLT_RsqMR270_Rsq0p09_MR200\nHLT_RsqMR270_Rsq0p09_MR200_4jet\nHLT_Rsq0p02_MR400_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200\nHLT_Rsq0p02_MR450_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200\nHLT_Rsq0p02_MR500_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200\nHLT_Rsq0p02_MR550_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200\nHLT_HT250_DisplacedDijet40_DisplacedTrack\nHLT_HT350_DisplacedDijet40_DisplacedTrack\nHLT_HT350_DisplacedDijet80_DisplacedTrack\nHLT_HT350_DisplacedDijet80_Tight_DisplacedTrack\nHLT_HT350_DisplacedDijet40_Inclusive\nHLT_HT550_DisplacedDijet80_Inclusive\nHLT_HT650_DisplacedDijet80_Inclusive\nHLT_HT750_DisplacedDijet80_Inclusive\nHLT_VBF_DisplacedJet40_DisplacedTrack\nHLT_VBF_DisplacedJet40_DisplacedTrack_2TrackIP2DSig5\nHLT_VBF_DisplacedJet40_TightID_DisplacedTrack\nHLT_VBF_DisplacedJet40_TightID_Hadronic\nHLT_VBF_DisplacedJet40_VTightID_Hadronic\nHLT_VBF_DisplacedJet40_VVTightID_Hadronic\nHLT_VBF_DisplacedJet40_VTightID_DisplacedTrack\nHLT_VBF_DisplacedJet40_VVTightID_DisplacedTrack\nHLT_PFMETNoMu110_PFMHTNoMu110_IDTight\nHLT_PFMETNoMu120_PFMHTNoMu120_IDTight\nHLT_MonoCentralPFJet80_PFMETNoMu110_PFMHTNoMu110_IDTight\nHLT_MonoCentralPFJet80_PFMETNoMu120_PFMHTNoMu120_IDTight\nHLT_Ele27_eta2p1_WPLoose_Gsf_HT200\nHLT_DoubleMu8_Mass8_PFHT300\nHLT_Mu8_Ele8_CaloIdM_TrackIdM_Mass8_PFHT300\nHLT_DoubleEle8_CaloIdM_TrackIdM_Mass8_PFHT300\nHLT_Mu10_CentralPFJet30_BTagCSV_p13\nHLT_DoubleMu3_PFMET50\nHLT_Ele10_CaloIdM_TrackIdM_CentralPFJet30_BTagCSV_p13\nHLT_Ele15_IsoVVVL_BTagCSV_p067_PFHT400\nHLT_Ele15_IsoVVVL_PFHT600\nHLT_Ele15_IsoVVVL_PFHT400_PFMET50\nHLT_Ele15_IsoVVVL_PFHT400\nHLT_Ele50_IsoVVVL_PFHT400\nHLT_Mu8_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT300_PFMETNoMu60\nHLT_Mu10_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT350_PFMETNoMu60\nHLT_Mu15_IsoVVVL_BTagCSV_p067_PFHT400\nHLT_Mu15_IsoVVVL_PFHT600\nHLT_Mu15_IsoVVVL_PFHT400_PFMET50\nHLT_Mu15_IsoVVVL_PFHT400\nHLT_Mu50_IsoVVVL_PFHT400\nHLT_Dimuon16_Jpsi\nHLT_Dimuon8_PsiPrime_Barrel\nHLT_Dimuon8_Upsilon_Barrel\nHLT_Dimuon0_Phi_Barrel\nHLT_TrkMu15_DoubleTrkMu5NoFiltersNoVtx\nHLT_TrkMu17_DoubleTrkMu8NoFiltersNoVtx\nHLT_Mu8\nHLT_Mu17\nHLT_Mu3_PFJet40\nHLT_Ele8_CaloIdM_TrackIdM_PFJet30\nHLT_Ele12_CaloIdM_TrackIdM_PFJet30\nHLT_Ele17_CaloIdM_TrackIdM_PFJet30\nHLT_Ele23_CaloIdM_TrackIdM_PFJet30\nHLT_Ele50_CaloIdVT_GsfTrkIdT_PFJet165\nHLT_PFHT400_SixJet30_DoubleBTagCSV_p056\nHLT_PFHT450_SixJet40_BTagCSV_p056\nHLT_PFHT400_SixJet30\nHLT_PFHT450_SixJet40\nHLT_Ele115_CaloIdVT_GsfTrkIdT\nHLT_Ele145_CaloIdVT_GsfTrkIdT\nHLT_Ele200_CaloIdVT_GsfTrkIdT\nHLT_Mu55\nHLT_Photon42_R9Id85_OR_CaloId24b40e_Iso50T80L_Photon25_AND_HE10_R9Id65_Eta2_Mass15\nHLT_Photon90_CaloIdL_PFHT600\nHLT_FullTracks_Multiplicity80\nHLT_FullTracks_Multiplicity100\nHLT_FullTracks_Multiplicity130\nHLT_FullTracks_Multiplicity150\nHLT_ECALHT800\nHLT_DiSC30_18_EIso_AND_HE_Mass70\nHLT_MET200\nHLT_Ele27_HighEta_Ele20_Mass55\nHLT_L1FatEvents\nHLT_Physics\nHLT_L1FatEvents_part0\nHLT_L1FatEvents_part1\nHLT_L1FatEvents_part2\nHLT_L1FatEvents_part3\nHLT_Random\nHLT_ZeroBias\nHLT_ZeroBias_part0\nHLT_ZeroBias_part1\nHLT_ZeroBias_part2\nHLT_ZeroBias_part3\nHLT_ZeroBias_part4\nHLT_ZeroBias_part5\nHLT_ZeroBias_part6\nHLT_ZeroBias_part7\nHLT_AK4CaloJet30\nHLT_AK4CaloJet40\nHLT_AK4CaloJet50\nHLT_AK4CaloJet80\nHLT_AK4CaloJet100\nHLT_AK4PFJet30\nHLT_AK4PFJet50\nHLT_AK4PFJet80\nHLT_AK4PFJet100\nHLT_HISinglePhoton10\nHLT_HISinglePhoton15\nHLT_HISinglePhoton20\nHLT_HISinglePhoton40\nHLT_HISinglePhoton60\nHLT_EcalCalibration\nHLT_HcalCalibration\nHLT_GlobalRunHPDNoise\nHLT_L1BptxMinus\nHLT_L1BptxPlus\nHLT_L1NotBptxOR\nHLT_L1MinimumBiasHF_OR_part0\nHLT_L1MinimumBiasHF_OR_part1\nHLT_L1MinimumBiasHF_OR_part2\nHLT_L1MinimumBiasHF_OR_part3\nHLT_L1MinimumBiasHF_OR_part4\nHLT_L1MinimumBiasHF_OR_part5\nHLT_L1MinimumBiasHF_OR_part6\nHLT_L1MinimumBiasHF_OR_part7\nHLT_L1MinimumBiasHF_OR_part8\nHLT_L1MinimumBiasHF_OR_part9\nHLT_L1MinimumBiasHF_AND\nHLT_HcalNZS\nHLT_HcalPhiSym\nHLT_HcalIsolatedbunch\nHLT_ZeroBias_FirstCollisionAfterAbortGap\nHLT_ZeroBias_FirstCollisionAfterAbortGap_copy\nHLT_ZeroBias_IsolatedBunches\nHLT_ZeroBias_FirstCollisionInTrain\nHLT_ZeroBias_FirstBXAfterTrain\nHLT_Photon500\nHLT_Photon600\nHLT_Mu300\nHLT_Mu350\nHLT_MET250\nHLT_MET300\nHLT_MET600\nHLT_MET700\nHLT_PFMET300\nHLT_PFMET400\nHLT_PFMET500\nHLT_PFMET600\nHLT_Ele250_CaloIdVT_GsfTrkIdT\nHLT_Ele300_CaloIdVT_GsfTrkIdT\nHLT_HT2000\nHLT_HT2500\nHLT_IsoTrackHE\nHLT_IsoTrackHB\n</code></pre> <pre><code>#import uproot\n\nsample = \"ttbar-semileptonic\"\n\nroot_path = fileset[sample][\"files\"][0]\nprint(\"Open:\", root_path)\n\nf = uproot.open(root_path)\nevents = f[\"Events\"]\n\nprint(\"# events:\", events.num_entries)\n\nall_keys = events.keys()\nprint(f\"Total braches : {len(all_keys)}\")\n\n</code></pre> <pre><code>Open: root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/120000/08FCB2ED-176B-064B-85AB-37B898773B98.root\n# events: 1233000\nTotal braches : 1504\n</code></pre> <pre><code>#from dpoa_workshop import pretty_print\n\npretty_print(all_keys, fmt='35s', require='Muon')\npretty_print(all_keys, fmt='35s', require='Electron')\npretty_print(all_keys, fmt='35s', require='Jet')\npretty_print(all_keys, fmt='35s', require='MET')\n\n</code></pre> <pre><code>Jet_nMuons                          L1PreFiringWeight_Muon_Nom          \nL1PreFiringWeight_Muon_StatDn       L1PreFiringWeight_Muon_StatUp       \nL1PreFiringWeight_Muon_SystDn       L1PreFiringWeight_Muon_SystUp       nMuon                               \nMuon_dxy                            Muon_dxyErr                         \nMuon_dxybs                          Muon_dz                             \nMuon_dzErr                          Muon_eta                            \nMuon_ip3d                           Muon_jetPtRelv2                     \nMuon_jetRelIso                      Muon_mass                           \nMuon_miniPFRelIso_all               Muon_miniPFRelIso_chg               \nMuon_pfRelIso03_all                 Muon_pfRelIso03_chg                 \nMuon_pfRelIso04_all                 Muon_phi                            Muon_pt                             \nMuon_ptErr                          Muon_segmentComp                    \nMuon_sip3d                          Muon_softMva                        \nMuon_tkRelIso                       Muon_tunepRelPt                     \nMuon_mvaLowPt                       Muon_mvaTTH                         \nMuon_charge                         Muon_jetIdx                         \nMuon_nStations                      Muon_nTrackerLayers                 \nMuon_pdgId                          Muon_tightCharge                    \nMuon_fsrPhotonIdx                   Muon_highPtId                       \nMuon_highPurity                     Muon_inTimeMuon                     \nMuon_isGlobal                       Muon_isPFcand                       \nMuon_isStandalone                   Muon_isTracker                      \nMuon_jetNDauCharged                 Muon_looseId                        \nMuon_mediumId                       Muon_mediumPromptId                 \nMuon_miniIsoId                      Muon_multiIsoId                     \nMuon_mvaId                          Muon_mvaLowPtId                     \nMuon_pfIsoId                        Muon_puppiIsoId                     \nMuon_softId                         Muon_softMvaId                      \nMuon_tightId                        Muon_tkIsoId                        \nMuon_triggerIdLoose                 Muon_genPartIdx                     \nMuon_genPartFlav                    Muon_cleanmask                      \nFlag_BadPFMuonFilter                Flag_BadPFMuonDzFilter              \nFlag_BadPFMuonSummer16Filter        HLT_Dimuon0_Jpsi_Muon               \nHLT_Dimuon0_Upsilon_Muon            HLT_QuadMuon0_Dimuon0_Jpsi          \nHLT_QuadMuon0_Dimuon0_Upsilon       \nnElectron                           Electron_dEscaleDown                \nElectron_dEscaleUp                  Electron_dEsigmaDown                \nElectron_dEsigmaUp                  Electron_deltaEtaSC                 \nElectron_dr03EcalRecHitSumEt        Electron_dr03HcalDepth1TowerSumEt   \nElectron_dr03TkSumPt                Electron_dr03TkSumPtHEEP            \nElectron_dxy                        Electron_dxyErr                     \nElectron_dz                         Electron_dzErr                      \nElectron_eCorr                      Electron_eInvMinusPInv              \nElectron_energyErr                  Electron_eta                        \nElectron_hoe                        Electron_ip3d                       \nElectron_jetPtRelv2                 Electron_jetRelIso                  \nElectron_mass                       Electron_miniPFRelIso_all           \nElectron_miniPFRelIso_chg           Electron_mvaFall17V2Iso             \nElectron_mvaFall17V2noIso           Electron_pfRelIso03_all             \nElectron_pfRelIso03_chg             Electron_phi                        \nElectron_pt                         Electron_r9                         \nElectron_scEtOverPt                 Electron_sieie                      \nElectron_sip3d                      Electron_mvaTTH                     \nElectron_charge                     Electron_cutBased                   \nElectron_jetIdx                     Electron_pdgId                      \nElectron_photonIdx                  Electron_tightCharge                \nElectron_vidNestedWPBitmap          Electron_vidNestedWPBitmapHEEP      \nElectron_convVeto                   Electron_cutBased_HEEP              \nElectron_isPFcand                   Electron_jetNDauCharged             \nElectron_lostHits                   Electron_mvaFall17V2Iso_WP80        \nElectron_mvaFall17V2Iso_WP90        Electron_mvaFall17V2Iso_WPL         \nElectron_mvaFall17V2noIso_WP80      Electron_mvaFall17V2noIso_WP90      \nElectron_mvaFall17V2noIso_WPL       Electron_seedGain                   \nJet_nElectrons                      nLowPtElectron                      \nLowPtElectron_ID                    LowPtElectron_convVtxRadius         \nLowPtElectron_deltaEtaSC            LowPtElectron_dxy                   \nLowPtElectron_dxyErr                LowPtElectron_dz                    \nLowPtElectron_dzErr                 LowPtElectron_eInvMinusPInv         \nLowPtElectron_embeddedID            LowPtElectron_energyErr             \nLowPtElectron_eta                   LowPtElectron_hoe                   \nLowPtElectron_mass                  LowPtElectron_miniPFRelIso_all      \nLowPtElectron_miniPFRelIso_chg      LowPtElectron_phi                   \nLowPtElectron_pt                    LowPtElectron_ptbiased              \nLowPtElectron_r9                    LowPtElectron_scEtOverPt            \nLowPtElectron_sieie                 LowPtElectron_unbiased              \nLowPtElectron_charge                LowPtElectron_convWP                \nLowPtElectron_pdgId                 LowPtElectron_convVeto              \nLowPtElectron_lostHits              Electron_genPartIdx                 \nElectron_genPartFlav                LowPtElectron_genPartIdx            \nLowPtElectron_genPartFlav           Electron_cleanmask                  \nnCorrT1METJet                       CorrT1METJet_area                   \nCorrT1METJet_eta                    CorrT1METJet_muonSubtrFactor        \nCorrT1METJet_phi                    CorrT1METJet_rawPt                  nFatJet                             \nFatJet_area                         FatJet_btagCSVV2                    \nFatJet_btagDDBvLV2                  FatJet_btagDDCvBV2                  \nFatJet_btagDDCvLV2                  FatJet_btagDeepB                    \nFatJet_btagHbb                      FatJet_deepTagMD_H4qvsQCD           \nFatJet_deepTagMD_HbbvsQCD           FatJet_deepTagMD_TvsQCD             \nFatJet_deepTagMD_WvsQCD             FatJet_deepTagMD_ZHbbvsQCD          \nFatJet_deepTagMD_ZHccvsQCD          FatJet_deepTagMD_ZbbvsQCD           \nFatJet_deepTagMD_ZvsQCD             FatJet_deepTagMD_bbvsLight          \nFatJet_deepTagMD_ccvsLight          FatJet_deepTag_H                    \nFatJet_deepTag_QCD                  FatJet_deepTag_QCDothers            \nFatJet_deepTag_TvsQCD               FatJet_deepTag_WvsQCD               \nFatJet_deepTag_ZvsQCD               FatJet_eta                          \nFatJet_mass                         FatJet_msoftdrop                    \nFatJet_n2b1                         FatJet_n3b1                         \nFatJet_particleNetMD_QCD            FatJet_particleNetMD_Xbb            \nFatJet_particleNetMD_Xcc            FatJet_particleNetMD_Xqq            \nFatJet_particleNet_H4qvsQCD         FatJet_particleNet_HbbvsQCD         \nFatJet_particleNet_HccvsQCD         FatJet_particleNet_QCD              \nFatJet_particleNet_TvsQCD           FatJet_particleNet_WvsQCD           \nFatJet_particleNet_ZvsQCD           FatJet_particleNet_mass             \nFatJet_phi                          FatJet_pt                           \nFatJet_rawFactor                    FatJet_tau1                         \nFatJet_tau2                         FatJet_tau3                         \nFatJet_tau4                         FatJet_lsf3                         \nFatJet_jetId                        FatJet_subJetIdx1                   \nFatJet_subJetIdx2                   FatJet_electronIdx3SJ               \nFatJet_muonIdx3SJ                   FatJet_nConstituents                \nnGenJetAK8                          GenJetAK8_eta                       \nGenJetAK8_mass                      GenJetAK8_phi                       \nGenJetAK8_pt                        nGenJet                             \nGenJet_eta                          GenJet_mass                         \nGenJet_phi                          GenJet_pt                           \nnSubGenJetAK8                       SubGenJetAK8_eta                    \nSubGenJetAK8_mass                   SubGenJetAK8_phi                    \nSubGenJetAK8_pt                     nJet                                Jet_area                            \nJet_btagCSVV2                       Jet_btagDeepB                       \nJet_btagDeepCvB                     Jet_btagDeepCvL                     \nJet_btagDeepFlavB                   Jet_btagDeepFlavCvB                 \nJet_btagDeepFlavCvL                 Jet_btagDeepFlavQG                  \nJet_chEmEF                          Jet_chFPV0EF                        \nJet_chHEF                           Jet_eta                             \nJet_hfsigmaEtaEta                   Jet_hfsigmaPhiPhi                   Jet_mass                            \nJet_muEF                            Jet_muonSubtrFactor                 \nJet_neEmEF                          Jet_neHEF                           Jet_phi                             \nJet_pt                              Jet_puIdDisc                        Jet_qgl                             \nJet_rawFactor                       Jet_bRegCorr                        \nJet_bRegRes                         Jet_cRegCorr                        \nJet_cRegRes                         Jet_electronIdx1                    \nJet_electronIdx2                    Jet_hfadjacentEtaStripsSize         \nJet_hfcentralEtaStripSize           Jet_jetId                           \nJet_muonIdx1                        Jet_muonIdx2                        \nJet_nElectrons                      Jet_nMuons                          Jet_puId                            \nJet_nConstituents                   nSoftActivityJet                    \nSoftActivityJet_eta                 SoftActivityJet_phi                 \nSoftActivityJet_pt                  SoftActivityJetHT                   \nSoftActivityJetHT10                 SoftActivityJetHT2                  \nSoftActivityJetHT5                  SoftActivityJetNjets10              \nSoftActivityJetNjets2               SoftActivityJetNjets5               nSubJet                             \nSubJet_btagCSVV2                    SubJet_btagDeepB                    \nSubJet_eta                          SubJet_mass                         \nSubJet_n2b1                         SubJet_n3b1                         \nSubJet_phi                          SubJet_pt                           \nSubJet_rawFactor                    SubJet_tau1                         \nSubJet_tau2                         SubJet_tau3                         \nSubJet_tau4                         FatJet_genJetAK8Idx                 \nFatJet_hadronFlavour                FatJet_nBHadrons                    \nFatJet_nCHadrons                    GenJetAK8_partonFlavour             \nGenJetAK8_hadronFlavour             GenJet_partonFlavour                \nGenJet_hadronFlavour                Jet_genJetIdx                       \nJet_hadronFlavour                   Jet_partonFlavour                   \nJet_cleanmask                       SubJet_hadronFlavour                \nSubJet_nBHadrons                    SubJet_nCHadrons                    \nL1_DoubleJet12_ForwardBackward      L1_DoubleJet16_ForwardBackward      \nL1_DoubleJet8_ForwardBackward       L1_DoubleJetC100                    \nL1_DoubleJetC112                    L1_DoubleJetC120                    \nL1_DoubleJetC40                     L1_DoubleJetC50                     \nL1_DoubleJetC60                     L1_DoubleJetC60_ETM60               \nL1_DoubleJetC80                     L1_ETM75_Jet60_dPhi_Min0p4          \nL1_Jet32_DoubleMu_10_0_dPhi_Jet_Mu0_Max0p4_dPhi_Mu_Mu_Min1p0 \nL1_Jet32_Mu0_EG10_dPhi_Jet_Mu_Max0p4_dPhi_Mu_EG_Min1p0 L1_Mu3_JetC120                      \nL1_Mu3_JetC120_dEta_Max0p4_dPhi_Max0p4 L1_Mu3_JetC16                       \nL1_Mu3_JetC16_dEta_Max0p4_dPhi_Max0p4 L1_Mu3_JetC60                       \nL1_Mu3_JetC60_dEta_Max0p4_dPhi_Max0p4 L1_QuadJetC36_Tau52                 \nL1_QuadJetC40                       L1_QuadJetC50                       \nL1_QuadJetC60                       L1_SingleJet120                     \nL1_SingleJet12_BptxAND              L1_SingleJet140                     \nL1_SingleJet150                     L1_SingleJet16                      \nL1_SingleJet160                     L1_SingleJet170                     \nL1_SingleJet180                     L1_SingleJet20                      \nL1_SingleJet200                     L1_SingleJet35                      \nL1_SingleJet60                      L1_SingleJet8_BptxAND               \nL1_SingleJet90                      L1_SingleJetC20_NotBptxOR           \nL1_SingleJetC20_NotBptxOR_3BX       L1_SingleJetC32_NotBptxOR           \nL1_SingleJetC32_NotBptxOR_3BX       L1_SingleJetC36_NotBptxOR_3BX       \nL1_TripleJet_84_68_48_VBF           L1_TripleJet_88_72_56_VBF           \nL1_TripleJet_92_76_64_VBF           HLT_AK8PFJet360_TrimMass30          \nHLT_AK8PFJet400_TrimMass30          HLT_AK8DiPFJet300_200_TrimMass30_BTagCSV_p20 \nHLT_AK8DiPFJet280_200_TrimMass30_BTagCSV_p087 \nHLT_AK8DiPFJet300_200_TrimMass30_BTagCSV_p087 HLT_AK8DiPFJet300_200_TrimMass30    \nHLT_AK8DiPFJet280_200_TrimMass30    HLT_AK8DiPFJet250_200_TrimMass30    \nHLT_AK8DiPFJet280_200_TrimMass30_BTagCSV_p20 \nHLT_AK8DiPFJet250_200_TrimMass30_BTagCSV_p20 HLT_CaloJet260                      \nHLT_CaloJet500_NoJetID              HLT_Ele27_WPTight_Gsf_L1JetTauSeeded \nHLT_Ele35_CaloIdVT_GsfTrkIdT_PFJet150_PFJet50 \nHLT_Ele45_WPLoose_Gsf_L1JetTauSeeded \nHLT_Ele45_CaloIdVT_GsfTrkIdT_PFJet200_PFJet50 HLT_JetE30_NoBPTX3BX                \nHLT_JetE30_NoBPTX                   HLT_JetE50_NoBPTX3BX                \nHLT_JetE70_NoBPTX3BX                HLT_Mu30_eta2p1_PFJet150_PFJet50    \nHLT_Mu40_eta2p1_PFJet200_PFJet50    \nHLT_Mu33NoFiltersNoVtxDisplaced_DisplacedJet50_Tight \nHLT_Mu33NoFiltersNoVtxDisplaced_DisplacedJet50_Loose \nHLT_Mu28NoFiltersNoVtx_DisplacedJet40_Loose \nHLT_Mu38NoFiltersNoVtxDisplaced_DisplacedJet60_Tight \nHLT_Mu38NoFiltersNoVtxDisplaced_DisplacedJet60_Loose \nHLT_Mu38NoFiltersNoVtx_DisplacedJet60_Loose \nHLT_Mu28NoFiltersNoVtx_CentralCaloJet40 HLT_PFHT550_4JetPt50                \nHLT_PFHT650_4JetPt50                HLT_PFHT750_4JetPt50                \nHLT_PFHT750_4JetPt70                HLT_PFHT750_4JetPt80                \nHLT_PFHT800_4JetPt50                HLT_PFHT850_4JetPt50                \nHLT_PFJet15_NoCaloMatched           HLT_PFJet25_NoCaloMatched           \nHLT_DiPFJet15_NoCaloMatched         HLT_DiPFJet25_NoCaloMatched         \nHLT_DiPFJet15_FBEta3_NoCaloMatched  HLT_DiPFJet25_FBEta3_NoCaloMatched  \nHLT_DiPFJetAve15_HFJEC              HLT_DiPFJetAve25_HFJEC              \nHLT_DiPFJetAve35_HFJEC              HLT_AK8PFJet40                      \nHLT_AK8PFJet60                      HLT_AK8PFJet80                      \nHLT_AK8PFJet140                     HLT_AK8PFJet200                     \nHLT_AK8PFJet260                     HLT_AK8PFJet320                     \nHLT_AK8PFJet400                     HLT_AK8PFJet450                     \nHLT_AK8PFJet500                     HLT_PFJet40                         \nHLT_PFJet60                         HLT_PFJet80                         \nHLT_PFJet140                        HLT_PFJet200                        \nHLT_PFJet260                        HLT_PFJet320                        \nHLT_PFJet400                        HLT_PFJet450                        \nHLT_PFJet500                        HLT_DiPFJetAve40                    \nHLT_DiPFJetAve60                    HLT_DiPFJetAve80                    \nHLT_DiPFJetAve140                   HLT_DiPFJetAve200                   \nHLT_DiPFJetAve260                   HLT_DiPFJetAve320                   \nHLT_DiPFJetAve400                   HLT_DiPFJetAve500                   \nHLT_DiPFJetAve60_HFJEC              HLT_DiPFJetAve80_HFJEC              \nHLT_DiPFJetAve100_HFJEC             HLT_DiPFJetAve160_HFJEC             \nHLT_DiPFJetAve220_HFJEC             HLT_DiPFJetAve300_HFJEC             \nHLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu140 \nHLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu80 HLT_DiCentralPFJet170               \nHLT_SingleCentralPFJet170_CFMax0p1  HLT_DiCentralPFJet170_CFMax0p1      \nHLT_DiCentralPFJet220_CFMax0p3      HLT_DiCentralPFJet330_CFMax0p5      \nHLT_DiCentralPFJet430               HLT_PFHT200_DiPFJetAve90_PFAlphaT0p57 \nHLT_PFHT200_DiPFJetAve90_PFAlphaT0p63 HLT_PFHT250_DiPFJetAve90_PFAlphaT0p55 \nHLT_PFHT250_DiPFJetAve90_PFAlphaT0p58 HLT_PFHT300_DiPFJetAve90_PFAlphaT0p53 \nHLT_PFHT300_DiPFJetAve90_PFAlphaT0p54 HLT_PFHT350_DiPFJetAve90_PFAlphaT0p52 \nHLT_PFHT350_DiPFJetAve90_PFAlphaT0p53 HLT_PFHT400_DiPFJetAve90_PFAlphaT0p51 \nHLT_PFHT400_DiPFJetAve90_PFAlphaT0p52 HLT_PFMET170_JetIdCleaned           \nHLT_QuadPFJet_BTagCSV_p016_p11_VBF_Mqq200 HLT_QuadPFJet_BTagCSV_p016_VBF_Mqq460 \nHLT_QuadPFJet_BTagCSV_p016_p11_VBF_Mqq240 HLT_QuadPFJet_BTagCSV_p016_VBF_Mqq500 \nHLT_QuadPFJet_VBF                   HLT_L1_TripleJet_VBF                \nHLT_QuadJet45_TripleBTagCSV_p087    HLT_QuadJet45_DoubleBTagCSV_p087    \nHLT_DoubleJet90_Double30_TripleBTagCSV_p087 \nHLT_DoubleJet90_Double30_DoubleBTagCSV_p087 \nHLT_DoubleJetsC100_DoubleBTagCSV_p026_DoublePFJetsC160 \nHLT_DoubleJetsC100_DoubleBTagCSV_p014_DoublePFJetsC100MaxDeta1p6 \nHLT_DoubleJetsC112_DoubleBTagCSV_p026_DoublePFJetsC172 \nHLT_DoubleJetsC112_DoubleBTagCSV_p014_DoublePFJetsC112MaxDeta1p6 \nHLT_DoubleJetsC100_SingleBTagCSV_p026 HLT_DoubleJetsC100_SingleBTagCSV_p014 \nHLT_DoubleJetsC100_SingleBTagCSV_p026_SinglePFJetC350 \nHLT_DoubleJetsC100_SingleBTagCSV_p014_SinglePFJetC350 \nHLT_Ele8_CaloIdL_TrackIdL_IsoVL_PFJet30 HLT_Ele12_CaloIdL_TrackIdL_IsoVL_PFJet30 \nHLT_Ele17_CaloIdL_TrackIdL_IsoVL_PFJet30 \nHLT_Ele23_CaloIdL_TrackIdL_IsoVL_PFJet30 HLT_BTagMu_DiJet20_Mu5              \nHLT_BTagMu_DiJet40_Mu5              HLT_BTagMu_DiJet70_Mu5              \nHLT_BTagMu_DiJet110_Mu5             HLT_BTagMu_DiJet170_Mu5             \nHLT_BTagMu_Jet300_Mu5               HLT_BTagMu_AK8Jet300_Mu5            \nHLT_Ele23_Ele12_CaloIdL_TrackIdL_IsoVL_DZ_L1JetTauSeeded \nHLT_PFHT650_WideJetMJJ900DEtaJJ1p5  HLT_PFHT650_WideJetMJJ950DEtaJJ1p5  \nHLT_Rsq0p02_MR300_TriPFJet80_60_40_BTagCSV_p063_p20_Mbb60_200 \nHLT_Rsq0p02_MR400_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200 \nHLT_Rsq0p02_MR450_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200 \nHLT_Rsq0p02_MR500_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200 \nHLT_Rsq0p02_MR550_TriPFJet80_60_40_DoubleBTagCSV_p063_Mbb60_200 \nHLT_VBF_DisplacedJet40_DisplacedTrack \nHLT_VBF_DisplacedJet40_DisplacedTrack_2TrackIP2DSig5 \nHLT_VBF_DisplacedJet40_TightID_DisplacedTrack HLT_VBF_DisplacedJet40_Hadronic     \nHLT_VBF_DisplacedJet40_Hadronic_2PromptTrack \nHLT_VBF_DisplacedJet40_TightID_Hadronic HLT_VBF_DisplacedJet40_VTightID_Hadronic \nHLT_VBF_DisplacedJet40_VVTightID_Hadronic \nHLT_VBF_DisplacedJet40_VTightID_DisplacedTrack \nHLT_VBF_DisplacedJet40_VVTightID_DisplacedTrack \nHLT_MonoCentralPFJet80_PFMETNoMu90_PFMHTNoMu90_IDTight \nHLT_MonoCentralPFJet80_PFMETNoMu100_PFMHTNoMu100_IDTight \nHLT_MonoCentralPFJet80_PFMETNoMu110_PFMHTNoMu110_IDTight \nHLT_MonoCentralPFJet80_PFMETNoMu120_PFMHTNoMu120_IDTight \nHLT_Mu10_CentralPFJet30_BTagCSV_p13 \nHLT_Ele10_CaloIdM_TrackIdM_CentralPFJet30_BTagCSV_p13 \nHLT_Mu8_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT300_PFMETNoMu60 \nHLT_Mu10_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT350_PFMETNoMu60 HLT_Mu3_PFJet40                     \nHLT_Ele8_CaloIdM_TrackIdM_PFJet30   HLT_Ele12_CaloIdM_TrackIdM_PFJet30  \nHLT_Ele17_CaloIdM_TrackIdM_PFJet30  HLT_Ele23_CaloIdM_TrackIdM_PFJet30  \nHLT_Ele50_CaloIdVT_GsfTrkIdT_PFJet140 HLT_Ele50_CaloIdVT_GsfTrkIdT_PFJet165 \nHLT_PFHT400_SixJet30_DoubleBTagCSV_p056 HLT_PFHT450_SixJet40_BTagCSV_p056   \nHLT_PFHT400_SixJet30                HLT_PFHT450_SixJet40                \nHLT_AK4CaloJet30                    HLT_AK4CaloJet40                    \nHLT_AK4CaloJet50                    HLT_AK4CaloJet80                    \nHLT_AK4CaloJet100                   HLT_AK4PFJet30                      \nHLT_AK4PFJet50                      HLT_AK4PFJet80                      \nHLT_AK4PFJet100                     \nCaloMET_phi                         CaloMET_pt                          \nCaloMET_sumEt                       ChsMET_phi                          \nChsMET_pt                           ChsMET_sumEt                        \nnCorrT1METJet                       CorrT1METJet_area                   \nCorrT1METJet_eta                    CorrT1METJet_muonSubtrFactor        \nCorrT1METJet_phi                    CorrT1METJet_rawPt                  \nDeepMETResolutionTune_phi           DeepMETResolutionTune_pt            \nDeepMETResponseTune_phi             DeepMETResponseTune_pt              \nGenMET_phi                          GenMET_pt                           \nMET_MetUnclustEnUpDeltaX            MET_MetUnclustEnUpDeltaY            \nMET_covXX                           MET_covXY                           \nMET_covYY                           MET_phi                             MET_pt                              \nMET_significance                    MET_sumEt                           \nMET_sumPtUnclustered                PuppiMET_phi                        \nPuppiMET_phiJERDown                 PuppiMET_phiJERUp                   \nPuppiMET_phiJESDown                 PuppiMET_phiJESUp                   \nPuppiMET_phiUnclusteredDown         PuppiMET_phiUnclusteredUp           \nPuppiMET_pt                         PuppiMET_ptJERDown                  \nPuppiMET_ptJERUp                    PuppiMET_ptJESDown                  \nPuppiMET_ptJESUp                    PuppiMET_ptUnclusteredDown          \nPuppiMET_ptUnclusteredUp            PuppiMET_sumEt                      \nRawMET_phi                          RawMET_pt                           \nRawMET_sumEt                        RawPuppiMET_phi                     \nRawPuppiMET_pt                      RawPuppiMET_sumEt                   \nTkMET_phi                           TkMET_pt                            \nTkMET_sumEt                         MET_fiducialGenPhi                  \nMET_fiducialGenPt                   Flag_METFilters                     \nHLT_HT250_CaloMET70                 HLT_Mu16_eta2p1_MET30               \nHLT_IsoMu16_eta2p1_MET30            \nHLT_IsoMu16_eta2p1_MET30_LooseIsoPFTau50_Trk30_eta2p1 \nHLT_LooseIsoPFTau50_Trk30_eta2p1_MET80 HLT_LooseIsoPFTau50_Trk30_eta2p1_MET90 \nHLT_LooseIsoPFTau50_Trk30_eta2p1_MET110 HLT_LooseIsoPFTau50_Trk30_eta2p1_MET120 \nHLT_PFHT300_PFMET100                HLT_PFHT300_PFMET110                \nHLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu140 \nHLT_DiPFJet40_DEta3p5_MJJ600_PFMETNoMu80 HLT_MET60_IsoTrk35_Loose            \nHLT_MET75_IsoTrk50                  HLT_MET90_IsoTrk50                  \nHLT_PFMET120_BTagCSV_p067           HLT_PFMET120_Mu5                    \nHLT_PFMET170_NotCleaned             HLT_PFMET170_NoiseCleaned           \nHLT_PFMET170_HBHECleaned            HLT_PFMET170_JetIdCleaned           \nHLT_PFMET170_BeamHaloCleaned        HLT_PFMET170_HBHE_BeamHaloCleaned   \nHLT_PFMETTypeOne190_HBHE_BeamHaloCleaned HLT_PFMET90_PFMHT90_IDTight         \nHLT_PFMET100_PFMHT100_IDTight       \nHLT_PFMET100_PFMHT100_IDTight_BeamHaloCleaned HLT_PFMET110_PFMHT110_IDTight       \nHLT_PFMET120_PFMHT120_IDTight       \nHLT_CaloMHTNoPU90_PFMET90_PFMHT90_IDTight_BTagCSV_p067 \nHLT_CaloMHTNoPU90_PFMET90_PFMHT90_IDTight HLT_Photon135_PFMET100              \nHLT_Photon22_R9Id90_HE10_Iso40_EBOnly_PFMET40 \nHLT_Photon36_R9Id90_HE10_Iso40_EBOnly_PFMET40 \nHLT_Photon50_R9Id90_HE10_Iso40_EBOnly_PFMET40 \nHLT_Photon75_R9Id90_HE10_Iso40_EBOnly_PFMET40 \nHLT_Photon90_R9Id90_HE10_Iso40_EBOnly_PFMET40 \nHLT_Photon120_R9Id90_HE10_Iso40_EBOnly_PFMET40 HLT_Mu3er_PFHT140_PFMET125          \nHLT_Mu6_PFHT200_PFMET80_BTagCSV_p067 HLT_Mu6_PFHT200_PFMET100            \nHLT_Mu14er_PFMET100                 HLT_PFMETNoMu90_PFMHTNoMu90_IDTight \nHLT_PFMETNoMu100_PFMHTNoMu100_IDTight HLT_PFMETNoMu110_PFMHTNoMu110_IDTight \nHLT_PFMETNoMu120_PFMHTNoMu120_IDTight \nHLT_MonoCentralPFJet80_PFMETNoMu90_PFMHTNoMu90_IDTight \nHLT_MonoCentralPFJet80_PFMETNoMu100_PFMHTNoMu100_IDTight \nHLT_MonoCentralPFJet80_PFMETNoMu110_PFMHTNoMu110_IDTight \nHLT_MonoCentralPFJet80_PFMETNoMu120_PFMHTNoMu120_IDTight HLT_DoubleMu3_PFMET50               \nHLT_Ele15_IsoVVVL_PFHT350_PFMET50   HLT_Ele15_IsoVVVL_PFHT400_PFMET50   \nHLT_Mu8_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT300_PFMETNoMu60 \nHLT_Mu10_TrkIsoVVL_DiPFJet40_DEta3p5_MJJ750_HTT350_PFMETNoMu60 \nHLT_Mu15_IsoVVVL_PFHT350_PFMET50    HLT_Mu15_IsoVVVL_PFHT400_PFMET50    \nHLT_MET100                          HLT_MET150                          \nHLT_MET200                          HLT_MET250                          \nHLT_MET300                          HLT_MET600                          \nHLT_MET700                          HLT_PFMET300                        \nHLT_PFMET400                        HLT_PFMET500                        \nHLT_PFMET600\n</code></pre> <pre><code>if \"genWeight\" in events.keys():\n    gw = events[\"genWeight\"].array(entry_stop=100_000, library=\"np\")\n    print(f\"genWeight: mean={gw.mean():.3f}, std={gw.std():.3f}, negativos={(gw&lt;0).mean():.2%}\")\nelse:\n    print(\"There is no genWeight\")\n\n</code></pre> <pre><code>genWeight: mean=300.828, std=39.097, negativos=0.42%\n</code></pre> <p>Because std &gt; 0 and negativos &gt; 0, you have learned a golden rule for this dataset:</p> <p>NEVER count events.</p> <p>Wrong: N = len(events)</p> <p>Right: N = sum(events.genWeight)</p> <p>If you just count them (len), you will be wrong by 0.84% (counting the negatives as positives instead of subtracting them).</p> <p>\u201cNow the goal is to explore the characteristic variables of these events and visualize them. We are not applying any cuts yet; this step is simply to inspect the raw distributions. For this, we will use the following:\u201d</p> <p>Note:</p> <p>Notice that we could reuse the dictionary built earlier (<code>fileset</code>) to automatically loop over all available datasets. However, for debugging and flexibility, a manual list of datasets was defined. At the beginning, only a few datasets were used for quick tests, and later this list was expanded to match the same datasets contained in the <code>fileset</code>. This approach allows easier control during development while keeping compatibility with the full processing workflow.</p> <pre><code>#import uproot\n#import awkward as ak\n#import numpy as np\n#import matplotlib.pyplot as plt\n\n\nselected_datasets = [\n    'SingleMuon', 'SingleElectron', \n    'ttbar-semileptonic', 't-channel-top', 'ttW', \n    'WJets-HT400to600', 'DYJets-Zpt200', \n    'WW', 'ZZ', 'Zvv'\n]\n\nvariables = [\n    \"Muon_pt\", \"Muon_eta\",\n    \"Jet_pt\", \"Jet_btagDeepFlavB\", \n    \"MET_pt\",\n    \"nJet\", \"nMuon\", \"nElectron\"\n]\n\n\nmax_files = 1\n\n\ndata = {} \n\n\nfor ds_name in selected_datasets:\n\n    #################################3\n\n    if ds_name not in fileset:\n        print(f\"Skipping {ds_name}: Not found in fileset\")\n        continue\n     ####################################   \n\n\n    file_list = fileset[ds_name][\"files\"][:max_files]\n\n    print(f\"Reading {ds_name}: {len(file_list)} files...\")\n\n    # We need a temporary place to hold data chunks from multiple files\n    # Structure: temp_storage[\"Muon_pt\"] = [ [array_file1], [array_file2] ]\n    temp_storage = {var: [] for var in variables}\n\n    # Loop over the selected files\n    for file_path in file_list:\n        try:\n            with uproot.open(f\"{file_path}:Events\") as f:\n\n                # Read raw data\n                raw_data = f.arrays(variables, library=\"ak\")\n\n                for var in variables:\n                    # Flatten the data for this specific file\n                    flat_array = ak.to_numpy(ak.flatten(raw_data[var], axis=None))\n\n                    # Add to our temporary list\n                    temp_storage[var].append(flat_array)\n\n        except Exception as e:\n            print(f\"  Error reading file in {ds_name}: {e}\")\n\n\n    data[ds_name] = {}\n\n    for var in variables:\n        if len(temp_storage[var]) &gt; 0:\n            # np.concatenate joins the list of arrays into one big array\n            data[ds_name][var] = np.concatenate(temp_storage[var])\n        else:\n            data[ds_name][var] = np.array([]) # Empty if something went wrong\n\n\n</code></pre> <pre><code>Reading SingleMuon: 1 files...\nReading SingleElectron: 1 files...\nReading ttbar-semileptonic: 1 files...\nReading t-channel-top: 1 files...\nReading ttW: 1 files...\nReading WJets-HT400to600: 1 files...\nReading DYJets-Zpt200: 1 files...\nReading WW: 1 files...\nReading ZZ: 1 files...\nReading Zvv: 1 files...\n</code></pre> <pre><code>plt.rcParams.update({'font.size': 12})\n\nfor var in variables:\n\n    plt.figure(figsize=(8, 6))\n\n    # Loop through the data we just loaded\n    for ds_name in selected_datasets:\n\n        # Check if we have data for this combination\n        if ds_name in data and var in data[ds_name]:\n\n            values = data[ds_name][var]\n\n            # Check if empty\n            if len(values) == 0: continue\n\n\n\n       ########################################################     \n\n\n            plt.hist(\n                values, \n                bins=50, \n                # Automatic range: cut off the top 1% outliers so plot looks good\n                range=(0, np.percentile(values, 99)), \n                histtype='bar',   \n                linewidth=2,      \n                label=ds_name      # Name in legend\n            )\n\n    plt.xlabel(var)\n    plt.ylabel(\"Events\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>\u201cBecause data and MC have very different event counts, we cannot compare their raw histograms directly. For now, we normalize each distribution so that its integral equals 1. This lets us compare the shape of the variables without worrying about absolute yields:\u201d</p> <pre><code># --- NORMALIZATION ---\n# We create weights so the sum of the histogram equals 1.\n# This allows us to compare SHAPES, not counts.\nweights = np.ones_like(vals) / len(vals)\n</code></pre> <p>Then copy the previous block and add this line, and in plt.his add weights</p> <pre><code>\n\nplt.rcParams.update({'font.size': 12})\n\nfor var in variables:\n\n    plt.figure(figsize=(8, 6))\n\n    for ds_name in selected_datasets:\n\n        # Check for data\n        if ds_name in data and var in data[ds_name]:\n\n            vals = data[ds_name][var]\n            if len(vals) == 0: continue\n\n            # --- NORMALIZATION STEP ---\n            # Create an array of weights where every entry is (1 / total_events)\n            # When summed up by the histogram, the total area will be 1.0\n            weights = np.ones_like(vals) / len(vals)\n\n            # --- PLOT ---\n            plt.hist(\n                vals, \n                bins=50, \n                range=(0, np.percentile(vals, 99)), \n                weights=weights,   # APPLY WEIGHTS HERE\n                histtype='bar',   \n                linewidth=2,       \n                label=ds_name      \n            )\n\n    # Labels\n    plt.xlabel(var)\n    plt.ylabel(\"Fraction of Events (Normalized)\") # Changed label\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>The workflow proceeds in three main steps:</p> <ol> <li> <p>Apply the baseline selection    We first select events that match the single-lepton topology (electron or muon).    This removes events that clearly do not belong to the channel and defines the core event structure we will work with.</p> </li> <li> <p>Filter the events    After the baseline, we keep only the events that satisfy these basic criteria.    This produces a clean and well-defined sample for further analysis.</p> </li> <li> <p>Apply additional cuts to build CR and SR    With the filtered sample, we introduce extra requirements to define:</p> </li> <li> <p>Control Regions (CR): used to study and validate the dominant backgrounds.</p> </li> <li>Signal Region (SR): where the dark-matter search is performed.</li> </ol> <p>This step-by-step structure mirrors the logic used in the CMS paper and ensures a transparent and reproducible analysis flow.</p> <p>So all of this is reflected in the tables, which are directly the cuts we should use,  and we don't have to explore it ourselves in this case for this guide.</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#base-line","title":"Base line","text":"<p>If you read the drafts</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#table-10-baseline-selection-sl-and-ah","title":"Table 10 \u2014 Baseline Selection (SL and AH)","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#what-does-it-contain","title":"What does it contain?","text":"<p>The minimum or baseline selection that every event must satisfy before being assigned to any region (Signal or Control).</p> <p>It includes:</p> <ul> <li>Lepton quality and type (muons, electrons)</li> <li>Minimum cuts on pT and \u03b7</li> <li>Requirements on jets and b-jets</li> <li>Event-cleaning criteria</li> <li>Additional-lepton veto</li> </ul>"},{"location":"Single_Lepton/Analisis_muon_final_version/#what-is-it-for","title":"What is it for?","text":"<p>It acts as the universal first filter of the analysis. No event enters the SR or CR without passing this baseline.</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#tables-11-and-12-expected-events-after-optimized-cuts-sl-and-ah","title":"Tables 11 and 12 \u2014 Expected Events After Optimized Cuts (SL and AH)","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#what-do-they-contain","title":"What do they contain?","text":"<p>Tables showing:</p> <ul> <li>Expected event yields for each background</li> <li>Predictions for different signal models</li> <li>Results after the baseline plus optimized cuts</li> </ul>"},{"location":"Single_Lepton/Analisis_muon_final_version/#what-is-it-for_1","title":"What is it for?","text":"<p>These tables serve only for internal validation in the paper:</p> <ul> <li>Checking consistency between simulated backgrounds and signals</li> <li>Showing how many events remain under different signal hypotheses</li> </ul> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(filename=\"Table_10.png\"))\n\n</code></pre> <p></p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#single-muon","title":"Single Muon","text":"<p>1.\u201cLoad all physics objects (muons, electrons, jets, MET) from the ROOT file and package them as 4-vectors. This prepares the event for the physics selections that follow.\u201d</p> <p>2.\u201cApply the baseline selection: require exactly one tight muon, veto additional leptons, and apply the minimal MET requirement. This defines the core single-lepton topology before any SR or CR classification.\u201d</p> <p>3.\u201cClean all jets (quality, pT, \u0394R from the muon) and classify them as central or forward. Retain only events with at least two central jets, as required by the topology used in the analysis.\u201d</p> <p>4.Compute the physics variables used throughout the analysis: W transverse mass, jet multiplicities, b-tag multiplicity, min\u0394\u03c6(jets,MET), forward-jet count, and mT(b). These variables are the basis for the SR/CR definitions and the later shape studies.</p> <p>5.Package all selected events and computed variables into NumPy arrays. This standardized structure makes it easy to plot distributions, apply further event selection, or feed the results into statistical tools.</p> <pre><code>#import awkward as ak\n#import uproot\n#import numpy as np\n#colorsimport vector\nvector.register_awkward()\n\n#  1.  ###################################\n\ndef process_file_muon(filename, dataset=\"Unknown\", IS_DATA=False):\n\n    try:\n        with uproot.open(f\"{filename}:Events\") as tree:\n            muons = ak.zip({\n                \"pt\": tree[\"Muon_pt\"].array(),\n                \"eta\": tree[\"Muon_eta\"].array(),\n                \"phi\": tree[\"Muon_phi\"].array(),\n                \"mass\": tree[\"Muon_mass\"].array(),\n                \"iso\": tree[\"Muon_pfRelIso04_all\"].array(),\n                \"tightId\": tree[\"Muon_tightId\"].array(),\n                \"looseId\": tree[\"Muon_looseId\"].array(),\n            }, with_name=\"Momentum4D\")\n\n            electrons = ak.zip({\n                \"pt\": tree[\"Electron_pt\"].array(),\n                \"eta\": tree[\"Electron_eta\"].array(),\n                \"phi\": tree[\"Electron_phi\"].array(),\n                \"mass\": tree[\"Electron_mass\"].array(),\n                \"cutBased\": tree[\"Electron_cutBased\"].array(), \n            }, with_name=\"Momentum4D\")\n\n            jets = ak.zip({\n                \"pt\": tree[\"Jet_pt\"].array(),\n                \"eta\": tree[\"Jet_eta\"].array(),\n                \"phi\": tree[\"Jet_phi\"].array(),\n                \"mass\": tree[\"Jet_mass\"].array(),\n                \"jetId\": tree[\"Jet_jetId\"].array(),\n                \"btag\": tree[\"Jet_btagDeepFlavB\"].array(),\n            }, with_name=\"Momentum4D\")\n\n            met_pt = tree[\"MET_pt\"].array()\n            met_phi = tree[\"MET_phi\"].array()\n\n            if not IS_DATA:\n                gen_weight = tree[\"genWeight\"].array()\n            else:\n                gen_weight = ak.ones_like(met_pt)\n\n##############################################################################\n\n        # --- 2. SELECCI\u00d3N BASELINE ---\n\n\n        mu_tight_mask = (muons.pt &gt; 30) &amp; (abs(muons.eta) &lt; 2.4) &amp; (muons.tightId) &amp; (muons.iso &lt; 0.15)\n        mu_loose_mask = (muons.pt &gt; 10) &amp; (abs(muons.eta) &lt; 2.4) &amp; (muons.looseId) &amp; (muons.iso &lt; 0.25)\n        ele_veto_mask = (electrons.pt &gt; 10) &amp; (abs(electrons.eta) &lt; 2.5) &amp; (electrons.cutBased &gt;= 1)\n\n        good_muons = muons[mu_tight_mask]\n        veto_muons = muons[mu_loose_mask]\n        veto_electrons = electrons[ele_veto_mask]\n\n        has_1_tight_mu = (ak.num(good_muons) == 1)\n        veto_extra_lep = (ak.num(veto_muons) == 1) &amp; (ak.num(veto_electrons) == 0)\n        pass_met = met_pt &gt; 150 \n\n        event_mask = has_1_tight_mu &amp; veto_extra_lep &amp; pass_met\n\n        # Filtrado inicial\n        good_muons = good_muons[event_mask]\n        jets = jets[event_mask]\n        met_pt = met_pt[event_mask]\n        met_phi = met_phi[event_mask]\n        gen_weight = gen_weight[event_mask]\n\n        leading_mu = good_muons[:, 0]\n\n\n        # 3. ###########################################################3\n        jet_clean_mask = (\n            (jets.pt &gt; 30) &amp; \n            (jets.jetId &gt;= 1) &amp; \n            (jets.deltaR(leading_mu) &gt; 0.4)\n        )\n        all_clean_jets = jets[jet_clean_mask]\n\n        # B. Separar Central vs Forward (NUEVO)\n        central_jets = all_clean_jets[abs(all_clean_jets.eta) &lt; 2.4]\n        forward_jets = all_clean_jets[(abs(all_clean_jets.eta) &gt;= 2.4) &amp; (abs(all_clean_jets.eta) &lt; 5.0)]\n\n        has_2_jets = ak.num(central_jets) &gt;= 2\n\n        .###################################33###############################3\n\n        # --- 4. ################################################################3\n        f_met = met_pt[has_2_jets]\n        f_met_phi = met_phi[has_2_jets]\n        f_mu = leading_mu[has_2_jets]\n        f_central = central_jets[has_2_jets]\n        f_forward = forward_jets[has_2_jets]\n        f_gen_weight = gen_weight[has_2_jets]\n\n        # mT_W\n        dphi_lep = f_mu.phi - f_met_phi\n        mt_w = np.sqrt(2 * f_mu.pt * f_met * (1 - np.cos(dphi_lep)))\n\n        # min DeltaPhi\n        jets_top2 = f_central[:, :2]\n        dphi_jets = abs(jets_top2.phi - f_met_phi)\n        dphi_jets = np.mod(dphi_jets + np.pi, 2*np.pi) - np.pi\n        min_dphi = ak.min(abs(dphi_jets), axis=1)\n\n        # B-tagging\n        is_btag = f_central.btag &gt; 0.2770\n        n_btag = ak.sum(is_btag, axis=1)\n\n\n        # 1. Conteo Forward Jets\n        n_fwd = ak.num(f_forward)\n\n        # 2. mT_b (Masa Transversa MET - Leading b-jet)\n        b_jets = f_central[is_btag]\n        has_b = ak.num(b_jets) &gt; 0\n\n        # Padding seguro\n        b_jet_padded = ak.fill_none(ak.firsts(b_jets), {\"pt\": 0, \"phi\": 0}, axis=0)\n        dphi_b = b_jet_padded.phi - f_met_phi\n        calc_mt_b = np.sqrt(2 * b_jet_padded.pt * f_met * (1 - np.cos(dphi_b)))\n\n        mt_b_np = ak.to_numpy(calc_mt_b)\n        has_b_np = ak.to_numpy(has_b)\n        mt_b_np[~has_b_np] = -1.0 # Valor dummy\n\n        ###############################################################################\n\n######## 5 . #############################################################################\n\n        return {\n            \"dataset\": dataset,\n            \"lep_pt\": ak.to_numpy(f_mu.pt),\n            \"lep_eta\": ak.to_numpy(f_mu.eta),\n            \"met\": ak.to_numpy(f_met),\n            \"mT_W\": ak.to_numpy(mt_w),\n            \"genWeight\": ak.to_numpy(f_gen_weight),\n            \"nJet\": ak.to_numpy(ak.num(f_central)),\n            \"nBTag\": ak.to_numpy(n_btag),\n            \"min_dphi\": ak.to_numpy(min_dphi),\n\n            ## to use the processed datasets later \n            \"nForwardJets\": ak.to_numpy(n_fwd),\n            \"mT_b\": mt_b_np\n        }\n\n    except Exception as e:\n        print(f\"Error procesando {filename}: {e}\")\n        return None\n</code></pre> <p>BLOCK 1 \u2014 Dataset list (Muon Channel only)</p> <p>This block defines the list of datasets that will be processed with the muon selection logic.</p> <p>SingleMuon is data.</p> <p>Everything else is MC background. This allows the orchestrator to automatically detect whether generator weights exist.</p> <p>BLOCK 2 \u2014 Orchestrator function</p> <p>This function:</p> <p>Determines whether the dataset is Data or MC.</p> <p>Checks that the dataset exists in the global fileset.</p> <p>Loops over the first n_files files of the dataset.</p> <p>For each file:</p> <p>Calls process_file_muon() (the physics function you wrote earlier).</p> <p>Converts the returned dict into a pandas.DataFrame.</p> <p>Merges all partial DataFrames.</p> <p>Saves the result to a Parquet file under output_raw/.</p> <p>Returns the full DataFrame.</p> <p>This function does not perform physics. Its job is to:</p> <p>--loop</p> <p>--collect</p> <p>--assemble</p> <p>--save</p> <p>It is the \u201cmanager\u201d layer.</p> <p>BLOCK 3 \u2014 Execution loop</p> <p>This simply iterates over all muon-channel datasets and processes each one with a chosen number of input ROOT files per dataset.</p> <pre><code>#import pandas as pd\n#import os\n#import awkward as ak\n#import uproot\n#import numpy as np\n#import vector\n\nvector.register_awkward()\n\n# BLOCK 1 \u2014 DATASET LIST (Muon Channel Only)\n\n\ndatasets_muon_channel = [\n    'SingleMuon',           # DATA\n    'ttbar-semileptonic',   # Main background\n    'ttW', 'WW', 'ZZ', 'Zvv',\n    'DYJets-Zpt200',\n    't-channel-top',\n    'WJets-HT400to600'\n]\n\n\n# BLOCK 2 \u2014 ORCHESTRATOR FUNCTION (Muon Analysis Logic)\n\ndef process_dataset_muon_raw(dataset, n_files=5):\n    \"\"\"\n    High-level orchestrator:\n    - Determines if dataset is Data or MC\n    - Retrieves ROOT files\n    - Calls the muon-physics function (process_file_muon)\n    - Converts results to DataFrames\n    - Concatenates them and saves as a Parquet file\n    \"\"\"\n\n    # Check if dataset is Data (only SingleMuon)\n    is_data = \"SingleMuon\" in dataset\n\n    print(f\" Processing RAW {dataset} (Is Data: {is_data})...\")\n\n    # Verify that dataset exists in the global fileset\n    if dataset not in fileset:\n        print(f\" {dataset} not found in fileset. Skipping.\")\n        return None\n\n    # Select only the first n_files\n    files = fileset[dataset][\"files\"][:n_files]\n    dfs = []\n\n    for f in files:\n        try:\n            # Call your physics selection function\n            data_dict = process_file_muon(f, dataset=dataset, IS_DATA=is_data)\n\n            # Turn dict into DataFrame\n            df = pd.DataFrame(data_dict)\n            dfs.append(df)\n\n        except Exception as e:\n            # A single bad file should not stop the batch\n            print(f\" Error in file {f}: {e}\")\n\n    # If no results were generated\n    if len(dfs) == 0:\n        print(f\" No valid events produced for {dataset}\")\n        return None\n\n    # Merge all dataframes\n    full_df = pd.concat(dfs, ignore_index=True)\n\n    # Save as parquet in output_raw/\n    os.makedirs(\"output_raw\", exist_ok=True)\n    output_path = f\"output_raw/{dataset}_raw.parquet\"\n\n    full_df.to_parquet(output_path, index=False)\n    print(f\" Saved: {output_path} with {len(full_df)} events.\")\n\n    return full_df\n\n\n# BLOCK 3 \u2014 EXECUTION LOOP\n\n\nN_FILES = 10  # Increase to 20+ for proper statistics\n\nprint(f\"=== STARTING MUON PROCESSING ({N_FILES} files per dataset) ===\")\n\nfor ds in datasets_muon_channel:\n    process_dataset_muon_raw(ds, n_files=N_FILES)\n\n</code></pre> <pre><code>=== STARTING MUON PROCESSING (10 files per dataset) ===\n Processing RAW SingleMuon (Is Data: True)...\n Saved: output_raw/SingleMuon_raw.parquet with 10832 events.\n Processing RAW ttbar-semileptonic (Is Data: False)...\n Saved: output_raw/ttbar-semileptonic_raw.parquet with 108389 events.\n Processing RAW ttW (Is Data: False)...\n Saved: output_raw/ttW_raw.parquet with 85898 events.\n Processing RAW WW (Is Data: False)...\n Saved: output_raw/WW_raw.parquet with 8476 events.\n Processing RAW ZZ (Is Data: False)...\n Saved: output_raw/ZZ_raw.parquet with 100 events.\n Processing RAW Zvv (Is Data: False)...\n Saved: output_raw/Zvv_raw.parquet with 4 events.\n Processing RAW DYJets-Zpt200 (Is Data: False)...\n Saved: output_raw/DYJets-Zpt200_raw.parquet with 2240 events.\n Processing RAW t-channel-top (Is Data: False)...\n Saved: output_raw/t-channel-top_raw.parquet with 6811 events.\n Processing RAW WJets-HT400to600 (Is Data: False)...\n Saved: output_raw/WJets-HT400to600_raw.parquet with 49574 events.\n</code></pre>"},{"location":"Single_Lepton/Analisis_muon_final_version/#why-normalization-is-needed","title":"\u2014 Why Normalization Is Needed","text":"<p>This section introduces the central problem: data and MC cannot be compared directly because:</p> <ul> <li>Data reflects what the detector actually recorded.</li> <li>MC is generated with arbitrary numbers of events and must be rescaled.</li> </ul> <p>Your introduction sets the conceptual foundation: MC must be weighted so that its event yields match the luminosity of the data sample.</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#what-this-enables","title":"What this enables","text":"<p>The next blocks will:</p> <ul> <li>Extract necessary metadata</li> <li>Compute normalization weights</li> <li>Apply them when building histograms</li> </ul> <p>Without this conceptual block, the following code would feel unmotivated.</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#extracting-total-generated-weights-sumgenweights","title":"Extracting Total Generated Weights (<code>sumGenWeights</code>)","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#explanation","title":"Explanation","text":"<p>To compute the MC weight</p> \\[ w = \\frac{\\sigma \\cdot L}{N_{\\text{gen}}} \\] <p>we need the denominator: the number of generated events, usually stored as sum of generated event weights:</p> <pre><code>Runs/genEventSumw\n</code></pre> <p>This block reads that information efficiently from the <code>Runs</code> tree in each file.</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#what-this-block-does-for-normalization","title":"What this block does for normalization","text":"<ul> <li>Provides N_gen \u2261 sumGenWeights</li> <li>Without it, normalization would be impossible</li> <li>For data, you correctly set the weight to 1 (data is never scaled)</li> </ul> <pre><code>#import uproot\n#import numpy as np\n\n\nsum_weights_map = {}\n\n\nprint(f\"{'Dataset':&lt;30} | {'SumW (Runs)':&lt;20}\")\nprint(\"-\" * 60)\n\nfor dataset_name, info in fileset.items():\n\n    # Identify data (data has no generator weights)\n    is_data = any(x in dataset_name for x in [\"SingleMuon\", \"SingleElectron\", \"MET\", \"EGamma\"])\n\n    if is_data:\n        sum_weights_map[dataset_name] = 1.0\n        print(f\"{dataset_name:&lt;30} | {'1.0 (DATA)':&lt;20}\")\n        continue\n\n    total_sum_w = 0.0\n    file_list = info[\"files\"]\n\n    for filename in file_list:\n        try:\n            with uproot.open(f\"{filename}:Runs\") as runs:\n                if \"genEventSumw\" in runs:\n                    w = runs[\"genEventSumw\"].array(library=\"np\")\n                    total_sum_w += np.sum(w)\n        except Exception as e:\n            print(f\" Error reading {filename}: {e}\")\n\n    sum_weights_map[dataset_name] = total_sum_w\n    print(f\"{dataset_name:&lt;30} | {total_sum_w:.2e}\")\n\nprint(\"\\n Normalization metadata loaded.\")\n\n</code></pre> <pre><code>Dataset                        | SumW (Runs)         \n------------------------------------------------------------\nmet                            | 0.00e+00\nSingleMuon                     | 1.0 (DATA)          \nSingleElectron                 | 1.0 (DATA)          \nttbar-semileptonic             | 4.35e+10\nttbar-hadronic                 | 3.36e+10\nt-channel-top                  | 2.30e+09\nttW                            | 1.11e+06\nWJets-HT400to600               | 2.12e+06\nDYJets-Zpt200                  | 7.51e+03\nWW                             | 1.58e+07\nZZ                             | 1.15e+06\nZvv                            | 6.47e+04\n\n Normalization metadata loaded.\n</code></pre>"},{"location":"Single_Lepton/Analisis_muon_final_version/#defining-the-physics-metadata-cross-sections-classification","title":"Defining the Physics Metadata (Cross Sections + Classification)","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#explanation_1","title":"Explanation","text":"<p>This block assigns:</p> <ul> <li>Cross sections (\u03c3)</li> <li>Dataset category (ttbar, WJets, etc.)</li> <li>Plotting style (colors)</li> </ul> <p>This is where you get the numerator of the weight formula:</p> <p>[ \\sigma \\cdot L ]</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#how-it-connects-to-normalization","title":"How it connects to normalization","text":"<p>You now have:</p> <ul> <li>\u03c3 (from <code>fileset[ds][\"metadata\"]</code>)</li> <li>L (your chosen <code>LUM</code>)</li> <li>N_gen (from previous block)</li> </ul> <p>\u2192 Everything needed to compute per-event scale factors.</p> <pre><code>import pandas as pd\n\nLUM = 2590.0  # pb^-1\n\nDATA_SL = {\"SingleMuon\", \"SingleElectron\"}\n\nCOLOR_MAP = {\n    \"ttbar\":     \"#FF9933\", \n    \"WJets\":     \"#33CC33\", \n    \"ZJets\":     \"#3399FF\", \n    \"SingleTop\": \"#CC33CC\", \n    \"Diboson\":   \"#FFCC00\", \n    \"Rare\":      \"#FF0000\", \n    \"Other\":     \"#999999\"   \n}\n\ndef get_group_info(name):\n\n    #####################################################################################################\n    if \"ttW\" in name or \"ttZ\" in name: return \"Rare\", r\"$t\\bar{t}V$\", COLOR_MAP[\"Rare\"]\n    if \"ttbar\" in name: return \"ttbar\", r\"$t\\bar{t}$ Semileptonic\", COLOR_MAP[\"ttbar\"]\n    if \"channel\" in name or \"tW\" in name: return \"SingleTop\", r\"Single Top\", COLOR_MAP[\"SingleTop\"]\n    if \"WJets\" in name: return \"WJets\", r\"W+Jets\", COLOR_MAP[\"WJets\"]\n    if \"DY\" in name or \"Zvv\" in name: return \"ZJets\", r\"Z/$\\gamma^*$+Jets\", COLOR_MAP[\"ZJets\"]\n    if name in [\"WW\", \"ZZ\", \"WZ\"]: return \"Diboson\", r\"VV (Diboson)\", COLOR_MAP[\"Diboson\"]\n    return \"Other\", \"Other\", COLOR_MAP[\"Other\"]\n    ###############################################################################################\n\n\nprint(f\"{'Dataset':&lt;25} | {'Xsec [pb]':&lt;10} \"\n      f\"| {'SumGenWeights':&lt;15} | {'Scale Factor':&lt;12}\")\nprint(\"-\" * 70)\n\ndatasets_general_check = list(fileset.keys())\n\nfor ds in datasets_general_check:\n\n    # DATA\n    if ds in DATA_SL:\n        print(f\"{ds:&lt;25} | {'-':&lt;10} | {'-':&lt;15} | {'1.00':&lt;12}\")\n        continue\n\n    if ds not in fileset:\n        print(f\"{ds:&lt;25} | {'No fileset':&lt;10} | -\")\n        continue\n\n    xsec = fileset[ds][\"metadata\"][\"xsec\"]\n    sum_w = sum_weights_map.get(ds, 0.0)\n\n    if sum_w &gt; 0 and xsec is not None:\n        scale = (xsec * LUM) / sum_w\n        scale_str = f\"{scale:.2e}\"\n        if scale &gt; 10.0:\n            scale_str += \" HIGH\"\n        print(f\"{ds:&lt;25} | {xsec:&lt;10.2f} | {sum_w:&lt;15.2e} | {scale_str:&lt;12}\")\n\n    else:\n        reason = \"SumW=0\" if sum_w == 0 else \"Xsec=None\"\n        print(f\"{ds:&lt;25} | {str(xsec):&lt;10} | {sum_w:&lt;15.2e} | ERROR ({reason})\")\n\n</code></pre> <pre><code>Dataset                   | Xsec [pb]  | SumGenWeights   | Scale Factor\n----------------------------------------------------------------------\nmet                       | None       | 0.00e+00        | ERROR (SumW=0)\nSingleMuon                | -          | -               | 1.00        \nSingleElectron            | -          | -               | 1.00        \nttbar-semileptonic        | 364.35     | 4.35e+10        | 2.17e-05    \nttbar-hadronic            | 377.96     | 3.36e+10        | 2.91e-05    \nt-channel-top             | 136.02     | 2.30e+09        | 1.53e-04    \nttW                       | 0.20       | 1.11e+06        | 4.75e-04    \nWJets-HT400to600          | 48.91      | 2.12e+06        | 5.99e-02    \nDYJets-Zpt200             | 1.27       | 7.51e+03        | 4.38e-01    \nWW                        | 118.70     | 1.58e+07        | 1.94e-02    \nZZ                        | 16.60      | 1.15e+06        | 3.74e-02    \nZvv                       | 77.30      | 6.47e+04        | 3.09e+00\n</code></pre> <p>Inventory: We define exactly which datasets we want to mix.</p> <p>Loop &amp; Load: We iterate through the list, read the processed Parquet files (which are much faster than ROOT files), and load them into RAM.</p> <p>Normalization: We apply the physics weights immediately so every dataframe in all_dfs is ready to be plotted.</p> <p>Visualization: We trigger the plots right at the end.</p> <p>To produce publication-quality plots\u2014like those found in CMS analyses\u2014we need a function that:</p> <ol> <li>Builds histograms for each MC process, applying the final per-event normalization weight.</li> <li>Groups MC samples by physics category (tt\u0304, W+jets, Single Top, Diboson, etc.).</li> <li>Stacks the MC contributions in the correct order (smallest on top) so that the final shape reflects the Standard Model expectation.</li> <li>Overlays the real DATA histogram with error bars.</li> <li>Applies CMS styling, including the \u201cPreliminary\u201d tag and integrated luminosity.</li> <li>Ensures the plot is clean, readable, and suitable for analysis notes or a professional report.</li> </ol> <p>This function does exactly that. By defining it once, we ensure that every observable (pT, eta, MET, jets, etc.) is plotted consistently and in the same professional format used in CMS.</p> <pre><code>import mplhep as hep\nimport matplotlib.pyplot as plt\nplt.style.use(hep.style.CMS)\n\n\ndef plot_grouped_stack(var_name, x_label, x_range, channel_data=\"SingleMuon\",\n                       n_bins=30, log_scale=False):\n\n    # 1. Bines\n    bins = np.linspace(x_range[0], x_range[1], n_bins + 1)\n\n    grouped_counts = {}\n    grouped_info   = {}\n\n    # 2. AGRUPAR MC\n    for name, df in all_dfs.items():\n\n        # Ignore DATA here\n        if name in DATA_SL:\n            continue\n\n        group_key, label, color = get_group_info(name)\n\n        counts, _ = np.histogram(df[var_name], bins=bins, weights=df[\"final_weight\"])\n\n        if group_key not in grouped_counts:\n            grouped_counts[group_key] = counts\n            grouped_info[group_key]   = {\"label\": label, \"color\": color, \"yield\": np.sum(counts)}\n        else:\n            grouped_counts[group_key] += counts\n            grouped_info[group_key][\"yield\"] += np.sum(counts)\n\n    # 3. ORDENAR GRUPOS (por yield)\n    active_groups = list(grouped_info.keys())\n    active_groups.sort(key=lambda g: grouped_info[g][\"yield\"])  # small \u2192 top in stack\n\n    mc_counts = []\n    mc_colors = []\n    mc_labels = []\n    total_mc = np.zeros(n_bins)\n\n    for g in active_groups:\n        mc_counts.append(grouped_counts[g])\n        mc_colors.append(grouped_info[g][\"color\"])\n        mc_labels.append(grouped_info[g][\"label\"])\n        total_mc += grouped_counts[g]\n\n    # 4. DATA\n    df_data = all_dfs.get(channel_data)\n    if df_data is None:\n        print(f\"ERROR: No DATA found for channel {channel_data}\")\n        return\n\n    data_counts, _ = np.histogram(df_data[var_name], bins=bins)\n\n    # 5. PLOT\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    # --- MC STACK -----\n    if len(mc_counts) &gt; 0:\n        hep.histplot(\n            mc_counts,\n            bins=bins,\n            stack=True,\n            histtype=\"fill\",\n            color=mc_colors,\n            label=mc_labels,\n            edgecolor=\"black\",\n            linewidth=1,\n            ax=ax\n        )\n\n    # --- DATA -----\n    hep.histplot(\n        data_counts,\n        bins=bins,\n        histtype=\"errorbar\",\n        color=\"black\",\n        label=f\"{channel_data} (Data)\",\n        yerr=True,\n        marker=\"o\",\n        markersize=5,\n        ax=ax\n    )\n\n    # --- CMS Style -----\n    hep.cms.label(\"Preliminary\", data=True, lumi=2.6, year=2016, ax=ax)\n\n    # Leyenda ordenada para que DATA quede arriba\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(reversed(handles), reversed(labels), fontsize=16, ncol=2, loc=\"upper right\")\n\n    # Ejes\n    ax.set_xlabel(x_label, fontsize=24)\n    ax.set_ylabel(\"Events\", fontsize=24)\n    ax.set_xlim(x_range)\n\n    if log_scale:\n        ax.set_yscale(\"log\")\n        ax.set_ylim(0.1, max(np.max(data_counts), np.max(total_mc)) * 500)\n    else:\n        ax.set_ylim(0, max(np.max(data_counts), np.max(total_mc)) * 1.5)\n\n    plt.tight_layout()\n    plt.show()\n\n</code></pre> <p>This block loads every muon-channel dataset from the previously generated _raw.parquet files, applies the correct normalization (data vs. Monte Carlo), assigns a final per-event weight, and stores everything inside a unified dictionary (all_dfs). Once all datasets are normalized, we can safely call plot_grouped_stack to generate stacked MC vs. Data plots that follow a professional CMS-style presentation.</p> <pre><code>loaded_dfs = {}\n\n\n# Full list of expected Muon datasets\ndatasets_muon_channel = [\n    'SingleMuon',           # Data\n    'ttbar-semileptonic',   # Top\n    'ttW', 'WW', 'ZZ', 'Zvv', 'DYJets-Zpt200',\n    't-channel-top',\n    'WJets-HT400to600'      # WJets binned (GOOD)\n]\n\nfor dataset in datasets_muon_channel:\n\n    # Muon channel uses '_raw.parquet' (no electron prefix)\n    path = f\"output_raw/{dataset}_raw.parquet\"\n\n    try:\n        df = pd.read_parquet(path)\n\n        # Normalization\n        if dataset in DATA_SL:\n            df[\"final_weight\"] = 1.0\n        else:\n            xsec = fileset[dataset][\"metadata\"][\"xsec\"]\n            sum_w = sum_weights_map.get(dataset, 1.0)\n            if sum_w == 0:\n                sum_w = 1.0\n            scale = (xsec * LUM) / sum_w\n\n            # Use genWeight if available\n            if \"genWeight\" in df.columns:\n                df[\"final_weight\"] = df[\"genWeight\"] * scale\n            else:\n                df[\"final_weight\"] = scale\n\n        loaded_dfs[dataset] = df\n        print(f\" Loaded: {dataset}\")\n\n    except FileNotFoundError:\n        # Skip missing datasets silently\n        continue\n\n# Final dictionary with loaded Muon datasets\nmuon_dfs_clean = loaded_dfs\n\n# Update all_dfs globally\nall_dfs = muon_dfs_clean\n\n\n# -------------------------------------------\n# ---          CLEAN PLOTS               ---\n# -------------------------------------------\n\nif len(muon_dfs_clean) &gt; 0:\n    mi_canal = \"SingleMuon\"\n\n    plot_grouped_stack(\"mT_W\", r\"$m_T^W$ [GeV]\", (0, 200), mi_canal, log_scale=False)\n    plot_grouped_stack(\"met\", r\"$p_T^{miss}$ [GeV]\", (150, 500), mi_canal, log_scale=True)\n    plot_grouped_stack(\"nJet\", r\"$N_{jets}$\", (2, 10), mi_canal, n_bins=8, log_scale=True)\n    plot_grouped_stack(\"nBTag\", r\"$N_{b-tags}$\", (0, 5), mi_canal, n_bins=5, log_scale=True)\n\nelse:\n    print(\" No data loaded. Check if the _raw.parquet files exist.\")\n\n</code></pre> <pre><code> Loaded: SingleMuon\n Loaded: ttbar-semileptonic\n Loaded: ttW\n Loaded: WW\n Loaded: ZZ\n Loaded: Zvv\n Loaded: DYJets-Zpt200\n Loaded: t-channel-top\n Loaded: WJets-HT400to600\n</code></pre> <p></p> <p></p> <p></p> <p></p> <pre><code>muon_dfs_clean = loaded_dfs\nall_dfs = muon_dfs_clean\n\n# ----- remover WJets -----\nif \"WJets-HT400to600\" in muon_dfs_clean:\n    muon_dfs_clean.pop(\"WJets-HT400to600\")\n\n# ----- graficar -----\nif len(muon_dfs_clean) &gt; 0:\n    mi_canal = \"SingleMuon\"\n\n    plot_grouped_stack(\"mT_W\", r\"$m_T^W$ [GeV]\", (0, 200), mi_canal, log_scale=False)\n    plot_grouped_stack(\"met\", r\"$p_T^{miss}$ [GeV]\", (150, 500), mi_canal, log_scale=True)\n    plot_grouped_stack(\"nJet\", r\"$N_{jets}$\", (2, 10), mi_canal, n_bins=8, log_scale=True)\n    plot_grouped_stack(\"nBTag\", r\"$N_{b-tags}$\", (0, 5), mi_canal, n_bins=5, log_scale=True)\n\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#electron","title":"Electron","text":"<p>To adapt the muon channel for the electron channel, we switch the signal object from <code>Muon</code> to <code>Electron</code> and adjust the kinematic cuts: pseudorapidity is restricted to \\(|\\eta| &lt; 2.1\\) (instead of 2.4), and a \"Gap Veto\" is implemented to exclude electrons in the barrel-endcap transition region (\\(1.4442 &lt; |\\eta| &lt; 1.566\\)). Furthermore, the identification criteria change from specific <code>tightId</code> and isolation cuts to the standard <code>cutBased == 4</code> (Tight ID), and Jet Cleaning is re-evaluated by requiring \\(\\Delta R &gt; 0.4\\) with respect to the selected electron rather than the muon.</p> <pre><code>#import awkward as ak\n#import uproot\n#import numpy as np\n#import vector\n#vector.register_awkward()\n\n\n\ndef process_electron(filename, dataset=\"Unknown\", IS_DATA=False):\n\n        with uproot.open(f\"{filename}:Events\") as tree:\n            electrons = ak.zip({\n                \"pt\": tree[\"Electron_pt\"].array(),\n                \"eta\": tree[\"Electron_eta\"].array(),\n                \"phi\": tree[\"Electron_phi\"].array(),\n                \"mass\": tree[\"Electron_mass\"].array(),\n                \"cutBased\": tree[\"Electron_cutBased\"].array(),\n            }, with_name=\"Momentum4D\")\n\n            muons = ak.zip({\n                \"pt\": tree[\"Muon_pt\"].array(),\n                \"eta\": tree[\"Muon_eta\"].array(),\n                \"phi\": tree[\"Muon_phi\"].array(),\n                \"mass\": tree[\"Muon_mass\"].array(),\n                \"looseId\": tree[\"Muon_looseId\"].array(),\n                \"iso\": tree[\"Muon_pfRelIso04_all\"].array(),\n            }, with_name=\"Momentum4D\")\n\n            jets = ak.zip({\n                \"pt\": tree[\"Jet_pt\"].array(),\n                \"eta\": tree[\"Jet_eta\"].array(),\n                \"phi\": tree[\"Jet_phi\"].array(),\n                \"mass\": tree[\"Jet_mass\"].array(),\n                \"jetId\": tree[\"Jet_jetId\"].array(),\n                \"btag\": tree[\"Jet_btagDeepFlavB\"].array(),\n            }, with_name=\"Momentum4D\")\n\n            met_pt = tree[\"MET_pt\"].array()\n            met_phi = tree[\"MET_phi\"].array()\n\n            if not IS_DATA: gen_weight = tree[\"genWeight\"].array()\n            else: gen_weight = ak.ones_like(met_pt)\n\n        # --- 1. selection---\n        in_gap = (abs(electrons.eta) &gt; 1.4442) &amp; (abs(electrons.eta) &lt; 1.566)\n        ele_tight_mask = (electrons.pt &gt; 30) &amp; (abs(electrons.eta) &lt; 2.1) &amp; (electrons.cutBased == 4) &amp; (~in_gap)\n        ele_veto_mask = (electrons.pt &gt; 10) &amp; (electrons.cutBased &gt;= 1)\n        mu_loose_mask = (muons.pt &gt; 10) &amp; (abs(muons.eta) &lt; 2.4) &amp; (muons.looseId) &amp; (muons.iso &lt; 0.25)\n\n        good_ele = electrons[ele_tight_mask]\n        veto_ele = electrons[ele_veto_mask]\n        veto_mu = muons[mu_loose_mask]\n\n        has_1_tight = (ak.num(good_ele) == 1)\n        is_exclusive = (ak.num(veto_ele) == 1) &amp; (ak.num(veto_mu) == 0)\n        pass_met = met_pt &gt; 150\n\n        event_mask = has_1_tight &amp; is_exclusive &amp; pass_met\n\n        good_ele = good_ele[event_mask]\n        jets = jets[event_mask]\n        met_pt = met_pt[event_mask]\n        met_phi = met_phi[event_mask]\n        gen_weight = gen_weight[event_mask]\n        leading_ele = good_ele[:, 0]\n\n\n        # A. Cleaning with ELECTRON\n        jet_clean_mask = (\n            (jets.pt &gt; 30) &amp; (jets.jetId &gt;= 1) &amp; \n            (jets.deltaR(leading_ele) &gt; 0.4)\n        )\n        all_clean_jets = jets[jet_clean_mask]\n\n        # B. Categories\n        central_jets = all_clean_jets[abs(all_clean_jets.eta) &lt; 2.4]\n        forward_jets = all_clean_jets[(abs(all_clean_jets.eta) &gt;= 2.4) &amp; (abs(all_clean_jets.eta) &lt; 5.0)]\n\n        has_2_jets = ak.num(central_jets) &gt;= 2\n\n        f_met = met_pt[has_2_jets]\n        f_met_phi = met_phi[has_2_jets]\n        f_ele = leading_ele[has_2_jets]\n        f_central = central_jets[has_2_jets]\n        f_forward = forward_jets[has_2_jets]\n        f_gen_weight = gen_weight[has_2_jets]\n\n        # mT\n        dphi = f_ele.phi - f_met_phi\n        mt = np.sqrt(2 * f_ele.pt * f_met * (1 - np.cos(dphi)))\n\n        # min DeltaPhi\n        jets_top2 = f_central[:, :2]\n        dphi_jets = abs(jets_top2.phi - f_met_phi)\n        dphi_jets = np.mod(dphi_jets + np.pi, 2*np.pi) - np.pi\n        min_dphi = ak.min(abs(dphi_jets), axis=1)\n\n        # B-tagging\n        is_btag = f_central.btag &gt; 0.2770\n        n_btag = ak.sum(is_btag, axis=1)\n\n        # Extras\n        n_fwd = ak.num(f_forward)\n\n        b_jets = f_central[is_btag]\n        has_b = ak.num(b_jets) &gt; 0\n        b_jet_padded = ak.fill_none(ak.firsts(b_jets), {\"pt\": 0, \"phi\": 0}, axis=0)\n        dphi_b = b_jet_padded.phi - f_met_phi\n        calc_mt_b = np.sqrt(2 * b_jet_padded.pt * f_met * (1 - np.cos(dphi_b)))\n\n        mt_b_np = ak.to_numpy(calc_mt_b)\n        has_b_np = ak.to_numpy(has_b)\n        mt_b_np[~has_b_np] = -1.0\n\n        return {\n            \"dataset\": dataset,\n            \"lep_pt\": ak.to_numpy(f_ele.pt),\n            \"lep_eta\": ak.to_numpy(f_ele.eta),\n            \"met\": ak.to_numpy(f_met),\n            \"mT_W\": ak.to_numpy(mt),\n            \"genWeight\": ak.to_numpy(f_gen_weight),\n            \"nJet\": ak.to_numpy(ak.num(f_central)),\n            \"nBTag\": ak.to_numpy(n_btag),\n            \"min_dphi\": ak.to_numpy(min_dphi),\n            \"nForwardJets\": ak.to_numpy(n_fwd),\n            \"mT_b\": mt_b_np\n        }\n\n</code></pre> <p>and with the same idea as the muon one</p> <pre><code>#import pandas as pd\n#import os\n\n\ndatasets_electron_channel = [\n    'SingleElectron',        # DATA\n    'ttbar-semileptonic',    # Main Background\n    'ttW',\n    'WW', 'ZZ', 'Zvv',\n    'DYJets-Zpt200',         # Drell\u2013Yan (important for Z\u2192ee)\n    't-channel-top',\n    'WJets-HT400to600'       # W+Jets\n]\n\ndef process_dataset_electron_raw(dataset, n_files=5):\n    # Explicit detection for data\n    is_data = \"SingleElectron\" in dataset\n\n    print(f\" Processing RAW Electron {dataset} (Is Data: {is_data})...\")\n\n    if dataset not in fileset:\n        print(f\"{dataset} not found in fileset. Skipping.\")\n        return None\n\n    files = fileset[dataset][\"files\"][:n_files]\n    dfs = []\n\n    for f in files:\n        try:\n            # Call the electron-level physics processing function\n            # (Make sure 'process_electron' is defined earlier)\n            data_dict = process_electron(f, dataset=dataset, IS_DATA=is_data)\n\n            df = pd.DataFrame(data_dict)\n            dfs.append(df)\n\n        except Exception as e:\n            # Report individual file errors without stopping the loop\n            print(f\" Error in file {f}: {e}\")\n\n    if len(dfs) == 0:\n        print(f\"No valid events were produced for {dataset}\")\n        return None\n\n    full_df = pd.concat(dfs, ignore_index=True)\n\n    # Save output\n    os.makedirs(\"output_raw\", exist_ok=True)\n\n    # NOTE: we use '_electron_raw' to avoid overlapping with muon outputs\n    output_path = f\"output_raw/{dataset}_electron_raw.parquet\"\n\n    full_df.to_parquet(output_path, index=False)\n    print(f\"Saved: {output_path} with {len(full_df)} events.\")\n\n    return full_df\n\n\n# --- 3. MASS EXECUTION ---\n# Use n_files=10 or more for reasonable statistics\nN_FILES = 10\n\nprint(f\"=== STARTING ELECTRON PROCESSING ({N_FILES} files per dataset) ===\")\n\nfor ds in datasets_electron_channel:\n    process_dataset_electron_raw(ds, n_files=N_FILES)\n\n</code></pre> <pre><code>=== STARTING ELECTRON PROCESSING (10 files per dataset) ===\n Processing RAW Electron SingleElectron (Is Data: True)...\nSaved: output_raw/SingleElectron_electron_raw.parquet with 10757 events.\n Processing RAW Electron ttbar-semileptonic (Is Data: False)...\nSaved: output_raw/ttbar-semileptonic_electron_raw.parquet with 78228 events.\n Processing RAW Electron ttW (Is Data: False)...\nSaved: output_raw/ttW_electron_raw.parquet with 60974 events.\n Processing RAW Electron WW (Is Data: False)...\nSaved: output_raw/WW_electron_raw.parquet with 6141 events.\n Processing RAW Electron ZZ (Is Data: False)...\nSaved: output_raw/ZZ_electron_raw.parquet with 83 events.\n Processing RAW Electron Zvv (Is Data: False)...\nSaved: output_raw/Zvv_electron_raw.parquet with 24 events.\n Processing RAW Electron DYJets-Zpt200 (Is Data: False)...\nSaved: output_raw/DYJets-Zpt200_electron_raw.parquet with 1398 events.\n Processing RAW Electron t-channel-top (Is Data: False)...\nSaved: output_raw/t-channel-top_electron_raw.parquet with 4238 events.\n Processing RAW Electron WJets-HT400to600 (Is Data: False)...\nSaved: output_raw/WJets-HT400to600_electron_raw.parquet with 35219 events.\n</code></pre> <pre><code>def load_all_electrons_cleaned():\n    loaded_dfs = {}\n\n    datasets_electron_channel = [\n        'SingleElectron', 'ttbar-semileptonic',\n        'ttW', 'WW', 'ZZ', 'Zvv', 'DYJets-Zpt200',\n        't-channel-top',\n        'WJets-HT400to600',\n    ]\n\n    for dataset in datasets_electron_channel:\n        path = f\"output_raw/{dataset}_electron_raw.parquet\"\n        try:\n            df = pd.read_parquet(path)\n\n            if dataset in DATA_SL:\n                df[\"final_weight\"] = 1.0\n            else:\n                xsec = fileset[dataset][\"metadata\"][\"xsec\"]\n                sum_w = sum_weights_map.get(dataset, 1.0)\n                if sum_w == 0: sum_w = 1.0\n                scale = (xsec * LUM) / sum_w\n\n                if \"genWeight\" in df.columns:\n                    df[\"final_weight\"] = df[\"genWeight\"] * scale\n                else:\n                    df[\"final_weight\"] = scale\n\n            loaded_dfs[dataset] = df\n            print(f\" Cargado: {dataset}\")\n\n        except FileNotFoundError:\n            continue\n\n    return loaded_dfs\n\n\nele_dfs_clean = load_all_electrons_cleaned()\n\nif len(ele_dfs_clean) &gt; 0:\n    print(\"\\n Generating CLEAN plots for SingleElectron...\")\n\n    all_dfs = ele_dfs_clean   # THIS CAUSES plot_grouped_stack TO USE ELECTRONS\n\n    mi_canal = \"SingleElectron\"\n\n\n    plot_grouped_stack(\"mT_W\", r\"$m_T^W$ [GeV]\", (0, 200), mi_canal, log_scale=False)\n    plot_grouped_stack(\"met\",  r\"$p_T^{miss}$ [GeV]\", (150, 500), mi_canal, log_scale=True)\n    plot_grouped_stack(\"nJet\", r\"$N_{jets}$\", (2, 10), mi_canal, n_bins=8, log_scale=True)\n    plot_grouped_stack(\"nBTag\", r\"$N_{b-tags}$\", (0, 5), mi_canal, n_bins=5, log_scale=True)\n\nelse:\n    print(\" No electron data was uploaded.\")\n\n\n</code></pre> <pre><code> Cargado: SingleElectron\n Cargado: ttbar-semileptonic\n Cargado: ttW\n Cargado: WW\n Cargado: ZZ\n Cargado: Zvv\n Cargado: DYJets-Zpt200\n Cargado: t-channel-top\n Cargado: WJets-HT400to600\n\n Generating CLEAN plots for SingleElectron...\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#signal-region","title":"Signal Region","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#table-13-final-event-selection-sl-and-ah","title":"Table 13 \u2014 Final Event Selection (SL and AH)","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#what-does-it-contain_1","title":"What does it contain?","text":"<p>The official final cuts defining the analysis Signal Regions (SR). Each SR is classified according to:</p> <ul> <li>Channel (semi-leptonic or hadronic)</li> <li>Number of b-jets</li> <li>Presence of forward jets</li> <li>Values of MET, MT, HT, etc.</li> </ul>"},{"location":"Single_Lepton/Analisis_muon_final_version/#what-is-it-for_2","title":"What is it for?","text":"<p>It defines the regions where dark matter signals are searched for. These regions are fed directly into the final statistical fit.</p> <pre><code>\ndisplay(Image(filename=\"Table_13.png\"))\n\n</code></pre> <p></p> <p>Now we move to the signal region. We use the events that were already filtered by the baseline selection and then apply the additional cuts described in Table 13. To do this, we first need to load the preprocessed datasets stored as parquet files, so the following function was created.</p> <p>This function loads only the datasets needed for the analysis channel (muon or electron)</p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#issues","title":"Issues","text":"<p>The normalization is applied again at the loading stage because the stored .parquet files do not consistently contain a final, analysis-ready weight. In some cases, the <code>final_weight</code> column is not saved at all, and in others it may have been produced using a different luminosity, a different fileset configuration, or an earlier definition of the analysis.</p> <p>To avoid mixing inconsistent normalizations and to guarantee that all samples are treated uniformly, the normalization is recomputed at runtime using the current cross sections and the total sum of generator weights. This ensures that the final event yields are coherent and directly comparable across datasets.</p> <pre><code>def load_data(channel=\"muon\"):\n    loaded_dfs = {}\n\n    datasets_to_load = [\n        'SingleMuon' if channel == 'muon' else 'SingleElectron',\n        'ttbar-semileptonic', 'ttW', 'WW', 'ZZ', 'Zvv', \n        'DYJets-Zpt200', 't-channel-top', 'WJets-HT400to600'\n    ]\n\n    suffix = \"_raw.parquet\" if channel == \"muon\" else \"_electron_raw.parquet\"\n\n    for ds in datasets_to_load:\n        path = f\"output_raw/{ds}{suffix}\"\n        try:\n            df = pd.read_parquet(path)\n\n            # Normalizaci\u00f3n\n            if ds in DATA_SL:\n                df[\"final_weight\"] = 1.0\n            else:\n                if ds not in fileset: continue\n                xsec = fileset[ds][\"metadata\"][\"xsec\"]\n                sum_w = sum_weights_map.get(ds, 1.0) \n                if sum_w == 0: sum_w = 1.0\n\n                scale = (xsec * LUM) / sum_w\n\n                if \"genWeight\" in df.columns:\n                    df[\"final_weight\"] = df[\"genWeight\"] * scale\n                else:\n                    df[\"final_weight\"] = scale\n\n            loaded_dfs[ds] = df\n\n        except FileNotFoundError:\n            continue\n\n    return loaded_dfs\n\n</code></pre> <pre><code>def signal_regions(df):\n\n\n    if df is None or len(df) == 0: return {}\n\n    # --- CORTES SUAVES ---\n    pass_common = (\n        (df[\"met\"] &gt; 160) &amp;\n        (df[\"min_dphi\"] &gt; 0.5) \n    )\n\n    df_sr = df[pass_common]\n    if len(df_sr) == 0: return {}\n\n    # Definici\u00f3n de las 3 Regiones (Categor\u00edas)\n    mask_1b_0f = (df_sr[\"nBTag\"] == 1) &amp; (df_sr[\"nForwardJets\"] == 0)\n    mask_1b_1f = (df_sr[\"nBTag\"] == 1) &amp; (df_sr[\"nForwardJets\"] &gt;= 1)\n    mask_2b = (df_sr[\"nBTag\"] &gt;= 2) \n\n    return {\n        \"SR_1b_0f\": df_sr[mask_1b_0f],\n        \"SR_1b_1f\": df_sr[mask_1b_1f],\n        \"SR_2b\":    df_sr[mask_2b]\n    }\n\n</code></pre> <p>Once <code>plot_grouped_stack</code> was defined, the same base code was reused to create <code>plot_region_stack</code>. The logic of the stacked histogram is the same, but extra features were added to handle analysis regions. In particular, a region label is drawn on the plot and the display is adapted to show results specific to a selected region. This keeps the core behavior unchanged while adding more flexibility for regional visualization.</p> <p>The main changes are:</p> <ul> <li>A region label parameter was added to the function signature.</li> <li>A text label was drawn on the plot using a line like:   <code>ax.text(..., region_label, ...)</code></li> <li>The plot title or decoration was adapted to reflect the selected region.</li> <li>Small adjustments were made to make the function work with region-specific dictionaries.</li> </ul> <p>All other parts of the stacking logic (histogram creation, grouping, colors, and legends) remain unchanged.</p> <pre><code>def plot_region_stack(dfs_dict, region_name, var_name, x_label, x_range, channel_label, n_bins=10):\n\n    bins = np.linspace(x_range[0], x_range[1], n_bins + 1)\n\n    # Preparar Datos para Stack\n    grouped_counts = {}\n    grouped_info = {}\n\n    for name, df in dfs_dict.items():\n        if name in DATA_SL: continue \n\n        counts, _ = np.histogram(df[var_name], bins=bins, weights=df[\"final_weight\"])\n        g_key, label, color = get_group_info(name)\n\n        if g_key not in grouped_counts:\n            grouped_counts[g_key] = counts\n            grouped_info[g_key] = {\"label\": label, \"color\": color, \"yield\": np.sum(counts)}\n        else:\n            grouped_counts[g_key] += counts\n            grouped_info[g_key][\"yield\"] += np.sum(counts)\n\n    # Ordenar Stack\n    active_groups = sorted(grouped_info.keys(), key=lambda k: grouped_info[k][\"yield\"])\n\n    mc_counts = []\n    mc_colors = []\n    mc_labels = []\n    total_mc = np.zeros(n_bins)\n\n    for g in active_groups:\n        mc_counts.append(grouped_counts[g])\n        mc_colors.append(grouped_info[g][\"color\"])\n        mc_labels.append(grouped_info[g][\"label\"])\n        total_mc += grouped_counts[g]\n\n    # Datos\n    df_data = dfs_dict.get(channel_label)\n    data_counts = np.zeros(n_bins)\n    if df_data is not None:\n        data_counts, _ = np.histogram(df_data[var_name], bins=bins)\n\n    # Graficar\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    if len(mc_counts) &gt; 0:\n        hep.histplot(mc_counts, bins=bins, stack=True, histtype=\"fill\",\n                     color=mc_colors, label=mc_labels, edgecolor=\"black\", linewidth=1, ax=ax)\n\n    hep.histplot(data_counts, bins=bins, histtype=\"errorbar\", color=\"black\",\n                 label=f\"{channel_label} (Data)\", yerr=True, marker='o', markersize=5, ax=ax)\n\n    # Decoraci\u00f3n\n    hep.cms.label(\"Preliminary\", data=True, lumi=35.9, year=2016, ax=ax)\n\n    # Leyenda Limpia\n    handles, labels = ax.get_legend_handles_labels()\n    by_label = OrderedDict(zip(labels[::-1], handles[::-1]))\n    ax.legend(by_label.values(), by_label.keys(), fontsize=15, ncol=2, loc='upper right')\n\n    # Etiqueta Interna\n    ax.text(0.05, 0.93, f\"{region_name}\", transform=ax.transAxes, \n            fontsize=20, fontweight='bold', va='top')\n\n    ax.set_xlabel(x_label, fontsize=24)\n    ax.set_ylabel(\"Events\", fontsize=24)\n    ax.set_xlim(x_range)\n    ax.set_yscale(\"log\") \n\n    max_y = max(np.max(data_counts), np.max(total_mc))\n    ax.set_ylim(0.1, max_y * 500) \n\n    plt.tight_layout()\n    plt.show()\n\n</code></pre> <pre><code>def run_analysis_for_channel(channel_mode):\n\n    raw_dfs = load_data(channel_mode)\n    if not raw_dfs: return\n\n    regions_db = {\"SR_1b_0f\": {}, \"SR_1b_1f\": {}, \"SR_2b\": {}}\n    for name, df in raw_dfs.items():\n        sub_regions = signal_regions(df)\n        for reg, df_reg in sub_regions.items():\n            regions_db[reg][name] = df_reg\n\n    d_label = \"SingleMuon\" if channel_mode == \"muon\" else \"SingleElectron\"\n\n    plot_region_stack(regions_db[\"SR_2b\"], \"SR 2b\", \"met\", r\"$p_T^{miss}$ [GeV]\", (160, 600), d_label)\n    plot_region_stack(regions_db[\"SR_1b_1f\"], r\"SR 1b $\\geq$1f\", \"met\", r\"$p_T^{miss}$ [GeV]\", (160, 600), d_label)\n    plot_region_stack(regions_db[\"SR_1b_0f\"], \"SR 1b 0f\", \"met\", r\"$p_T^{miss}$ [GeV]\", (160, 600), d_label)\n\n</code></pre> <pre><code>from collections import OrderedDict\n\n\nif 'sum_weights_map' in globals():\n    run_analysis_for_channel(\"muon\")\n    run_analysis_for_channel(\"electron\")\nelse:\n    print(\" Error.\")\n\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Single_Lepton/Analisis_muon_final_version/#control-region","title":"Control Region","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#table-14-control-region-definitions-sl-and-ah","title":"Table 14 \u2014 Control Region Definitions (SL and AH)","text":""},{"location":"Single_Lepton/Analisis_muon_final_version/#what-does-it-contain_2","title":"What does it contain?","text":"<p>This table defines the Control Regions (CRs) required to calibrate and validate the main background processes in the analysis:</p> <ul> <li>Dileptonic tt\u0304</li> <li>W+jets</li> <li>Z\u2192\u03bd\u03bd (in the all-hadronic channel)</li> <li>Semileptonic tt\u0304 (in the all-hadronic channel)</li> </ul>"},{"location":"Single_Lepton/Analisis_muon_final_version/#what-is-it-used-for","title":"What is it used for?","text":"<p>These regions serve to:</p> <ul> <li>Constrain and normalize the simulated background yields</li> <li>Compare Monte Carlo predictions against real data</li> <li>Reduce systematic uncertainties</li> </ul> <p>They are essential for controlling the dominant backgrounds before extracting the signal in the Signal Regions (SRs).</p> <pre><code>\ndisplay(Image(filename=\"Table_14.png\"))\n\n</code></pre> <p></p> <pre><code>def filter_cr_wlnu(df):\n    \"\"\"\n    Control Region: CR W(l\u03bd)\n    Nombres oficiales Tabla 14:\n        - SL1eWR  (electr\u00f3n)\n        - SL1mWR  (mu\u00f3n)\n    \"\"\"\n    if df is None or len(df) == 0:\n        return {}\n\n    mask = (\n        (df[\"met\"] &gt;= 160) &amp;\n        (df[\"mT_W\"] &gt;= 80) &amp;\n        (df[\"nBTag\"] == 0) &amp;\n        (df[\"nJet\"] &gt;= 2)\n    )\n\n    df_cr = df[mask]\n    if len(df_cr) == 0:\n        return {}\n\n    return {\"CR_Wlnu\": df_cr}\n\n</code></pre> <p>Once <code>plot_region_stack</code> was defined, the same base structure was reused to create <code>plot_cr_stack</code>. The core stacked histogram logic stays the same, but additional lines were added or modified to specialize the plot for Control Regions.</p> <p>The main changes are:</p> <ul> <li>A new function name was created: <code>plot_cr_stack</code>.</li> <li>A new input parameter was added to handle the control region label (for example: <code>region_label</code>).</li> <li>The plot now includes a region tag drawn on the figure, added through a line like:   <code>ax.text(..., region_label, ...)</code></li> <li>The function was adapted to work with Control Region\u2013filtered datasets, instead of general or signal regions.</li> <li>The default axis ranges and binning were tuned for control-region kinematics.</li> </ul> <p>All histogram stacking, coloring, grouping, and legend logic remains the same as in the previous functions.</p> <pre><code>def plot_cr_stack(dfs_dict, region_label, var_name, x_label, x_range, data_label, n_bins=15):\n\n    bins = np.linspace(x_range[0], x_range[1], n_bins + 1)\n    grouped_counts = {}\n    grouped_info = {}\n\n    # A) MC\n    for name, df in dfs_dict.items():\n        if name in DATA_SL:\n            continue\n\n        counts, _ = np.histogram(df[var_name], bins=bins, weights=df[\"final_weight\"])\n        g_key, label, color = get_group_info(name)\n\n        if g_key not in grouped_counts:\n            grouped_counts[g_key] = counts\n            grouped_info[g_key] = {\"label\": label, \"color\": color, \"yield\": np.sum(counts)}\n        else:\n            grouped_counts[g_key] += counts\n            grouped_info[g_key][\"yield\"] += np.sum(counts)\n\n    active_groups = sorted(grouped_info.keys(), key=lambda k: grouped_info[k][\"yield\"])\n\n    mc_counts = []\n    mc_colors = []\n    mc_labels = []\n    total_mc = np.zeros(n_bins)\n\n    for g in active_groups:\n        mc_counts.append(grouped_counts[g])\n        mc_colors.append(grouped_info[g][\"color\"])\n        mc_labels.append(grouped_info[g][\"label\"])\n        total_mc += grouped_counts[g]\n\n    # B) DATA\n    df_data = dfs_dict.get(data_label)\n    if df_data is not None:\n        data_counts, _ = np.histogram(df_data[var_name], bins=bins)\n    else:\n        data_counts = np.zeros(n_bins)\n\n    # === Plot ===\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    hep.histplot(mc_counts, bins=bins, stack=True, histtype=\"fill\",\n                 color=mc_colors, label=mc_labels, edgecolor=\"black\", linewidth=1, ax=ax)\n\n    hep.histplot(data_counts, bins=bins, histtype=\"errorbar\", color=\"black\",\n                 label=f\"{data_label} (Data)\", yerr=True, marker='o', markersize=5, ax=ax)\n\n    hep.cms.label(\"Preliminary\", data=True, lumi=35.9, year=2016, ax=ax)\n\n    ax.text(0.05, 0.93, region_label, transform=ax.transAxes,\n            fontsize=20, fontweight='bold', va='top', ha='left')\n\n    handles, labels = ax.get_legend_handles_labels()\n    by_label = OrderedDict(zip(labels[::-1], handles[::-1]))\n    ax.legend(by_label.values(), by_label.keys(), fontsize=15, ncol=2, loc='upper right')\n\n    ax.set_xlabel(x_label, fontsize=24)\n    ax.set_ylabel(\"Events\", fontsize=24)\n    ax.set_xlim(x_range)\n    ax.set_yscale(\"log\")\n\n    max_y = max(np.max(data_counts), np.max(total_mc))\n    ax.set_ylim(0.1, max_y * 500)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n</code></pre> <pre><code>\ndef run_SL(channel_mode):\n\n    # 1. Load datasets\n    raw_dfs = load_data(channel_mode)\n    if not raw_dfs:\n        return\n\n    # 2. Filtrar regi\u00f3n CR W(l\u03bd)\n    cr_dict = {}\n    for name, df in raw_dfs.items():\n        reg = filter_cr_wlnu(df)\n        if \"CR_Wlnu\" in reg:\n            cr_dict[name] = reg[\"CR_Wlnu\"]\n\n    if len(cr_dict) == 0:\n        print(\" No events passed CR W(l\u03bd).\")\n        return\n\n    # 3. Etiquetas correctas seg\u00fan Tabla 14\n    if channel_mode == \"muon\":\n        region_label = \"SL1mWR\"\n        data_label = \"SingleMuon\"\n    else:\n        region_label = \"SL1eWR\"\n        data_label = \"SingleElectron\"\n\n\n    # plot MET\n    plot_cr_stack(cr_dict, region_label, \"met\", r\"$p_T^{miss}$ [GeV]\", (160, 600), data_label, n_bins=15)\n\n    # plot MT\n    #plot_cr_stack(cr_dict, region_label, \"mT_W\", r\"$m_T^W$ [GeV]\", (160, 500), data_label, n_bins=15)\n\n\n# --- RUN ---\nrun_SL(\"muon\")\nrun_SL(\"electron\")\n</code></pre> <p></p> <p></p>"}]}